{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import Saliency\n",
    "from captum.attr import DeepLift\n",
    "from captum.attr import NoiseTunnel\n",
    "from captum.attr import visualization as viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.cn1 = nn.Conv2d(1, 16, 3, 1)\n",
    "        self.cn2 = nn.Conv2d(16, 32, 3, 1)\n",
    "        self.dp1 = nn.Dropout2d(0.10)\n",
    "        self.dp2 = nn.Dropout2d(0.25)\n",
    "        self.fc1 = nn.Linear(4608, 64) # 4608 is basically 12 X 12 X 32\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.cn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.cn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dp1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dp2(x)\n",
    "        x = self.fc2(x)\n",
    "        op = F.log_softmax(x, dim=1)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define training and inference routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_dataloader, optim, epoch):\n",
    "    model.train()\n",
    "    for b_i, (X, y) in enumerate(train_dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optim.zero_grad()\n",
    "        pred_prob = model(X)\n",
    "        loss = F.nll_loss(pred_prob, y) # nll is the negative likelihood loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        if b_i % 10 == 0:\n",
    "            print('epoch: {} [{}/{} ({:.0f}%)]\\t training loss: {:.6f}'.format(\n",
    "                epoch, b_i * len(X), len(train_dataloader.dataset),\n",
    "                100. * b_i / len(train_dataloader), loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_dataloader):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    success = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred_prob = model(X)\n",
    "            loss += F.nll_loss(pred_prob, y, reduction='sum').item()  # loss summed across the batch\n",
    "            pred = pred_prob.argmax(dim=1, keepdim=True)  # us argmax to get the most likely prediction\n",
    "            success += pred.eq(y.view_as(pred)).sum().item()\n",
    "\n",
    "    loss /= len(test_dataloader.dataset)\n",
    "\n",
    "    print('\\nTest dataset: Overall Loss: {:.4f}, Overall Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        loss, success, len(test_dataloader.dataset),\n",
    "        100. * success / len(test_dataloader.dataset)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean and standard deviation values are calculated as the mean of all pixel values of all images in the training dataset\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,))])), # train_X.mean()/256. and train_X.std()/256.\n",
    "    batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1302,), (0.3069,)) \n",
    "                   ])),\n",
    "    batch_size=500, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define optimizer and run training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "model = ConvNet()\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [0/60000 (0%)]\t training loss: 2.333043\n",
      "epoch: 1 [320/60000 (1%)]\t training loss: 1.943062\n",
      "epoch: 1 [640/60000 (1%)]\t training loss: 1.304336\n",
      "epoch: 1 [960/60000 (2%)]\t training loss: 1.238040\n",
      "epoch: 1 [1280/60000 (2%)]\t training loss: 0.958737\n",
      "epoch: 1 [1600/60000 (3%)]\t training loss: 0.493184\n",
      "epoch: 1 [1920/60000 (3%)]\t training loss: 0.643627\n",
      "epoch: 1 [2240/60000 (4%)]\t training loss: 0.265545\n",
      "epoch: 1 [2560/60000 (4%)]\t training loss: 0.560548\n",
      "epoch: 1 [2880/60000 (5%)]\t training loss: 0.261553\n",
      "epoch: 1 [3200/60000 (5%)]\t training loss: 0.306239\n",
      "epoch: 1 [3520/60000 (6%)]\t training loss: 0.300254\n",
      "epoch: 1 [3840/60000 (6%)]\t training loss: 0.261871\n",
      "epoch: 1 [4160/60000 (7%)]\t training loss: 0.253512\n",
      "epoch: 1 [4480/60000 (7%)]\t training loss: 0.103370\n",
      "epoch: 1 [4800/60000 (8%)]\t training loss: 0.244615\n",
      "epoch: 1 [5120/60000 (9%)]\t training loss: 0.246581\n",
      "epoch: 1 [5440/60000 (9%)]\t training loss: 0.426752\n",
      "epoch: 1 [5760/60000 (10%)]\t training loss: 0.164684\n",
      "epoch: 1 [6080/60000 (10%)]\t training loss: 0.063364\n",
      "epoch: 1 [6400/60000 (11%)]\t training loss: 0.220145\n",
      "epoch: 1 [6720/60000 (11%)]\t training loss: 0.410414\n",
      "epoch: 1 [7040/60000 (12%)]\t training loss: 0.323727\n",
      "epoch: 1 [7360/60000 (12%)]\t training loss: 0.294908\n",
      "epoch: 1 [7680/60000 (13%)]\t training loss: 0.091494\n",
      "epoch: 1 [8000/60000 (13%)]\t training loss: 0.188713\n",
      "epoch: 1 [8320/60000 (14%)]\t training loss: 0.144609\n",
      "epoch: 1 [8640/60000 (14%)]\t training loss: 0.325974\n",
      "epoch: 1 [8960/60000 (15%)]\t training loss: 0.031163\n",
      "epoch: 1 [9280/60000 (15%)]\t training loss: 0.036093\n",
      "epoch: 1 [9600/60000 (16%)]\t training loss: 0.214139\n",
      "epoch: 1 [9920/60000 (17%)]\t training loss: 0.156931\n",
      "epoch: 1 [10240/60000 (17%)]\t training loss: 0.096676\n",
      "epoch: 1 [10560/60000 (18%)]\t training loss: 0.183215\n",
      "epoch: 1 [10880/60000 (18%)]\t training loss: 0.033010\n",
      "epoch: 1 [11200/60000 (19%)]\t training loss: 0.106560\n",
      "epoch: 1 [11520/60000 (19%)]\t training loss: 0.167177\n",
      "epoch: 1 [11840/60000 (20%)]\t training loss: 0.116418\n",
      "epoch: 1 [12160/60000 (20%)]\t training loss: 0.347938\n",
      "epoch: 1 [12480/60000 (21%)]\t training loss: 0.072275\n",
      "epoch: 1 [12800/60000 (21%)]\t training loss: 0.069344\n",
      "epoch: 1 [13120/60000 (22%)]\t training loss: 0.088382\n",
      "epoch: 1 [13440/60000 (22%)]\t training loss: 0.096705\n",
      "epoch: 1 [13760/60000 (23%)]\t training loss: 0.128923\n",
      "epoch: 1 [14080/60000 (23%)]\t training loss: 0.127995\n",
      "epoch: 1 [14400/60000 (24%)]\t training loss: 0.125990\n",
      "epoch: 1 [14720/60000 (25%)]\t training loss: 0.148126\n",
      "epoch: 1 [15040/60000 (25%)]\t training loss: 0.409140\n",
      "epoch: 1 [15360/60000 (26%)]\t training loss: 0.133046\n",
      "epoch: 1 [15680/60000 (26%)]\t training loss: 0.120051\n",
      "epoch: 1 [16000/60000 (27%)]\t training loss: 0.219374\n",
      "epoch: 1 [16320/60000 (27%)]\t training loss: 0.152537\n",
      "epoch: 1 [16640/60000 (28%)]\t training loss: 0.350288\n",
      "epoch: 1 [16960/60000 (28%)]\t training loss: 0.273461\n",
      "epoch: 1 [17280/60000 (29%)]\t training loss: 0.261697\n",
      "epoch: 1 [17600/60000 (29%)]\t training loss: 0.085179\n",
      "epoch: 1 [17920/60000 (30%)]\t training loss: 0.033157\n",
      "epoch: 1 [18240/60000 (30%)]\t training loss: 0.081473\n",
      "epoch: 1 [18560/60000 (31%)]\t training loss: 0.004736\n",
      "epoch: 1 [18880/60000 (31%)]\t training loss: 0.185923\n",
      "epoch: 1 [19200/60000 (32%)]\t training loss: 0.293117\n",
      "epoch: 1 [19520/60000 (33%)]\t training loss: 0.064243\n",
      "epoch: 1 [19840/60000 (33%)]\t training loss: 0.044118\n",
      "epoch: 1 [20160/60000 (34%)]\t training loss: 0.028630\n",
      "epoch: 1 [20480/60000 (34%)]\t training loss: 0.117261\n",
      "epoch: 1 [20800/60000 (35%)]\t training loss: 0.266663\n",
      "epoch: 1 [21120/60000 (35%)]\t training loss: 0.032758\n",
      "epoch: 1 [21440/60000 (36%)]\t training loss: 0.084564\n",
      "epoch: 1 [21760/60000 (36%)]\t training loss: 0.135888\n",
      "epoch: 1 [22080/60000 (37%)]\t training loss: 0.040037\n",
      "epoch: 1 [22400/60000 (37%)]\t training loss: 0.019712\n",
      "epoch: 1 [22720/60000 (38%)]\t training loss: 0.150382\n",
      "epoch: 1 [23040/60000 (38%)]\t training loss: 0.067946\n",
      "epoch: 1 [23360/60000 (39%)]\t training loss: 0.221487\n",
      "epoch: 1 [23680/60000 (39%)]\t training loss: 0.148346\n",
      "epoch: 1 [24000/60000 (40%)]\t training loss: 0.125404\n",
      "epoch: 1 [24320/60000 (41%)]\t training loss: 0.116576\n",
      "epoch: 1 [24640/60000 (41%)]\t training loss: 0.081610\n",
      "epoch: 1 [24960/60000 (42%)]\t training loss: 0.040039\n",
      "epoch: 1 [25280/60000 (42%)]\t training loss: 0.111751\n",
      "epoch: 1 [25600/60000 (43%)]\t training loss: 0.027178\n",
      "epoch: 1 [25920/60000 (43%)]\t training loss: 0.039238\n",
      "epoch: 1 [26240/60000 (44%)]\t training loss: 0.046624\n",
      "epoch: 1 [26560/60000 (44%)]\t training loss: 0.036259\n",
      "epoch: 1 [26880/60000 (45%)]\t training loss: 0.042893\n",
      "epoch: 1 [27200/60000 (45%)]\t training loss: 0.021309\n",
      "epoch: 1 [27520/60000 (46%)]\t training loss: 0.078010\n",
      "epoch: 1 [27840/60000 (46%)]\t training loss: 0.067319\n",
      "epoch: 1 [28160/60000 (47%)]\t training loss: 0.019019\n",
      "epoch: 1 [28480/60000 (47%)]\t training loss: 0.109955\n",
      "epoch: 1 [28800/60000 (48%)]\t training loss: 0.075527\n",
      "epoch: 1 [29120/60000 (49%)]\t training loss: 0.050420\n",
      "epoch: 1 [29440/60000 (49%)]\t training loss: 0.058509\n",
      "epoch: 1 [29760/60000 (50%)]\t training loss: 0.042245\n",
      "epoch: 1 [30080/60000 (50%)]\t training loss: 0.067011\n",
      "epoch: 1 [30400/60000 (51%)]\t training loss: 0.025555\n",
      "epoch: 1 [30720/60000 (51%)]\t training loss: 0.170165\n",
      "epoch: 1 [31040/60000 (52%)]\t training loss: 0.015962\n",
      "epoch: 1 [31360/60000 (52%)]\t training loss: 0.320396\n",
      "epoch: 1 [31680/60000 (53%)]\t training loss: 0.014895\n",
      "epoch: 1 [32000/60000 (53%)]\t training loss: 0.273716\n",
      "epoch: 1 [32320/60000 (54%)]\t training loss: 0.201027\n",
      "epoch: 1 [32640/60000 (54%)]\t training loss: 0.324443\n",
      "epoch: 1 [32960/60000 (55%)]\t training loss: 0.125436\n",
      "epoch: 1 [33280/60000 (55%)]\t training loss: 0.077653\n",
      "epoch: 1 [33600/60000 (56%)]\t training loss: 0.022727\n",
      "epoch: 1 [33920/60000 (57%)]\t training loss: 0.153698\n",
      "epoch: 1 [34240/60000 (57%)]\t training loss: 0.192000\n",
      "epoch: 1 [34560/60000 (58%)]\t training loss: 0.106195\n",
      "epoch: 1 [34880/60000 (58%)]\t training loss: 0.015246\n",
      "epoch: 1 [35200/60000 (59%)]\t training loss: 0.056208\n",
      "epoch: 1 [35520/60000 (59%)]\t training loss: 0.023621\n",
      "epoch: 1 [35840/60000 (60%)]\t training loss: 0.072618\n",
      "epoch: 1 [36160/60000 (60%)]\t training loss: 0.049412\n",
      "epoch: 1 [36480/60000 (61%)]\t training loss: 0.078804\n",
      "epoch: 1 [36800/60000 (61%)]\t training loss: 0.019641\n",
      "epoch: 1 [37120/60000 (62%)]\t training loss: 0.073145\n",
      "epoch: 1 [37440/60000 (62%)]\t training loss: 0.084906\n",
      "epoch: 1 [37760/60000 (63%)]\t training loss: 0.165430\n",
      "epoch: 1 [38080/60000 (63%)]\t training loss: 0.138081\n",
      "epoch: 1 [38400/60000 (64%)]\t training loss: 0.255426\n",
      "epoch: 1 [38720/60000 (65%)]\t training loss: 0.105837\n",
      "epoch: 1 [39040/60000 (65%)]\t training loss: 0.195803\n",
      "epoch: 1 [39360/60000 (66%)]\t training loss: 0.108841\n",
      "epoch: 1 [39680/60000 (66%)]\t training loss: 0.005558\n",
      "epoch: 1 [40000/60000 (67%)]\t training loss: 0.065162\n",
      "epoch: 1 [40320/60000 (67%)]\t training loss: 0.048789\n",
      "epoch: 1 [40640/60000 (68%)]\t training loss: 0.070413\n",
      "epoch: 1 [40960/60000 (68%)]\t training loss: 0.078550\n",
      "epoch: 1 [41280/60000 (69%)]\t training loss: 0.015537\n",
      "epoch: 1 [41600/60000 (69%)]\t training loss: 0.130916\n",
      "epoch: 1 [41920/60000 (70%)]\t training loss: 0.075333\n",
      "epoch: 1 [42240/60000 (70%)]\t training loss: 0.092604\n",
      "epoch: 1 [42560/60000 (71%)]\t training loss: 0.027642\n",
      "epoch: 1 [42880/60000 (71%)]\t training loss: 0.071713\n",
      "epoch: 1 [43200/60000 (72%)]\t training loss: 0.148932\n",
      "epoch: 1 [43520/60000 (73%)]\t training loss: 0.044493\n",
      "epoch: 1 [43840/60000 (73%)]\t training loss: 0.094794\n",
      "epoch: 1 [44160/60000 (74%)]\t training loss: 0.040495\n",
      "epoch: 1 [44480/60000 (74%)]\t training loss: 0.023940\n",
      "epoch: 1 [44800/60000 (75%)]\t training loss: 0.014863\n",
      "epoch: 1 [45120/60000 (75%)]\t training loss: 0.080790\n",
      "epoch: 1 [45440/60000 (76%)]\t training loss: 0.016429\n",
      "epoch: 1 [45760/60000 (76%)]\t training loss: 0.042312\n",
      "epoch: 1 [46080/60000 (77%)]\t training loss: 0.122510\n",
      "epoch: 1 [46400/60000 (77%)]\t training loss: 0.018108\n",
      "epoch: 1 [46720/60000 (78%)]\t training loss: 0.042726\n",
      "epoch: 1 [47040/60000 (78%)]\t training loss: 0.033287\n",
      "epoch: 1 [47360/60000 (79%)]\t training loss: 0.004960\n",
      "epoch: 1 [47680/60000 (79%)]\t training loss: 0.159315\n",
      "epoch: 1 [48000/60000 (80%)]\t training loss: 0.163358\n",
      "epoch: 1 [48320/60000 (81%)]\t training loss: 0.133813\n",
      "epoch: 1 [48640/60000 (81%)]\t training loss: 0.067080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 [48960/60000 (82%)]\t training loss: 0.189037\n",
      "epoch: 1 [49280/60000 (82%)]\t training loss: 0.027550\n",
      "epoch: 1 [49600/60000 (83%)]\t training loss: 0.030760\n",
      "epoch: 1 [49920/60000 (83%)]\t training loss: 0.047113\n",
      "epoch: 1 [50240/60000 (84%)]\t training loss: 0.201450\n",
      "epoch: 1 [50560/60000 (84%)]\t training loss: 0.163309\n",
      "epoch: 1 [50880/60000 (85%)]\t training loss: 0.230963\n",
      "epoch: 1 [51200/60000 (85%)]\t training loss: 0.208547\n",
      "epoch: 1 [51520/60000 (86%)]\t training loss: 0.193209\n",
      "epoch: 1 [51840/60000 (86%)]\t training loss: 0.052333\n",
      "epoch: 1 [52160/60000 (87%)]\t training loss: 0.071918\n",
      "epoch: 1 [52480/60000 (87%)]\t training loss: 0.104872\n",
      "epoch: 1 [52800/60000 (88%)]\t training loss: 0.100245\n",
      "epoch: 1 [53120/60000 (89%)]\t training loss: 0.116145\n",
      "epoch: 1 [53440/60000 (89%)]\t training loss: 0.052742\n",
      "epoch: 1 [53760/60000 (90%)]\t training loss: 0.207156\n",
      "epoch: 1 [54080/60000 (90%)]\t training loss: 0.055222\n",
      "epoch: 1 [54400/60000 (91%)]\t training loss: 0.100362\n",
      "epoch: 1 [54720/60000 (91%)]\t training loss: 0.146641\n",
      "epoch: 1 [55040/60000 (92%)]\t training loss: 0.297926\n",
      "epoch: 1 [55360/60000 (92%)]\t training loss: 0.023830\n",
      "epoch: 1 [55680/60000 (93%)]\t training loss: 0.019826\n",
      "epoch: 1 [56000/60000 (93%)]\t training loss: 0.210100\n",
      "epoch: 1 [56320/60000 (94%)]\t training loss: 0.008069\n",
      "epoch: 1 [56640/60000 (94%)]\t training loss: 0.099603\n",
      "epoch: 1 [56960/60000 (95%)]\t training loss: 0.028852\n",
      "epoch: 1 [57280/60000 (95%)]\t training loss: 0.015403\n",
      "epoch: 1 [57600/60000 (96%)]\t training loss: 0.209963\n",
      "epoch: 1 [57920/60000 (97%)]\t training loss: 0.084345\n",
      "epoch: 1 [58240/60000 (97%)]\t training loss: 0.005287\n",
      "epoch: 1 [58560/60000 (98%)]\t training loss: 0.226485\n",
      "epoch: 1 [58880/60000 (98%)]\t training loss: 0.167074\n",
      "epoch: 1 [59200/60000 (99%)]\t training loss: 0.045646\n",
      "epoch: 1 [59520/60000 (99%)]\t training loss: 0.013449\n",
      "epoch: 1 [59840/60000 (100%)]\t training loss: 0.024415\n",
      "\n",
      "Test dataset: Overall Loss: 0.0522, Overall Accuracy: 9825/10000 (98%)\n",
      "\n",
      "epoch: 2 [0/60000 (0%)]\t training loss: 0.283887\n",
      "epoch: 2 [320/60000 (1%)]\t training loss: 0.001043\n",
      "epoch: 2 [640/60000 (1%)]\t training loss: 0.002641\n",
      "epoch: 2 [960/60000 (2%)]\t training loss: 0.017910\n",
      "epoch: 2 [1280/60000 (2%)]\t training loss: 0.035087\n",
      "epoch: 2 [1600/60000 (3%)]\t training loss: 0.006630\n",
      "epoch: 2 [1920/60000 (3%)]\t training loss: 0.309971\n",
      "epoch: 2 [2240/60000 (4%)]\t training loss: 0.174572\n",
      "epoch: 2 [2560/60000 (4%)]\t training loss: 0.071286\n",
      "epoch: 2 [2880/60000 (5%)]\t training loss: 0.008842\n",
      "epoch: 2 [3200/60000 (5%)]\t training loss: 0.043614\n",
      "epoch: 2 [3520/60000 (6%)]\t training loss: 0.009304\n",
      "epoch: 2 [3840/60000 (6%)]\t training loss: 0.094230\n",
      "epoch: 2 [4160/60000 (7%)]\t training loss: 0.013201\n",
      "epoch: 2 [4480/60000 (7%)]\t training loss: 0.004689\n",
      "epoch: 2 [4800/60000 (8%)]\t training loss: 0.152283\n",
      "epoch: 2 [5120/60000 (9%)]\t training loss: 0.042221\n",
      "epoch: 2 [5440/60000 (9%)]\t training loss: 0.013336\n",
      "epoch: 2 [5760/60000 (10%)]\t training loss: 0.012721\n",
      "epoch: 2 [6080/60000 (10%)]\t training loss: 0.021102\n",
      "epoch: 2 [6400/60000 (11%)]\t training loss: 0.438950\n",
      "epoch: 2 [6720/60000 (11%)]\t training loss: 0.002385\n",
      "epoch: 2 [7040/60000 (12%)]\t training loss: 0.035253\n",
      "epoch: 2 [7360/60000 (12%)]\t training loss: 0.048008\n",
      "epoch: 2 [7680/60000 (13%)]\t training loss: 0.002561\n",
      "epoch: 2 [8000/60000 (13%)]\t training loss: 0.011579\n",
      "epoch: 2 [8320/60000 (14%)]\t training loss: 0.026800\n",
      "epoch: 2 [8640/60000 (14%)]\t training loss: 0.270779\n",
      "epoch: 2 [8960/60000 (15%)]\t training loss: 0.001698\n",
      "epoch: 2 [9280/60000 (15%)]\t training loss: 0.051953\n",
      "epoch: 2 [9600/60000 (16%)]\t training loss: 0.145548\n",
      "epoch: 2 [9920/60000 (17%)]\t training loss: 0.172020\n",
      "epoch: 2 [10240/60000 (17%)]\t training loss: 0.022827\n",
      "epoch: 2 [10560/60000 (18%)]\t training loss: 0.016456\n",
      "epoch: 2 [10880/60000 (18%)]\t training loss: 0.051300\n",
      "epoch: 2 [11200/60000 (19%)]\t training loss: 0.069796\n",
      "epoch: 2 [11520/60000 (19%)]\t training loss: 0.190750\n",
      "epoch: 2 [11840/60000 (20%)]\t training loss: 0.267316\n",
      "epoch: 2 [12160/60000 (20%)]\t training loss: 0.303864\n",
      "epoch: 2 [12480/60000 (21%)]\t training loss: 0.044847\n",
      "epoch: 2 [12800/60000 (21%)]\t training loss: 0.153253\n",
      "epoch: 2 [13120/60000 (22%)]\t training loss: 0.010190\n",
      "epoch: 2 [13440/60000 (22%)]\t training loss: 0.125087\n",
      "epoch: 2 [13760/60000 (23%)]\t training loss: 0.002830\n",
      "epoch: 2 [14080/60000 (23%)]\t training loss: 0.001888\n",
      "epoch: 2 [14400/60000 (24%)]\t training loss: 0.064268\n",
      "epoch: 2 [14720/60000 (25%)]\t training loss: 0.002603\n",
      "epoch: 2 [15040/60000 (25%)]\t training loss: 0.061247\n",
      "epoch: 2 [15360/60000 (26%)]\t training loss: 0.217783\n",
      "epoch: 2 [15680/60000 (26%)]\t training loss: 0.062138\n",
      "epoch: 2 [16000/60000 (27%)]\t training loss: 0.012053\n",
      "epoch: 2 [16320/60000 (27%)]\t training loss: 0.006884\n",
      "epoch: 2 [16640/60000 (28%)]\t training loss: 0.067185\n",
      "epoch: 2 [16960/60000 (28%)]\t training loss: 0.011688\n",
      "epoch: 2 [17280/60000 (29%)]\t training loss: 0.017941\n",
      "epoch: 2 [17600/60000 (29%)]\t training loss: 0.316944\n",
      "epoch: 2 [17920/60000 (30%)]\t training loss: 0.055586\n",
      "epoch: 2 [18240/60000 (30%)]\t training loss: 0.340394\n",
      "epoch: 2 [18560/60000 (31%)]\t training loss: 0.086103\n",
      "epoch: 2 [18880/60000 (31%)]\t training loss: 0.035126\n",
      "epoch: 2 [19200/60000 (32%)]\t training loss: 0.018281\n",
      "epoch: 2 [19520/60000 (33%)]\t training loss: 0.128952\n",
      "epoch: 2 [19840/60000 (33%)]\t training loss: 0.050536\n",
      "epoch: 2 [20160/60000 (34%)]\t training loss: 0.026000\n",
      "epoch: 2 [20480/60000 (34%)]\t training loss: 0.035401\n",
      "epoch: 2 [20800/60000 (35%)]\t training loss: 0.156032\n",
      "epoch: 2 [21120/60000 (35%)]\t training loss: 0.109075\n",
      "epoch: 2 [21440/60000 (36%)]\t training loss: 0.002974\n",
      "epoch: 2 [21760/60000 (36%)]\t training loss: 0.098302\n",
      "epoch: 2 [22080/60000 (37%)]\t training loss: 0.052790\n",
      "epoch: 2 [22400/60000 (37%)]\t training loss: 0.048912\n",
      "epoch: 2 [22720/60000 (38%)]\t training loss: 0.072185\n",
      "epoch: 2 [23040/60000 (38%)]\t training loss: 0.032872\n",
      "epoch: 2 [23360/60000 (39%)]\t training loss: 0.285448\n",
      "epoch: 2 [23680/60000 (39%)]\t training loss: 0.028597\n",
      "epoch: 2 [24000/60000 (40%)]\t training loss: 0.077726\n",
      "epoch: 2 [24320/60000 (41%)]\t training loss: 0.055067\n",
      "epoch: 2 [24640/60000 (41%)]\t training loss: 0.244763\n",
      "epoch: 2 [24960/60000 (42%)]\t training loss: 0.054967\n",
      "epoch: 2 [25280/60000 (42%)]\t training loss: 0.087433\n",
      "epoch: 2 [25600/60000 (43%)]\t training loss: 0.023282\n",
      "epoch: 2 [25920/60000 (43%)]\t training loss: 0.175667\n",
      "epoch: 2 [26240/60000 (44%)]\t training loss: 0.201598\n",
      "epoch: 2 [26560/60000 (44%)]\t training loss: 0.005322\n",
      "epoch: 2 [26880/60000 (45%)]\t training loss: 0.013716\n",
      "epoch: 2 [27200/60000 (45%)]\t training loss: 0.041442\n",
      "epoch: 2 [27520/60000 (46%)]\t training loss: 0.008199\n",
      "epoch: 2 [27840/60000 (46%)]\t training loss: 0.303657\n",
      "epoch: 2 [28160/60000 (47%)]\t training loss: 0.029500\n",
      "epoch: 2 [28480/60000 (47%)]\t training loss: 0.045069\n",
      "epoch: 2 [28800/60000 (48%)]\t training loss: 0.212420\n",
      "epoch: 2 [29120/60000 (49%)]\t training loss: 0.111298\n",
      "epoch: 2 [29440/60000 (49%)]\t training loss: 0.010411\n",
      "epoch: 2 [29760/60000 (50%)]\t training loss: 0.020577\n",
      "epoch: 2 [30080/60000 (50%)]\t training loss: 0.155358\n",
      "epoch: 2 [30400/60000 (51%)]\t training loss: 0.211448\n",
      "epoch: 2 [30720/60000 (51%)]\t training loss: 0.039495\n",
      "epoch: 2 [31040/60000 (52%)]\t training loss: 0.048508\n",
      "epoch: 2 [31360/60000 (52%)]\t training loss: 0.016975\n",
      "epoch: 2 [31680/60000 (53%)]\t training loss: 0.008171\n",
      "epoch: 2 [32000/60000 (53%)]\t training loss: 0.063109\n",
      "epoch: 2 [32320/60000 (54%)]\t training loss: 0.068897\n",
      "epoch: 2 [32640/60000 (54%)]\t training loss: 0.048091\n",
      "epoch: 2 [32960/60000 (55%)]\t training loss: 0.090008\n",
      "epoch: 2 [33280/60000 (55%)]\t training loss: 0.106342\n",
      "epoch: 2 [33600/60000 (56%)]\t training loss: 0.023091\n",
      "epoch: 2 [33920/60000 (57%)]\t training loss: 0.007353\n",
      "epoch: 2 [34240/60000 (57%)]\t training loss: 0.016746\n",
      "epoch: 2 [34560/60000 (58%)]\t training loss: 0.015652\n",
      "epoch: 2 [34880/60000 (58%)]\t training loss: 0.440961\n",
      "epoch: 2 [35200/60000 (59%)]\t training loss: 0.054698\n",
      "epoch: 2 [35520/60000 (59%)]\t training loss: 0.013922\n",
      "epoch: 2 [35840/60000 (60%)]\t training loss: 0.102710\n",
      "epoch: 2 [36160/60000 (60%)]\t training loss: 0.081401\n",
      "epoch: 2 [36480/60000 (61%)]\t training loss: 0.003860\n",
      "epoch: 2 [36800/60000 (61%)]\t training loss: 0.009652\n",
      "epoch: 2 [37120/60000 (62%)]\t training loss: 0.006531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 [37440/60000 (62%)]\t training loss: 0.122305\n",
      "epoch: 2 [37760/60000 (63%)]\t training loss: 0.145930\n",
      "epoch: 2 [38080/60000 (63%)]\t training loss: 0.016439\n",
      "epoch: 2 [38400/60000 (64%)]\t training loss: 0.045070\n",
      "epoch: 2 [38720/60000 (65%)]\t training loss: 0.018864\n",
      "epoch: 2 [39040/60000 (65%)]\t training loss: 0.007952\n",
      "epoch: 2 [39360/60000 (66%)]\t training loss: 0.016014\n",
      "epoch: 2 [39680/60000 (66%)]\t training loss: 0.068150\n",
      "epoch: 2 [40000/60000 (67%)]\t training loss: 0.050566\n",
      "epoch: 2 [40320/60000 (67%)]\t training loss: 0.011531\n",
      "epoch: 2 [40640/60000 (68%)]\t training loss: 0.005553\n",
      "epoch: 2 [40960/60000 (68%)]\t training loss: 0.014355\n",
      "epoch: 2 [41280/60000 (69%)]\t training loss: 0.008815\n",
      "epoch: 2 [41600/60000 (69%)]\t training loss: 0.008510\n",
      "epoch: 2 [41920/60000 (70%)]\t training loss: 0.002213\n",
      "epoch: 2 [42240/60000 (70%)]\t training loss: 0.156662\n",
      "epoch: 2 [42560/60000 (71%)]\t training loss: 0.224040\n",
      "epoch: 2 [42880/60000 (71%)]\t training loss: 0.254498\n",
      "epoch: 2 [43200/60000 (72%)]\t training loss: 0.144746\n",
      "epoch: 2 [43520/60000 (73%)]\t training loss: 0.003408\n",
      "epoch: 2 [43840/60000 (73%)]\t training loss: 0.033842\n",
      "epoch: 2 [44160/60000 (74%)]\t training loss: 0.011321\n",
      "epoch: 2 [44480/60000 (74%)]\t training loss: 0.172604\n",
      "epoch: 2 [44800/60000 (75%)]\t training loss: 0.005733\n",
      "epoch: 2 [45120/60000 (75%)]\t training loss: 0.002113\n",
      "epoch: 2 [45440/60000 (76%)]\t training loss: 0.183646\n",
      "epoch: 2 [45760/60000 (76%)]\t training loss: 0.025354\n",
      "epoch: 2 [46080/60000 (77%)]\t training loss: 0.284551\n",
      "epoch: 2 [46400/60000 (77%)]\t training loss: 0.008812\n",
      "epoch: 2 [46720/60000 (78%)]\t training loss: 0.094086\n",
      "epoch: 2 [47040/60000 (78%)]\t training loss: 0.010397\n",
      "epoch: 2 [47360/60000 (79%)]\t training loss: 0.039547\n",
      "epoch: 2 [47680/60000 (79%)]\t training loss: 0.110431\n",
      "epoch: 2 [48000/60000 (80%)]\t training loss: 0.039305\n",
      "epoch: 2 [48320/60000 (81%)]\t training loss: 0.199064\n",
      "epoch: 2 [48640/60000 (81%)]\t training loss: 0.002353\n",
      "epoch: 2 [48960/60000 (82%)]\t training loss: 0.004221\n",
      "epoch: 2 [49280/60000 (82%)]\t training loss: 0.047369\n",
      "epoch: 2 [49600/60000 (83%)]\t training loss: 0.177750\n",
      "epoch: 2 [49920/60000 (83%)]\t training loss: 0.003290\n",
      "epoch: 2 [50240/60000 (84%)]\t training loss: 0.225221\n",
      "epoch: 2 [50560/60000 (84%)]\t training loss: 0.076244\n",
      "epoch: 2 [50880/60000 (85%)]\t training loss: 0.008935\n",
      "epoch: 2 [51200/60000 (85%)]\t training loss: 0.005751\n",
      "epoch: 2 [51520/60000 (86%)]\t training loss: 0.012980\n",
      "epoch: 2 [51840/60000 (86%)]\t training loss: 0.001507\n",
      "epoch: 2 [52160/60000 (87%)]\t training loss: 0.006742\n",
      "epoch: 2 [52480/60000 (87%)]\t training loss: 0.117215\n",
      "epoch: 2 [52800/60000 (88%)]\t training loss: 0.022851\n",
      "epoch: 2 [53120/60000 (89%)]\t training loss: 0.050420\n",
      "epoch: 2 [53440/60000 (89%)]\t training loss: 0.056454\n",
      "epoch: 2 [53760/60000 (90%)]\t training loss: 0.003290\n",
      "epoch: 2 [54080/60000 (90%)]\t training loss: 0.026371\n",
      "epoch: 2 [54400/60000 (91%)]\t training loss: 0.071585\n",
      "epoch: 2 [54720/60000 (91%)]\t training loss: 0.070469\n",
      "epoch: 2 [55040/60000 (92%)]\t training loss: 0.003723\n",
      "epoch: 2 [55360/60000 (92%)]\t training loss: 0.003024\n",
      "epoch: 2 [55680/60000 (93%)]\t training loss: 0.242654\n",
      "epoch: 2 [56000/60000 (93%)]\t training loss: 0.125871\n",
      "epoch: 2 [56320/60000 (94%)]\t training loss: 0.005245\n",
      "epoch: 2 [56640/60000 (94%)]\t training loss: 0.000520\n",
      "epoch: 2 [56960/60000 (95%)]\t training loss: 0.188250\n",
      "epoch: 2 [57280/60000 (95%)]\t training loss: 0.044744\n",
      "epoch: 2 [57600/60000 (96%)]\t training loss: 0.029123\n",
      "epoch: 2 [57920/60000 (97%)]\t training loss: 0.002372\n",
      "epoch: 2 [58240/60000 (97%)]\t training loss: 0.011676\n",
      "epoch: 2 [58560/60000 (98%)]\t training loss: 0.032765\n",
      "epoch: 2 [58880/60000 (98%)]\t training loss: 0.002476\n",
      "epoch: 2 [59200/60000 (99%)]\t training loss: 0.003405\n",
      "epoch: 2 [59520/60000 (99%)]\t training loss: 0.095505\n",
      "epoch: 2 [59840/60000 (100%)]\t training loss: 0.004768\n",
      "\n",
      "Test dataset: Overall Loss: 0.0401, Overall Accuracy: 9871/10000 (99%)\n",
      "\n",
      "epoch: 3 [0/60000 (0%)]\t training loss: 0.117580\n",
      "epoch: 3 [320/60000 (1%)]\t training loss: 0.003278\n",
      "epoch: 3 [640/60000 (1%)]\t training loss: 0.022930\n",
      "epoch: 3 [960/60000 (2%)]\t training loss: 0.008122\n",
      "epoch: 3 [1280/60000 (2%)]\t training loss: 0.074044\n",
      "epoch: 3 [1600/60000 (3%)]\t training loss: 0.302815\n",
      "epoch: 3 [1920/60000 (3%)]\t training loss: 0.009573\n",
      "epoch: 3 [2240/60000 (4%)]\t training loss: 0.193783\n",
      "epoch: 3 [2560/60000 (4%)]\t training loss: 0.069124\n",
      "epoch: 3 [2880/60000 (5%)]\t training loss: 0.009838\n",
      "epoch: 3 [3200/60000 (5%)]\t training loss: 0.184739\n",
      "epoch: 3 [3520/60000 (6%)]\t training loss: 0.143406\n",
      "epoch: 3 [3840/60000 (6%)]\t training loss: 0.001216\n",
      "epoch: 3 [4160/60000 (7%)]\t training loss: 0.022747\n",
      "epoch: 3 [4480/60000 (7%)]\t training loss: 0.045008\n",
      "epoch: 3 [4800/60000 (8%)]\t training loss: 0.012183\n",
      "epoch: 3 [5120/60000 (9%)]\t training loss: 0.005618\n",
      "epoch: 3 [5440/60000 (9%)]\t training loss: 0.068662\n",
      "epoch: 3 [5760/60000 (10%)]\t training loss: 0.017462\n",
      "epoch: 3 [6080/60000 (10%)]\t training loss: 0.014210\n",
      "epoch: 3 [6400/60000 (11%)]\t training loss: 0.001791\n",
      "epoch: 3 [6720/60000 (11%)]\t training loss: 0.021131\n",
      "epoch: 3 [7040/60000 (12%)]\t training loss: 0.251084\n",
      "epoch: 3 [7360/60000 (12%)]\t training loss: 0.211350\n",
      "epoch: 3 [7680/60000 (13%)]\t training loss: 0.171988\n",
      "epoch: 3 [8000/60000 (13%)]\t training loss: 0.005259\n",
      "epoch: 3 [8320/60000 (14%)]\t training loss: 0.027885\n",
      "epoch: 3 [8640/60000 (14%)]\t training loss: 0.082617\n",
      "epoch: 3 [8960/60000 (15%)]\t training loss: 0.055418\n",
      "epoch: 3 [9280/60000 (15%)]\t training loss: 0.033825\n",
      "epoch: 3 [9600/60000 (16%)]\t training loss: 0.068692\n",
      "epoch: 3 [9920/60000 (17%)]\t training loss: 0.049627\n",
      "epoch: 3 [10240/60000 (17%)]\t training loss: 0.004872\n",
      "epoch: 3 [10560/60000 (18%)]\t training loss: 0.175702\n",
      "epoch: 3 [10880/60000 (18%)]\t training loss: 0.037119\n",
      "epoch: 3 [11200/60000 (19%)]\t training loss: 0.186256\n",
      "epoch: 3 [11520/60000 (19%)]\t training loss: 0.016427\n",
      "epoch: 3 [11840/60000 (20%)]\t training loss: 0.005360\n",
      "epoch: 3 [12160/60000 (20%)]\t training loss: 0.004850\n",
      "epoch: 3 [12480/60000 (21%)]\t training loss: 0.069982\n",
      "epoch: 3 [12800/60000 (21%)]\t training loss: 0.170284\n",
      "epoch: 3 [13120/60000 (22%)]\t training loss: 0.001340\n",
      "epoch: 3 [13440/60000 (22%)]\t training loss: 0.001803\n",
      "epoch: 3 [13760/60000 (23%)]\t training loss: 0.008399\n",
      "epoch: 3 [14080/60000 (23%)]\t training loss: 0.005065\n",
      "epoch: 3 [14400/60000 (24%)]\t training loss: 0.020550\n",
      "epoch: 3 [14720/60000 (25%)]\t training loss: 0.260863\n",
      "epoch: 3 [15040/60000 (25%)]\t training loss: 0.003394\n",
      "epoch: 3 [15360/60000 (26%)]\t training loss: 0.111044\n",
      "epoch: 3 [15680/60000 (26%)]\t training loss: 0.004080\n",
      "epoch: 3 [16000/60000 (27%)]\t training loss: 0.006783\n",
      "epoch: 3 [16320/60000 (27%)]\t training loss: 0.002531\n",
      "epoch: 3 [16640/60000 (28%)]\t training loss: 0.000853\n",
      "epoch: 3 [16960/60000 (28%)]\t training loss: 0.014720\n",
      "epoch: 3 [17280/60000 (29%)]\t training loss: 0.138105\n",
      "epoch: 3 [17600/60000 (29%)]\t training loss: 0.063135\n",
      "epoch: 3 [17920/60000 (30%)]\t training loss: 0.003576\n",
      "epoch: 3 [18240/60000 (30%)]\t training loss: 0.071218\n",
      "epoch: 3 [18560/60000 (31%)]\t training loss: 0.170441\n",
      "epoch: 3 [18880/60000 (31%)]\t training loss: 0.016705\n",
      "epoch: 3 [19200/60000 (32%)]\t training loss: 0.035410\n",
      "epoch: 3 [19520/60000 (33%)]\t training loss: 0.123909\n",
      "epoch: 3 [19840/60000 (33%)]\t training loss: 0.060635\n",
      "epoch: 3 [20160/60000 (34%)]\t training loss: 0.221151\n",
      "epoch: 3 [20480/60000 (34%)]\t training loss: 0.003909\n",
      "epoch: 3 [20800/60000 (35%)]\t training loss: 0.005907\n",
      "epoch: 3 [21120/60000 (35%)]\t training loss: 0.032804\n",
      "epoch: 3 [21440/60000 (36%)]\t training loss: 0.009589\n",
      "epoch: 3 [21760/60000 (36%)]\t training loss: 0.009479\n",
      "epoch: 3 [22080/60000 (37%)]\t training loss: 0.017526\n",
      "epoch: 3 [22400/60000 (37%)]\t training loss: 0.022227\n",
      "epoch: 3 [22720/60000 (38%)]\t training loss: 0.003578\n",
      "epoch: 3 [23040/60000 (38%)]\t training loss: 0.000565\n",
      "epoch: 3 [23360/60000 (39%)]\t training loss: 0.238663\n",
      "epoch: 3 [23680/60000 (39%)]\t training loss: 0.065176\n",
      "epoch: 3 [24000/60000 (40%)]\t training loss: 0.035882\n",
      "epoch: 3 [24320/60000 (41%)]\t training loss: 0.002021\n",
      "epoch: 3 [24640/60000 (41%)]\t training loss: 0.144289\n",
      "epoch: 3 [24960/60000 (42%)]\t training loss: 0.004273\n",
      "epoch: 3 [25280/60000 (42%)]\t training loss: 0.066936\n",
      "epoch: 3 [25600/60000 (43%)]\t training loss: 0.009636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 [25920/60000 (43%)]\t training loss: 0.011488\n",
      "epoch: 3 [26240/60000 (44%)]\t training loss: 0.003887\n",
      "epoch: 3 [26560/60000 (44%)]\t training loss: 0.315628\n",
      "epoch: 3 [26880/60000 (45%)]\t training loss: 0.147676\n",
      "epoch: 3 [27200/60000 (45%)]\t training loss: 0.006523\n",
      "epoch: 3 [27520/60000 (46%)]\t training loss: 0.032355\n",
      "epoch: 3 [27840/60000 (46%)]\t training loss: 0.162399\n",
      "epoch: 3 [28160/60000 (47%)]\t training loss: 0.000650\n",
      "epoch: 3 [28480/60000 (47%)]\t training loss: 0.060941\n",
      "epoch: 3 [28800/60000 (48%)]\t training loss: 0.049059\n",
      "epoch: 3 [29120/60000 (49%)]\t training loss: 0.011951\n",
      "epoch: 3 [29440/60000 (49%)]\t training loss: 0.007248\n",
      "epoch: 3 [29760/60000 (50%)]\t training loss: 0.032459\n",
      "epoch: 3 [30080/60000 (50%)]\t training loss: 0.001153\n",
      "epoch: 3 [30400/60000 (51%)]\t training loss: 0.001172\n",
      "epoch: 3 [30720/60000 (51%)]\t training loss: 0.075565\n",
      "epoch: 3 [31040/60000 (52%)]\t training loss: 0.022946\n",
      "epoch: 3 [31360/60000 (52%)]\t training loss: 0.005423\n",
      "epoch: 3 [31680/60000 (53%)]\t training loss: 0.004720\n",
      "epoch: 3 [32000/60000 (53%)]\t training loss: 0.107121\n",
      "epoch: 3 [32320/60000 (54%)]\t training loss: 0.006401\n",
      "epoch: 3 [32640/60000 (54%)]\t training loss: 0.000888\n",
      "epoch: 3 [32960/60000 (55%)]\t training loss: 0.054888\n",
      "epoch: 3 [33280/60000 (55%)]\t training loss: 0.001098\n",
      "epoch: 3 [33600/60000 (56%)]\t training loss: 0.013894\n",
      "epoch: 3 [33920/60000 (57%)]\t training loss: 0.084992\n",
      "epoch: 3 [34240/60000 (57%)]\t training loss: 0.021688\n",
      "epoch: 3 [34560/60000 (58%)]\t training loss: 0.058899\n",
      "epoch: 3 [34880/60000 (58%)]\t training loss: 0.067690\n",
      "epoch: 3 [35200/60000 (59%)]\t training loss: 0.053615\n",
      "epoch: 3 [35520/60000 (59%)]\t training loss: 0.198723\n",
      "epoch: 3 [35840/60000 (60%)]\t training loss: 0.016806\n",
      "epoch: 3 [36160/60000 (60%)]\t training loss: 0.044431\n",
      "epoch: 3 [36480/60000 (61%)]\t training loss: 0.117965\n",
      "epoch: 3 [36800/60000 (61%)]\t training loss: 0.053264\n",
      "epoch: 3 [37120/60000 (62%)]\t training loss: 0.058884\n",
      "epoch: 3 [37440/60000 (62%)]\t training loss: 0.047894\n",
      "epoch: 3 [37760/60000 (63%)]\t training loss: 0.028838\n",
      "epoch: 3 [38080/60000 (63%)]\t training loss: 0.107215\n",
      "epoch: 3 [38400/60000 (64%)]\t training loss: 0.079754\n",
      "epoch: 3 [38720/60000 (65%)]\t training loss: 0.002141\n",
      "epoch: 3 [39040/60000 (65%)]\t training loss: 0.033317\n",
      "epoch: 3 [39360/60000 (66%)]\t training loss: 0.031559\n",
      "epoch: 3 [39680/60000 (66%)]\t training loss: 0.008555\n",
      "epoch: 3 [40000/60000 (67%)]\t training loss: 0.009716\n",
      "epoch: 3 [40320/60000 (67%)]\t training loss: 0.006519\n",
      "epoch: 3 [40640/60000 (68%)]\t training loss: 0.006280\n",
      "epoch: 3 [40960/60000 (68%)]\t training loss: 0.005860\n",
      "epoch: 3 [41280/60000 (69%)]\t training loss: 0.028329\n",
      "epoch: 3 [41600/60000 (69%)]\t training loss: 0.063689\n",
      "epoch: 3 [41920/60000 (70%)]\t training loss: 0.004370\n",
      "epoch: 3 [42240/60000 (70%)]\t training loss: 0.000868\n",
      "epoch: 3 [42560/60000 (71%)]\t training loss: 0.039717\n",
      "epoch: 3 [42880/60000 (71%)]\t training loss: 0.131979\n",
      "epoch: 3 [43200/60000 (72%)]\t training loss: 0.045734\n",
      "epoch: 3 [43520/60000 (73%)]\t training loss: 0.059374\n",
      "epoch: 3 [43840/60000 (73%)]\t training loss: 0.046793\n",
      "epoch: 3 [44160/60000 (74%)]\t training loss: 0.003413\n",
      "epoch: 3 [44480/60000 (74%)]\t training loss: 0.098559\n",
      "epoch: 3 [44800/60000 (75%)]\t training loss: 0.019069\n",
      "epoch: 3 [45120/60000 (75%)]\t training loss: 0.027830\n",
      "epoch: 3 [45440/60000 (76%)]\t training loss: 0.005980\n",
      "epoch: 3 [45760/60000 (76%)]\t training loss: 0.358821\n",
      "epoch: 3 [46080/60000 (77%)]\t training loss: 0.005387\n",
      "epoch: 3 [46400/60000 (77%)]\t training loss: 0.019085\n",
      "epoch: 3 [46720/60000 (78%)]\t training loss: 0.008419\n",
      "epoch: 3 [47040/60000 (78%)]\t training loss: 0.034136\n",
      "epoch: 3 [47360/60000 (79%)]\t training loss: 0.087363\n",
      "epoch: 3 [47680/60000 (79%)]\t training loss: 0.100079\n",
      "epoch: 3 [48000/60000 (80%)]\t training loss: 0.007467\n",
      "epoch: 3 [48320/60000 (81%)]\t training loss: 0.018951\n",
      "epoch: 3 [48640/60000 (81%)]\t training loss: 0.007282\n",
      "epoch: 3 [48960/60000 (82%)]\t training loss: 0.004417\n",
      "epoch: 3 [49280/60000 (82%)]\t training loss: 0.178678\n",
      "epoch: 3 [49600/60000 (83%)]\t training loss: 0.104473\n",
      "epoch: 3 [49920/60000 (83%)]\t training loss: 0.108430\n",
      "epoch: 3 [50240/60000 (84%)]\t training loss: 0.017439\n",
      "epoch: 3 [50560/60000 (84%)]\t training loss: 0.021110\n",
      "epoch: 3 [50880/60000 (85%)]\t training loss: 0.068198\n",
      "epoch: 3 [51200/60000 (85%)]\t training loss: 0.004673\n",
      "epoch: 3 [51520/60000 (86%)]\t training loss: 0.003408\n",
      "epoch: 3 [51840/60000 (86%)]\t training loss: 0.062133\n",
      "epoch: 3 [52160/60000 (87%)]\t training loss: 0.033689\n",
      "epoch: 3 [52480/60000 (87%)]\t training loss: 0.022669\n",
      "epoch: 3 [52800/60000 (88%)]\t training loss: 0.002386\n",
      "epoch: 3 [53120/60000 (89%)]\t training loss: 0.076253\n",
      "epoch: 3 [53440/60000 (89%)]\t training loss: 0.002208\n",
      "epoch: 3 [53760/60000 (90%)]\t training loss: 0.002066\n",
      "epoch: 3 [54080/60000 (90%)]\t training loss: 0.035759\n",
      "epoch: 3 [54400/60000 (91%)]\t training loss: 0.005854\n",
      "epoch: 3 [54720/60000 (91%)]\t training loss: 0.464891\n",
      "epoch: 3 [55040/60000 (92%)]\t training loss: 0.132868\n",
      "epoch: 3 [55360/60000 (92%)]\t training loss: 0.017058\n",
      "epoch: 3 [55680/60000 (93%)]\t training loss: 0.361732\n",
      "epoch: 3 [56000/60000 (93%)]\t training loss: 0.008855\n",
      "epoch: 3 [56320/60000 (94%)]\t training loss: 0.001362\n",
      "epoch: 3 [56640/60000 (94%)]\t training loss: 0.030658\n",
      "epoch: 3 [56960/60000 (95%)]\t training loss: 0.500097\n",
      "epoch: 3 [57280/60000 (95%)]\t training loss: 0.002589\n",
      "epoch: 3 [57600/60000 (96%)]\t training loss: 0.006339\n",
      "epoch: 3 [57920/60000 (97%)]\t training loss: 0.008814\n",
      "epoch: 3 [58240/60000 (97%)]\t training loss: 0.035940\n",
      "epoch: 3 [58560/60000 (98%)]\t training loss: 0.107217\n",
      "epoch: 3 [58880/60000 (98%)]\t training loss: 0.108878\n",
      "epoch: 3 [59200/60000 (99%)]\t training loss: 0.001573\n",
      "epoch: 3 [59520/60000 (99%)]\t training loss: 0.037652\n",
      "epoch: 3 [59840/60000 (100%)]\t training loss: 0.037789\n",
      "\n",
      "Test dataset: Overall Loss: 0.0334, Overall Accuracy: 9879/10000 (99%)\n",
      "\n",
      "epoch: 4 [0/60000 (0%)]\t training loss: 0.005004\n",
      "epoch: 4 [320/60000 (1%)]\t training loss: 0.005202\n",
      "epoch: 4 [640/60000 (1%)]\t training loss: 0.047373\n",
      "epoch: 4 [960/60000 (2%)]\t training loss: 0.032521\n",
      "epoch: 4 [1280/60000 (2%)]\t training loss: 0.130020\n",
      "epoch: 4 [1600/60000 (3%)]\t training loss: 0.001583\n",
      "epoch: 4 [1920/60000 (3%)]\t training loss: 0.015736\n",
      "epoch: 4 [2240/60000 (4%)]\t training loss: 0.007914\n",
      "epoch: 4 [2560/60000 (4%)]\t training loss: 0.076269\n",
      "epoch: 4 [2880/60000 (5%)]\t training loss: 0.049411\n",
      "epoch: 4 [3200/60000 (5%)]\t training loss: 0.000759\n",
      "epoch: 4 [3520/60000 (6%)]\t training loss: 0.145089\n",
      "epoch: 4 [3840/60000 (6%)]\t training loss: 0.085385\n",
      "epoch: 4 [4160/60000 (7%)]\t training loss: 0.001807\n",
      "epoch: 4 [4480/60000 (7%)]\t training loss: 0.093947\n",
      "epoch: 4 [4800/60000 (8%)]\t training loss: 0.009711\n",
      "epoch: 4 [5120/60000 (9%)]\t training loss: 0.053272\n",
      "epoch: 4 [5440/60000 (9%)]\t training loss: 0.071860\n",
      "epoch: 4 [5760/60000 (10%)]\t training loss: 0.093316\n",
      "epoch: 4 [6080/60000 (10%)]\t training loss: 0.333558\n",
      "epoch: 4 [6400/60000 (11%)]\t training loss: 0.002752\n",
      "epoch: 4 [6720/60000 (11%)]\t training loss: 0.017905\n",
      "epoch: 4 [7040/60000 (12%)]\t training loss: 0.010576\n",
      "epoch: 4 [7360/60000 (12%)]\t training loss: 0.000107\n",
      "epoch: 4 [7680/60000 (13%)]\t training loss: 0.041988\n",
      "epoch: 4 [8000/60000 (13%)]\t training loss: 0.158216\n",
      "epoch: 4 [8320/60000 (14%)]\t training loss: 0.001110\n",
      "epoch: 4 [8640/60000 (14%)]\t training loss: 0.144622\n",
      "epoch: 4 [8960/60000 (15%)]\t training loss: 0.014720\n",
      "epoch: 4 [9280/60000 (15%)]\t training loss: 0.099282\n",
      "epoch: 4 [9600/60000 (16%)]\t training loss: 0.183645\n",
      "epoch: 4 [9920/60000 (17%)]\t training loss: 0.017185\n",
      "epoch: 4 [10240/60000 (17%)]\t training loss: 0.009517\n",
      "epoch: 4 [10560/60000 (18%)]\t training loss: 0.016466\n",
      "epoch: 4 [10880/60000 (18%)]\t training loss: 0.014296\n",
      "epoch: 4 [11200/60000 (19%)]\t training loss: 0.010244\n",
      "epoch: 4 [11520/60000 (19%)]\t training loss: 0.006669\n",
      "epoch: 4 [11840/60000 (20%)]\t training loss: 0.006154\n",
      "epoch: 4 [12160/60000 (20%)]\t training loss: 0.005871\n",
      "epoch: 4 [12480/60000 (21%)]\t training loss: 0.017062\n",
      "epoch: 4 [12800/60000 (21%)]\t training loss: 0.134063\n",
      "epoch: 4 [13120/60000 (22%)]\t training loss: 0.010124\n",
      "epoch: 4 [13440/60000 (22%)]\t training loss: 0.023089\n",
      "epoch: 4 [13760/60000 (23%)]\t training loss: 0.016086\n",
      "epoch: 4 [14080/60000 (23%)]\t training loss: 0.170113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 [14400/60000 (24%)]\t training loss: 0.001610\n",
      "epoch: 4 [14720/60000 (25%)]\t training loss: 0.070175\n",
      "epoch: 4 [15040/60000 (25%)]\t training loss: 0.013863\n",
      "epoch: 4 [15360/60000 (26%)]\t training loss: 0.012433\n",
      "epoch: 4 [15680/60000 (26%)]\t training loss: 0.158234\n",
      "epoch: 4 [16000/60000 (27%)]\t training loss: 0.002333\n",
      "epoch: 4 [16320/60000 (27%)]\t training loss: 0.005482\n",
      "epoch: 4 [16640/60000 (28%)]\t training loss: 0.001946\n",
      "epoch: 4 [16960/60000 (28%)]\t training loss: 0.047215\n",
      "epoch: 4 [17280/60000 (29%)]\t training loss: 0.019467\n",
      "epoch: 4 [17600/60000 (29%)]\t training loss: 0.075563\n",
      "epoch: 4 [17920/60000 (30%)]\t training loss: 0.035971\n",
      "epoch: 4 [18240/60000 (30%)]\t training loss: 0.002394\n",
      "epoch: 4 [18560/60000 (31%)]\t training loss: 0.001120\n",
      "epoch: 4 [18880/60000 (31%)]\t training loss: 0.025516\n",
      "epoch: 4 [19200/60000 (32%)]\t training loss: 0.077274\n",
      "epoch: 4 [19520/60000 (33%)]\t training loss: 0.294797\n",
      "epoch: 4 [19840/60000 (33%)]\t training loss: 0.010043\n",
      "epoch: 4 [20160/60000 (34%)]\t training loss: 0.028039\n",
      "epoch: 4 [20480/60000 (34%)]\t training loss: 0.005619\n",
      "epoch: 4 [20800/60000 (35%)]\t training loss: 0.006589\n",
      "epoch: 4 [21120/60000 (35%)]\t training loss: 0.001882\n",
      "epoch: 4 [21440/60000 (36%)]\t training loss: 0.092774\n",
      "epoch: 4 [21760/60000 (36%)]\t training loss: 0.000599\n",
      "epoch: 4 [22080/60000 (37%)]\t training loss: 0.004264\n",
      "epoch: 4 [22400/60000 (37%)]\t training loss: 0.091383\n",
      "epoch: 4 [22720/60000 (38%)]\t training loss: 0.035040\n",
      "epoch: 4 [23040/60000 (38%)]\t training loss: 0.028779\n",
      "epoch: 4 [23360/60000 (39%)]\t training loss: 0.001068\n",
      "epoch: 4 [23680/60000 (39%)]\t training loss: 0.078600\n",
      "epoch: 4 [24000/60000 (40%)]\t training loss: 0.000748\n",
      "epoch: 4 [24320/60000 (41%)]\t training loss: 0.003989\n",
      "epoch: 4 [24640/60000 (41%)]\t training loss: 0.003464\n",
      "epoch: 4 [24960/60000 (42%)]\t training loss: 0.000540\n",
      "epoch: 4 [25280/60000 (42%)]\t training loss: 0.023129\n",
      "epoch: 4 [25600/60000 (43%)]\t training loss: 0.002896\n",
      "epoch: 4 [25920/60000 (43%)]\t training loss: 0.070928\n",
      "epoch: 4 [26240/60000 (44%)]\t training loss: 0.104639\n",
      "epoch: 4 [26560/60000 (44%)]\t training loss: 0.002139\n",
      "epoch: 4 [26880/60000 (45%)]\t training loss: 0.016782\n",
      "epoch: 4 [27200/60000 (45%)]\t training loss: 0.245871\n",
      "epoch: 4 [27520/60000 (46%)]\t training loss: 0.000968\n",
      "epoch: 4 [27840/60000 (46%)]\t training loss: 0.002824\n",
      "epoch: 4 [28160/60000 (47%)]\t training loss: 0.005639\n",
      "epoch: 4 [28480/60000 (47%)]\t training loss: 0.001080\n",
      "epoch: 4 [28800/60000 (48%)]\t training loss: 0.007457\n",
      "epoch: 4 [29120/60000 (49%)]\t training loss: 0.151841\n",
      "epoch: 4 [29440/60000 (49%)]\t training loss: 0.036772\n",
      "epoch: 4 [29760/60000 (50%)]\t training loss: 0.001384\n",
      "epoch: 4 [30080/60000 (50%)]\t training loss: 0.002531\n",
      "epoch: 4 [30400/60000 (51%)]\t training loss: 0.001964\n",
      "epoch: 4 [30720/60000 (51%)]\t training loss: 0.027992\n",
      "epoch: 4 [31040/60000 (52%)]\t training loss: 0.104327\n",
      "epoch: 4 [31360/60000 (52%)]\t training loss: 0.011168\n",
      "epoch: 4 [31680/60000 (53%)]\t training loss: 0.036652\n",
      "epoch: 4 [32000/60000 (53%)]\t training loss: 0.000749\n",
      "epoch: 4 [32320/60000 (54%)]\t training loss: 0.041235\n",
      "epoch: 4 [32640/60000 (54%)]\t training loss: 0.131390\n",
      "epoch: 4 [32960/60000 (55%)]\t training loss: 0.003560\n",
      "epoch: 4 [33280/60000 (55%)]\t training loss: 0.046083\n",
      "epoch: 4 [33600/60000 (56%)]\t training loss: 0.018471\n",
      "epoch: 4 [33920/60000 (57%)]\t training loss: 0.002393\n",
      "epoch: 4 [34240/60000 (57%)]\t training loss: 0.018845\n",
      "epoch: 4 [34560/60000 (58%)]\t training loss: 0.002453\n",
      "epoch: 4 [34880/60000 (58%)]\t training loss: 0.280431\n",
      "epoch: 4 [35200/60000 (59%)]\t training loss: 0.241689\n",
      "epoch: 4 [35520/60000 (59%)]\t training loss: 0.030137\n",
      "epoch: 4 [35840/60000 (60%)]\t training loss: 0.000647\n",
      "epoch: 4 [36160/60000 (60%)]\t training loss: 0.012818\n",
      "epoch: 4 [36480/60000 (61%)]\t training loss: 0.001206\n",
      "epoch: 4 [36800/60000 (61%)]\t training loss: 0.091806\n",
      "epoch: 4 [37120/60000 (62%)]\t training loss: 0.029830\n",
      "epoch: 4 [37440/60000 (62%)]\t training loss: 0.027802\n",
      "epoch: 4 [37760/60000 (63%)]\t training loss: 0.021126\n",
      "epoch: 4 [38080/60000 (63%)]\t training loss: 0.001772\n",
      "epoch: 4 [38400/60000 (64%)]\t training loss: 0.003052\n",
      "epoch: 4 [38720/60000 (65%)]\t training loss: 0.000742\n",
      "epoch: 4 [39040/60000 (65%)]\t training loss: 0.020241\n",
      "epoch: 4 [39360/60000 (66%)]\t training loss: 0.097239\n",
      "epoch: 4 [39680/60000 (66%)]\t training loss: 0.162862\n",
      "epoch: 4 [40000/60000 (67%)]\t training loss: 0.000209\n",
      "epoch: 4 [40320/60000 (67%)]\t training loss: 0.002302\n",
      "epoch: 4 [40640/60000 (68%)]\t training loss: 0.005299\n",
      "epoch: 4 [40960/60000 (68%)]\t training loss: 0.047017\n",
      "epoch: 4 [41280/60000 (69%)]\t training loss: 0.002345\n",
      "epoch: 4 [41600/60000 (69%)]\t training loss: 0.002102\n",
      "epoch: 4 [41920/60000 (70%)]\t training loss: 0.012853\n",
      "epoch: 4 [42240/60000 (70%)]\t training loss: 0.002341\n",
      "epoch: 4 [42560/60000 (71%)]\t training loss: 0.155154\n",
      "epoch: 4 [42880/60000 (71%)]\t training loss: 0.004047\n",
      "epoch: 4 [43200/60000 (72%)]\t training loss: 0.002115\n",
      "epoch: 4 [43520/60000 (73%)]\t training loss: 0.000279\n",
      "epoch: 4 [43840/60000 (73%)]\t training loss: 0.217341\n",
      "epoch: 4 [44160/60000 (74%)]\t training loss: 0.026985\n",
      "epoch: 4 [44480/60000 (74%)]\t training loss: 0.002457\n",
      "epoch: 4 [44800/60000 (75%)]\t training loss: 0.315962\n",
      "epoch: 4 [45120/60000 (75%)]\t training loss: 0.005291\n",
      "epoch: 4 [45440/60000 (76%)]\t training loss: 0.005015\n",
      "epoch: 4 [45760/60000 (76%)]\t training loss: 0.001301\n",
      "epoch: 4 [46080/60000 (77%)]\t training loss: 0.012672\n",
      "epoch: 4 [46400/60000 (77%)]\t training loss: 0.020146\n",
      "epoch: 4 [46720/60000 (78%)]\t training loss: 0.010617\n",
      "epoch: 4 [47040/60000 (78%)]\t training loss: 0.104126\n",
      "epoch: 4 [47360/60000 (79%)]\t training loss: 0.046990\n",
      "epoch: 4 [47680/60000 (79%)]\t training loss: 0.003854\n",
      "epoch: 4 [48000/60000 (80%)]\t training loss: 0.010838\n",
      "epoch: 4 [48320/60000 (81%)]\t training loss: 0.001802\n",
      "epoch: 4 [48640/60000 (81%)]\t training loss: 0.001541\n",
      "epoch: 4 [48960/60000 (82%)]\t training loss: 0.267227\n",
      "epoch: 4 [49280/60000 (82%)]\t training loss: 0.002826\n",
      "epoch: 4 [49600/60000 (83%)]\t training loss: 0.003209\n",
      "epoch: 4 [49920/60000 (83%)]\t training loss: 0.010734\n",
      "epoch: 4 [50240/60000 (84%)]\t training loss: 0.005324\n",
      "epoch: 4 [50560/60000 (84%)]\t training loss: 0.253788\n",
      "epoch: 4 [50880/60000 (85%)]\t training loss: 0.000709\n",
      "epoch: 4 [51200/60000 (85%)]\t training loss: 0.036714\n",
      "epoch: 4 [51520/60000 (86%)]\t training loss: 0.092385\n",
      "epoch: 4 [51840/60000 (86%)]\t training loss: 0.005129\n",
      "epoch: 4 [52160/60000 (87%)]\t training loss: 0.002564\n",
      "epoch: 4 [52480/60000 (87%)]\t training loss: 0.016012\n",
      "epoch: 4 [52800/60000 (88%)]\t training loss: 0.008383\n",
      "epoch: 4 [53120/60000 (89%)]\t training loss: 0.024948\n",
      "epoch: 4 [53440/60000 (89%)]\t training loss: 0.000843\n",
      "epoch: 4 [53760/60000 (90%)]\t training loss: 0.029645\n",
      "epoch: 4 [54080/60000 (90%)]\t training loss: 0.024851\n",
      "epoch: 4 [54400/60000 (91%)]\t training loss: 0.006790\n",
      "epoch: 4 [54720/60000 (91%)]\t training loss: 0.001100\n",
      "epoch: 4 [55040/60000 (92%)]\t training loss: 0.006912\n",
      "epoch: 4 [55360/60000 (92%)]\t training loss: 0.000658\n",
      "epoch: 4 [55680/60000 (93%)]\t training loss: 0.000735\n",
      "epoch: 4 [56000/60000 (93%)]\t training loss: 0.002781\n",
      "epoch: 4 [56320/60000 (94%)]\t training loss: 0.121837\n",
      "epoch: 4 [56640/60000 (94%)]\t training loss: 0.039940\n",
      "epoch: 4 [56960/60000 (95%)]\t training loss: 0.186422\n",
      "epoch: 4 [57280/60000 (95%)]\t training loss: 0.212681\n",
      "epoch: 4 [57600/60000 (96%)]\t training loss: 0.018788\n",
      "epoch: 4 [57920/60000 (97%)]\t training loss: 0.000949\n",
      "epoch: 4 [58240/60000 (97%)]\t training loss: 0.001067\n",
      "epoch: 4 [58560/60000 (98%)]\t training loss: 0.050781\n",
      "epoch: 4 [58880/60000 (98%)]\t training loss: 0.003623\n",
      "epoch: 4 [59200/60000 (99%)]\t training loss: 0.000867\n",
      "epoch: 4 [59520/60000 (99%)]\t training loss: 0.042586\n",
      "epoch: 4 [59840/60000 (100%)]\t training loss: 0.002554\n",
      "\n",
      "Test dataset: Overall Loss: 0.0367, Overall Accuracy: 9886/10000 (99%)\n",
      "\n",
      "epoch: 5 [0/60000 (0%)]\t training loss: 0.010070\n",
      "epoch: 5 [320/60000 (1%)]\t training loss: 0.003787\n",
      "epoch: 5 [640/60000 (1%)]\t training loss: 0.017631\n",
      "epoch: 5 [960/60000 (2%)]\t training loss: 0.069232\n",
      "epoch: 5 [1280/60000 (2%)]\t training loss: 0.088592\n",
      "epoch: 5 [1600/60000 (3%)]\t training loss: 0.016618\n",
      "epoch: 5 [1920/60000 (3%)]\t training loss: 0.086539\n",
      "epoch: 5 [2240/60000 (4%)]\t training loss: 0.001964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 [2560/60000 (4%)]\t training loss: 0.001106\n",
      "epoch: 5 [2880/60000 (5%)]\t training loss: 0.006430\n",
      "epoch: 5 [3200/60000 (5%)]\t training loss: 0.006645\n",
      "epoch: 5 [3520/60000 (6%)]\t training loss: 0.064642\n",
      "epoch: 5 [3840/60000 (6%)]\t training loss: 0.000591\n",
      "epoch: 5 [4160/60000 (7%)]\t training loss: 0.006640\n",
      "epoch: 5 [4480/60000 (7%)]\t training loss: 0.021118\n",
      "epoch: 5 [4800/60000 (8%)]\t training loss: 0.009449\n",
      "epoch: 5 [5120/60000 (9%)]\t training loss: 0.011730\n",
      "epoch: 5 [5440/60000 (9%)]\t training loss: 0.001851\n",
      "epoch: 5 [5760/60000 (10%)]\t training loss: 0.114151\n",
      "epoch: 5 [6080/60000 (10%)]\t training loss: 0.040879\n",
      "epoch: 5 [6400/60000 (11%)]\t training loss: 0.038032\n",
      "epoch: 5 [6720/60000 (11%)]\t training loss: 0.005098\n",
      "epoch: 5 [7040/60000 (12%)]\t training loss: 0.078207\n",
      "epoch: 5 [7360/60000 (12%)]\t training loss: 0.005355\n",
      "epoch: 5 [7680/60000 (13%)]\t training loss: 0.002868\n",
      "epoch: 5 [8000/60000 (13%)]\t training loss: 0.012961\n",
      "epoch: 5 [8320/60000 (14%)]\t training loss: 0.072866\n",
      "epoch: 5 [8640/60000 (14%)]\t training loss: 0.014431\n",
      "epoch: 5 [8960/60000 (15%)]\t training loss: 0.000292\n",
      "epoch: 5 [9280/60000 (15%)]\t training loss: 0.076717\n",
      "epoch: 5 [9600/60000 (16%)]\t training loss: 0.000401\n",
      "epoch: 5 [9920/60000 (17%)]\t training loss: 0.000572\n",
      "epoch: 5 [10240/60000 (17%)]\t training loss: 0.012219\n",
      "epoch: 5 [10560/60000 (18%)]\t training loss: 0.016286\n",
      "epoch: 5 [10880/60000 (18%)]\t training loss: 0.127803\n",
      "epoch: 5 [11200/60000 (19%)]\t training loss: 0.084620\n",
      "epoch: 5 [11520/60000 (19%)]\t training loss: 0.004515\n",
      "epoch: 5 [11840/60000 (20%)]\t training loss: 0.094172\n",
      "epoch: 5 [12160/60000 (20%)]\t training loss: 0.182865\n",
      "epoch: 5 [12480/60000 (21%)]\t training loss: 0.002302\n",
      "epoch: 5 [12800/60000 (21%)]\t training loss: 0.000824\n",
      "epoch: 5 [13120/60000 (22%)]\t training loss: 0.060730\n",
      "epoch: 5 [13440/60000 (22%)]\t training loss: 0.022876\n",
      "epoch: 5 [13760/60000 (23%)]\t training loss: 0.001682\n",
      "epoch: 5 [14080/60000 (23%)]\t training loss: 0.022452\n",
      "epoch: 5 [14400/60000 (24%)]\t training loss: 0.003395\n",
      "epoch: 5 [14720/60000 (25%)]\t training loss: 0.004322\n",
      "epoch: 5 [15040/60000 (25%)]\t training loss: 0.000247\n",
      "epoch: 5 [15360/60000 (26%)]\t training loss: 0.002021\n",
      "epoch: 5 [15680/60000 (26%)]\t training loss: 0.069937\n",
      "epoch: 5 [16000/60000 (27%)]\t training loss: 0.000197\n",
      "epoch: 5 [16320/60000 (27%)]\t training loss: 0.000250\n",
      "epoch: 5 [16640/60000 (28%)]\t training loss: 0.025561\n",
      "epoch: 5 [16960/60000 (28%)]\t training loss: 0.006471\n",
      "epoch: 5 [17280/60000 (29%)]\t training loss: 0.001989\n",
      "epoch: 5 [17600/60000 (29%)]\t training loss: 0.005772\n",
      "epoch: 5 [17920/60000 (30%)]\t training loss: 0.002549\n",
      "epoch: 5 [18240/60000 (30%)]\t training loss: 0.089210\n",
      "epoch: 5 [18560/60000 (31%)]\t training loss: 0.002986\n",
      "epoch: 5 [18880/60000 (31%)]\t training loss: 0.000883\n",
      "epoch: 5 [19200/60000 (32%)]\t training loss: 0.007099\n",
      "epoch: 5 [19520/60000 (33%)]\t training loss: 0.060222\n",
      "epoch: 5 [19840/60000 (33%)]\t training loss: 0.002088\n",
      "epoch: 5 [20160/60000 (34%)]\t training loss: 0.001858\n",
      "epoch: 5 [20480/60000 (34%)]\t training loss: 0.001986\n",
      "epoch: 5 [20800/60000 (35%)]\t training loss: 0.002755\n",
      "epoch: 5 [21120/60000 (35%)]\t training loss: 0.009408\n",
      "epoch: 5 [21440/60000 (36%)]\t training loss: 0.018641\n",
      "epoch: 5 [21760/60000 (36%)]\t training loss: 0.048414\n",
      "epoch: 5 [22080/60000 (37%)]\t training loss: 0.007540\n",
      "epoch: 5 [22400/60000 (37%)]\t training loss: 0.193058\n",
      "epoch: 5 [22720/60000 (38%)]\t training loss: 0.002270\n",
      "epoch: 5 [23040/60000 (38%)]\t training loss: 0.011743\n",
      "epoch: 5 [23360/60000 (39%)]\t training loss: 0.003912\n",
      "epoch: 5 [23680/60000 (39%)]\t training loss: 0.003179\n",
      "epoch: 5 [24000/60000 (40%)]\t training loss: 0.056040\n",
      "epoch: 5 [24320/60000 (41%)]\t training loss: 0.004919\n",
      "epoch: 5 [24640/60000 (41%)]\t training loss: 0.005078\n",
      "epoch: 5 [24960/60000 (42%)]\t training loss: 0.000142\n",
      "epoch: 5 [25280/60000 (42%)]\t training loss: 0.105995\n",
      "epoch: 5 [25600/60000 (43%)]\t training loss: 0.056802\n",
      "epoch: 5 [25920/60000 (43%)]\t training loss: 0.247514\n",
      "epoch: 5 [26240/60000 (44%)]\t training loss: 0.377228\n",
      "epoch: 5 [26560/60000 (44%)]\t training loss: 0.002052\n",
      "epoch: 5 [26880/60000 (45%)]\t training loss: 0.007325\n",
      "epoch: 5 [27200/60000 (45%)]\t training loss: 0.001216\n",
      "epoch: 5 [27520/60000 (46%)]\t training loss: 0.020070\n",
      "epoch: 5 [27840/60000 (46%)]\t training loss: 0.016777\n",
      "epoch: 5 [28160/60000 (47%)]\t training loss: 0.157246\n",
      "epoch: 5 [28480/60000 (47%)]\t training loss: 0.002588\n",
      "epoch: 5 [28800/60000 (48%)]\t training loss: 0.015895\n",
      "epoch: 5 [29120/60000 (49%)]\t training loss: 0.069824\n",
      "epoch: 5 [29440/60000 (49%)]\t training loss: 0.003901\n",
      "epoch: 5 [29760/60000 (50%)]\t training loss: 0.001185\n",
      "epoch: 5 [30080/60000 (50%)]\t training loss: 0.000616\n",
      "epoch: 5 [30400/60000 (51%)]\t training loss: 0.000991\n",
      "epoch: 5 [30720/60000 (51%)]\t training loss: 0.008619\n",
      "epoch: 5 [31040/60000 (52%)]\t training loss: 0.210741\n",
      "epoch: 5 [31360/60000 (52%)]\t training loss: 0.027488\n",
      "epoch: 5 [31680/60000 (53%)]\t training loss: 0.006754\n",
      "epoch: 5 [32000/60000 (53%)]\t training loss: 0.071987\n",
      "epoch: 5 [32320/60000 (54%)]\t training loss: 0.000210\n",
      "epoch: 5 [32640/60000 (54%)]\t training loss: 0.000323\n",
      "epoch: 5 [32960/60000 (55%)]\t training loss: 0.638587\n",
      "epoch: 5 [33280/60000 (55%)]\t training loss: 0.002367\n",
      "epoch: 5 [33600/60000 (56%)]\t training loss: 0.003660\n",
      "epoch: 5 [33920/60000 (57%)]\t training loss: 0.002062\n",
      "epoch: 5 [34240/60000 (57%)]\t training loss: 0.023973\n",
      "epoch: 5 [34560/60000 (58%)]\t training loss: 0.000103\n",
      "epoch: 5 [34880/60000 (58%)]\t training loss: 0.001832\n",
      "epoch: 5 [35200/60000 (59%)]\t training loss: 0.003550\n",
      "epoch: 5 [35520/60000 (59%)]\t training loss: 0.002394\n",
      "epoch: 5 [35840/60000 (60%)]\t training loss: 0.006140\n",
      "epoch: 5 [36160/60000 (60%)]\t training loss: 0.001372\n",
      "epoch: 5 [36480/60000 (61%)]\t training loss: 0.001334\n",
      "epoch: 5 [36800/60000 (61%)]\t training loss: 0.009840\n",
      "epoch: 5 [37120/60000 (62%)]\t training loss: 0.001075\n",
      "epoch: 5 [37440/60000 (62%)]\t training loss: 0.044095\n",
      "epoch: 5 [37760/60000 (63%)]\t training loss: 0.152259\n",
      "epoch: 5 [38080/60000 (63%)]\t training loss: 0.044580\n",
      "epoch: 5 [38400/60000 (64%)]\t training loss: 0.000593\n",
      "epoch: 5 [38720/60000 (65%)]\t training loss: 0.113967\n",
      "epoch: 5 [39040/60000 (65%)]\t training loss: 0.001306\n",
      "epoch: 5 [39360/60000 (66%)]\t training loss: 0.022270\n",
      "epoch: 5 [39680/60000 (66%)]\t training loss: 0.019256\n",
      "epoch: 5 [40000/60000 (67%)]\t training loss: 0.043041\n",
      "epoch: 5 [40320/60000 (67%)]\t training loss: 0.034269\n",
      "epoch: 5 [40640/60000 (68%)]\t training loss: 0.039250\n",
      "epoch: 5 [40960/60000 (68%)]\t training loss: 0.036557\n",
      "epoch: 5 [41280/60000 (69%)]\t training loss: 0.062584\n",
      "epoch: 5 [41600/60000 (69%)]\t training loss: 0.007238\n",
      "epoch: 5 [41920/60000 (70%)]\t training loss: 0.050754\n",
      "epoch: 5 [42240/60000 (70%)]\t training loss: 0.007195\n",
      "epoch: 5 [42560/60000 (71%)]\t training loss: 0.013320\n",
      "epoch: 5 [42880/60000 (71%)]\t training loss: 0.005214\n",
      "epoch: 5 [43200/60000 (72%)]\t training loss: 0.001549\n",
      "epoch: 5 [43520/60000 (73%)]\t training loss: 0.006244\n",
      "epoch: 5 [43840/60000 (73%)]\t training loss: 0.015841\n",
      "epoch: 5 [44160/60000 (74%)]\t training loss: 0.086212\n",
      "epoch: 5 [44480/60000 (74%)]\t training loss: 0.000793\n",
      "epoch: 5 [44800/60000 (75%)]\t training loss: 0.018930\n",
      "epoch: 5 [45120/60000 (75%)]\t training loss: 0.102105\n",
      "epoch: 5 [45440/60000 (76%)]\t training loss: 0.000339\n",
      "epoch: 5 [45760/60000 (76%)]\t training loss: 0.024094\n",
      "epoch: 5 [46080/60000 (77%)]\t training loss: 0.013268\n",
      "epoch: 5 [46400/60000 (77%)]\t training loss: 0.000367\n",
      "epoch: 5 [46720/60000 (78%)]\t training loss: 0.163219\n",
      "epoch: 5 [47040/60000 (78%)]\t training loss: 0.000053\n",
      "epoch: 5 [47360/60000 (79%)]\t training loss: 0.013798\n",
      "epoch: 5 [47680/60000 (79%)]\t training loss: 0.009742\n",
      "epoch: 5 [48000/60000 (80%)]\t training loss: 0.011169\n",
      "epoch: 5 [48320/60000 (81%)]\t training loss: 0.002052\n",
      "epoch: 5 [48640/60000 (81%)]\t training loss: 0.011809\n",
      "epoch: 5 [48960/60000 (82%)]\t training loss: 0.007397\n",
      "epoch: 5 [49280/60000 (82%)]\t training loss: 0.003807\n",
      "epoch: 5 [49600/60000 (83%)]\t training loss: 0.001376\n",
      "epoch: 5 [49920/60000 (83%)]\t training loss: 0.005858\n",
      "epoch: 5 [50240/60000 (84%)]\t training loss: 0.553323\n",
      "epoch: 5 [50560/60000 (84%)]\t training loss: 0.001847\n",
      "epoch: 5 [50880/60000 (85%)]\t training loss: 0.067193\n",
      "epoch: 5 [51200/60000 (85%)]\t training loss: 0.070244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 [51520/60000 (86%)]\t training loss: 0.001317\n",
      "epoch: 5 [51840/60000 (86%)]\t training loss: 0.095925\n",
      "epoch: 5 [52160/60000 (87%)]\t training loss: 0.004173\n",
      "epoch: 5 [52480/60000 (87%)]\t training loss: 0.001185\n",
      "epoch: 5 [52800/60000 (88%)]\t training loss: 0.003036\n",
      "epoch: 5 [53120/60000 (89%)]\t training loss: 0.021657\n",
      "epoch: 5 [53440/60000 (89%)]\t training loss: 0.000199\n",
      "epoch: 5 [53760/60000 (90%)]\t training loss: 0.001553\n",
      "epoch: 5 [54080/60000 (90%)]\t training loss: 0.013963\n",
      "epoch: 5 [54400/60000 (91%)]\t training loss: 0.020786\n",
      "epoch: 5 [54720/60000 (91%)]\t training loss: 0.008905\n",
      "epoch: 5 [55040/60000 (92%)]\t training loss: 0.039044\n",
      "epoch: 5 [55360/60000 (92%)]\t training loss: 0.062111\n",
      "epoch: 5 [55680/60000 (93%)]\t training loss: 0.044069\n",
      "epoch: 5 [56000/60000 (93%)]\t training loss: 0.293081\n",
      "epoch: 5 [56320/60000 (94%)]\t training loss: 0.001521\n",
      "epoch: 5 [56640/60000 (94%)]\t training loss: 0.044059\n",
      "epoch: 5 [56960/60000 (95%)]\t training loss: 0.002164\n",
      "epoch: 5 [57280/60000 (95%)]\t training loss: 0.019973\n",
      "epoch: 5 [57600/60000 (96%)]\t training loss: 0.002672\n",
      "epoch: 5 [57920/60000 (97%)]\t training loss: 0.067601\n",
      "epoch: 5 [58240/60000 (97%)]\t training loss: 0.153791\n",
      "epoch: 5 [58560/60000 (98%)]\t training loss: 0.128535\n",
      "epoch: 5 [58880/60000 (98%)]\t training loss: 0.355670\n",
      "epoch: 5 [59200/60000 (99%)]\t training loss: 0.156918\n",
      "epoch: 5 [59520/60000 (99%)]\t training loss: 0.007529\n",
      "epoch: 5 [59840/60000 (100%)]\t training loss: 0.057660\n",
      "\n",
      "Test dataset: Overall Loss: 0.0332, Overall Accuracy: 9891/10000 (99%)\n",
      "\n",
      "epoch: 6 [0/60000 (0%)]\t training loss: 0.000166\n",
      "epoch: 6 [320/60000 (1%)]\t training loss: 0.002151\n",
      "epoch: 6 [640/60000 (1%)]\t training loss: 0.003457\n",
      "epoch: 6 [960/60000 (2%)]\t training loss: 0.003728\n",
      "epoch: 6 [1280/60000 (2%)]\t training loss: 0.049435\n",
      "epoch: 6 [1600/60000 (3%)]\t training loss: 0.004021\n",
      "epoch: 6 [1920/60000 (3%)]\t training loss: 0.005528\n",
      "epoch: 6 [2240/60000 (4%)]\t training loss: 0.003417\n",
      "epoch: 6 [2560/60000 (4%)]\t training loss: 0.303697\n",
      "epoch: 6 [2880/60000 (5%)]\t training loss: 0.021284\n",
      "epoch: 6 [3200/60000 (5%)]\t training loss: 0.035803\n",
      "epoch: 6 [3520/60000 (6%)]\t training loss: 0.001704\n",
      "epoch: 6 [3840/60000 (6%)]\t training loss: 0.000942\n",
      "epoch: 6 [4160/60000 (7%)]\t training loss: 0.000470\n",
      "epoch: 6 [4480/60000 (7%)]\t training loss: 0.028361\n",
      "epoch: 6 [4800/60000 (8%)]\t training loss: 0.145959\n",
      "epoch: 6 [5120/60000 (9%)]\t training loss: 0.001184\n",
      "epoch: 6 [5440/60000 (9%)]\t training loss: 0.014698\n",
      "epoch: 6 [5760/60000 (10%)]\t training loss: 0.001966\n",
      "epoch: 6 [6080/60000 (10%)]\t training loss: 0.003562\n",
      "epoch: 6 [6400/60000 (11%)]\t training loss: 0.010852\n",
      "epoch: 6 [6720/60000 (11%)]\t training loss: 0.001657\n",
      "epoch: 6 [7040/60000 (12%)]\t training loss: 0.000213\n",
      "epoch: 6 [7360/60000 (12%)]\t training loss: 0.146847\n",
      "epoch: 6 [7680/60000 (13%)]\t training loss: 0.036499\n",
      "epoch: 6 [8000/60000 (13%)]\t training loss: 0.034782\n",
      "epoch: 6 [8320/60000 (14%)]\t training loss: 0.003107\n",
      "epoch: 6 [8640/60000 (14%)]\t training loss: 0.131457\n",
      "epoch: 6 [8960/60000 (15%)]\t training loss: 0.578220\n",
      "epoch: 6 [9280/60000 (15%)]\t training loss: 0.000701\n",
      "epoch: 6 [9600/60000 (16%)]\t training loss: 0.044859\n",
      "epoch: 6 [9920/60000 (17%)]\t training loss: 0.021785\n",
      "epoch: 6 [10240/60000 (17%)]\t training loss: 0.054060\n",
      "epoch: 6 [10560/60000 (18%)]\t training loss: 0.003195\n",
      "epoch: 6 [10880/60000 (18%)]\t training loss: 0.001612\n",
      "epoch: 6 [11200/60000 (19%)]\t training loss: 0.392747\n",
      "epoch: 6 [11520/60000 (19%)]\t training loss: 0.002520\n",
      "epoch: 6 [11840/60000 (20%)]\t training loss: 0.000488\n",
      "epoch: 6 [12160/60000 (20%)]\t training loss: 0.027585\n",
      "epoch: 6 [12480/60000 (21%)]\t training loss: 0.001046\n",
      "epoch: 6 [12800/60000 (21%)]\t training loss: 0.000284\n",
      "epoch: 6 [13120/60000 (22%)]\t training loss: 0.031416\n",
      "epoch: 6 [13440/60000 (22%)]\t training loss: 0.003102\n",
      "epoch: 6 [13760/60000 (23%)]\t training loss: 0.004981\n",
      "epoch: 6 [14080/60000 (23%)]\t training loss: 0.003102\n",
      "epoch: 6 [14400/60000 (24%)]\t training loss: 0.019235\n",
      "epoch: 6 [14720/60000 (25%)]\t training loss: 0.014464\n",
      "epoch: 6 [15040/60000 (25%)]\t training loss: 0.047742\n",
      "epoch: 6 [15360/60000 (26%)]\t training loss: 0.003669\n",
      "epoch: 6 [15680/60000 (26%)]\t training loss: 0.020421\n",
      "epoch: 6 [16000/60000 (27%)]\t training loss: 0.080839\n",
      "epoch: 6 [16320/60000 (27%)]\t training loss: 0.013257\n",
      "epoch: 6 [16640/60000 (28%)]\t training loss: 0.035571\n",
      "epoch: 6 [16960/60000 (28%)]\t training loss: 0.001471\n",
      "epoch: 6 [17280/60000 (29%)]\t training loss: 0.010400\n",
      "epoch: 6 [17600/60000 (29%)]\t training loss: 0.029003\n",
      "epoch: 6 [17920/60000 (30%)]\t training loss: 0.061628\n",
      "epoch: 6 [18240/60000 (30%)]\t training loss: 0.000638\n",
      "epoch: 6 [18560/60000 (31%)]\t training loss: 0.000728\n",
      "epoch: 6 [18880/60000 (31%)]\t training loss: 0.002096\n",
      "epoch: 6 [19200/60000 (32%)]\t training loss: 0.019266\n",
      "epoch: 6 [19520/60000 (33%)]\t training loss: 0.003558\n",
      "epoch: 6 [19840/60000 (33%)]\t training loss: 0.008111\n",
      "epoch: 6 [20160/60000 (34%)]\t training loss: 0.008497\n",
      "epoch: 6 [20480/60000 (34%)]\t training loss: 0.038874\n",
      "epoch: 6 [20800/60000 (35%)]\t training loss: 0.006100\n",
      "epoch: 6 [21120/60000 (35%)]\t training loss: 0.008987\n",
      "epoch: 6 [21440/60000 (36%)]\t training loss: 0.001117\n",
      "epoch: 6 [21760/60000 (36%)]\t training loss: 0.034135\n",
      "epoch: 6 [22080/60000 (37%)]\t training loss: 0.273590\n",
      "epoch: 6 [22400/60000 (37%)]\t training loss: 0.049186\n",
      "epoch: 6 [22720/60000 (38%)]\t training loss: 0.016794\n",
      "epoch: 6 [23040/60000 (38%)]\t training loss: 0.001200\n",
      "epoch: 6 [23360/60000 (39%)]\t training loss: 0.004484\n",
      "epoch: 6 [23680/60000 (39%)]\t training loss: 0.000455\n",
      "epoch: 6 [24000/60000 (40%)]\t training loss: 0.084878\n",
      "epoch: 6 [24320/60000 (41%)]\t training loss: 0.000726\n",
      "epoch: 6 [24640/60000 (41%)]\t training loss: 0.024057\n",
      "epoch: 6 [24960/60000 (42%)]\t training loss: 0.000174\n",
      "epoch: 6 [25280/60000 (42%)]\t training loss: 0.057778\n",
      "epoch: 6 [25600/60000 (43%)]\t training loss: 0.006446\n",
      "epoch: 6 [25920/60000 (43%)]\t training loss: 0.006433\n",
      "epoch: 6 [26240/60000 (44%)]\t training loss: 0.000757\n",
      "epoch: 6 [26560/60000 (44%)]\t training loss: 0.150580\n",
      "epoch: 6 [26880/60000 (45%)]\t training loss: 0.000326\n",
      "epoch: 6 [27200/60000 (45%)]\t training loss: 0.173261\n",
      "epoch: 6 [27520/60000 (46%)]\t training loss: 0.145251\n",
      "epoch: 6 [27840/60000 (46%)]\t training loss: 0.000368\n",
      "epoch: 6 [28160/60000 (47%)]\t training loss: 0.002509\n",
      "epoch: 6 [28480/60000 (47%)]\t training loss: 0.000604\n",
      "epoch: 6 [28800/60000 (48%)]\t training loss: 0.000733\n",
      "epoch: 6 [29120/60000 (49%)]\t training loss: 0.016024\n",
      "epoch: 6 [29440/60000 (49%)]\t training loss: 0.116027\n",
      "epoch: 6 [29760/60000 (50%)]\t training loss: 0.196762\n",
      "epoch: 6 [30080/60000 (50%)]\t training loss: 0.028154\n",
      "epoch: 6 [30400/60000 (51%)]\t training loss: 0.001057\n",
      "epoch: 6 [30720/60000 (51%)]\t training loss: 0.002292\n",
      "epoch: 6 [31040/60000 (52%)]\t training loss: 0.003710\n",
      "epoch: 6 [31360/60000 (52%)]\t training loss: 0.018203\n",
      "epoch: 6 [31680/60000 (53%)]\t training loss: 0.011580\n",
      "epoch: 6 [32000/60000 (53%)]\t training loss: 0.029023\n",
      "epoch: 6 [32320/60000 (54%)]\t training loss: 0.032205\n",
      "epoch: 6 [32640/60000 (54%)]\t training loss: 0.012061\n",
      "epoch: 6 [32960/60000 (55%)]\t training loss: 0.000443\n",
      "epoch: 6 [33280/60000 (55%)]\t training loss: 0.051385\n",
      "epoch: 6 [33600/60000 (56%)]\t training loss: 0.041531\n",
      "epoch: 6 [33920/60000 (57%)]\t training loss: 0.000334\n",
      "epoch: 6 [34240/60000 (57%)]\t training loss: 0.010923\n",
      "epoch: 6 [34560/60000 (58%)]\t training loss: 0.007128\n",
      "epoch: 6 [34880/60000 (58%)]\t training loss: 0.003526\n",
      "epoch: 6 [35200/60000 (59%)]\t training loss: 0.007068\n",
      "epoch: 6 [35520/60000 (59%)]\t training loss: 0.000362\n",
      "epoch: 6 [35840/60000 (60%)]\t training loss: 0.075202\n",
      "epoch: 6 [36160/60000 (60%)]\t training loss: 0.013428\n",
      "epoch: 6 [36480/60000 (61%)]\t training loss: 0.016443\n",
      "epoch: 6 [36800/60000 (61%)]\t training loss: 0.000156\n",
      "epoch: 6 [37120/60000 (62%)]\t training loss: 0.025362\n",
      "epoch: 6 [37440/60000 (62%)]\t training loss: 0.080504\n",
      "epoch: 6 [37760/60000 (63%)]\t training loss: 0.039529\n",
      "epoch: 6 [38080/60000 (63%)]\t training loss: 0.067887\n",
      "epoch: 6 [38400/60000 (64%)]\t training loss: 0.000938\n",
      "epoch: 6 [38720/60000 (65%)]\t training loss: 0.144723\n",
      "epoch: 6 [39040/60000 (65%)]\t training loss: 0.007154\n",
      "epoch: 6 [39360/60000 (66%)]\t training loss: 0.063134\n",
      "epoch: 6 [39680/60000 (66%)]\t training loss: 0.027806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 [40000/60000 (67%)]\t training loss: 0.000179\n",
      "epoch: 6 [40320/60000 (67%)]\t training loss: 0.000553\n",
      "epoch: 6 [40640/60000 (68%)]\t training loss: 0.003415\n",
      "epoch: 6 [40960/60000 (68%)]\t training loss: 0.002057\n",
      "epoch: 6 [41280/60000 (69%)]\t training loss: 0.000112\n",
      "epoch: 6 [41600/60000 (69%)]\t training loss: 0.012264\n",
      "epoch: 6 [41920/60000 (70%)]\t training loss: 0.000271\n",
      "epoch: 6 [42240/60000 (70%)]\t training loss: 0.025492\n",
      "epoch: 6 [42560/60000 (71%)]\t training loss: 0.039831\n",
      "epoch: 6 [42880/60000 (71%)]\t training loss: 0.034893\n",
      "epoch: 6 [43200/60000 (72%)]\t training loss: 0.029071\n",
      "epoch: 6 [43520/60000 (73%)]\t training loss: 0.000352\n",
      "epoch: 6 [43840/60000 (73%)]\t training loss: 0.004263\n",
      "epoch: 6 [44160/60000 (74%)]\t training loss: 0.013897\n",
      "epoch: 6 [44480/60000 (74%)]\t training loss: 0.073374\n",
      "epoch: 6 [44800/60000 (75%)]\t training loss: 0.013917\n",
      "epoch: 6 [45120/60000 (75%)]\t training loss: 0.112081\n",
      "epoch: 6 [45440/60000 (76%)]\t training loss: 0.002833\n",
      "epoch: 6 [45760/60000 (76%)]\t training loss: 0.002229\n",
      "epoch: 6 [46080/60000 (77%)]\t training loss: 0.015066\n",
      "epoch: 6 [46400/60000 (77%)]\t training loss: 0.000417\n",
      "epoch: 6 [46720/60000 (78%)]\t training loss: 0.121150\n",
      "epoch: 6 [47040/60000 (78%)]\t training loss: 0.220408\n",
      "epoch: 6 [47360/60000 (79%)]\t training loss: 0.001878\n",
      "epoch: 6 [47680/60000 (79%)]\t training loss: 0.006969\n",
      "epoch: 6 [48000/60000 (80%)]\t training loss: 0.001172\n",
      "epoch: 6 [48320/60000 (81%)]\t training loss: 0.014794\n",
      "epoch: 6 [48640/60000 (81%)]\t training loss: 0.023504\n",
      "epoch: 6 [48960/60000 (82%)]\t training loss: 0.097862\n",
      "epoch: 6 [49280/60000 (82%)]\t training loss: 0.142791\n",
      "epoch: 6 [49600/60000 (83%)]\t training loss: 0.004318\n",
      "epoch: 6 [49920/60000 (83%)]\t training loss: 0.012206\n",
      "epoch: 6 [50240/60000 (84%)]\t training loss: 0.000420\n",
      "epoch: 6 [50560/60000 (84%)]\t training loss: 0.000256\n",
      "epoch: 6 [50880/60000 (85%)]\t training loss: 0.014496\n",
      "epoch: 6 [51200/60000 (85%)]\t training loss: 0.029155\n",
      "epoch: 6 [51520/60000 (86%)]\t training loss: 0.002195\n",
      "epoch: 6 [51840/60000 (86%)]\t training loss: 0.068567\n",
      "epoch: 6 [52160/60000 (87%)]\t training loss: 0.000238\n",
      "epoch: 6 [52480/60000 (87%)]\t training loss: 0.003842\n",
      "epoch: 6 [52800/60000 (88%)]\t training loss: 0.138590\n",
      "epoch: 6 [53120/60000 (89%)]\t training loss: 0.007010\n",
      "epoch: 6 [53440/60000 (89%)]\t training loss: 0.000995\n",
      "epoch: 6 [53760/60000 (90%)]\t training loss: 0.001917\n",
      "epoch: 6 [54080/60000 (90%)]\t training loss: 0.000764\n",
      "epoch: 6 [54400/60000 (91%)]\t training loss: 0.044623\n",
      "epoch: 6 [54720/60000 (91%)]\t training loss: 0.220543\n",
      "epoch: 6 [55040/60000 (92%)]\t training loss: 0.069147\n",
      "epoch: 6 [55360/60000 (92%)]\t training loss: 0.000701\n",
      "epoch: 6 [55680/60000 (93%)]\t training loss: 0.116224\n",
      "epoch: 6 [56000/60000 (93%)]\t training loss: 0.039493\n",
      "epoch: 6 [56320/60000 (94%)]\t training loss: 0.006787\n",
      "epoch: 6 [56640/60000 (94%)]\t training loss: 0.002880\n",
      "epoch: 6 [56960/60000 (95%)]\t training loss: 0.005604\n",
      "epoch: 6 [57280/60000 (95%)]\t training loss: 0.000409\n",
      "epoch: 6 [57600/60000 (96%)]\t training loss: 0.000780\n",
      "epoch: 6 [57920/60000 (97%)]\t training loss: 0.006738\n",
      "epoch: 6 [58240/60000 (97%)]\t training loss: 0.020786\n",
      "epoch: 6 [58560/60000 (98%)]\t training loss: 0.219012\n",
      "epoch: 6 [58880/60000 (98%)]\t training loss: 0.116275\n",
      "epoch: 6 [59200/60000 (99%)]\t training loss: 0.015954\n",
      "epoch: 6 [59520/60000 (99%)]\t training loss: 0.036801\n",
      "epoch: 6 [59840/60000 (100%)]\t training loss: 0.003021\n",
      "\n",
      "Test dataset: Overall Loss: 0.0318, Overall Accuracy: 9906/10000 (99%)\n",
      "\n",
      "epoch: 7 [0/60000 (0%)]\t training loss: 0.001782\n",
      "epoch: 7 [320/60000 (1%)]\t training loss: 0.000110\n",
      "epoch: 7 [640/60000 (1%)]\t training loss: 0.010041\n",
      "epoch: 7 [960/60000 (2%)]\t training loss: 0.026830\n",
      "epoch: 7 [1280/60000 (2%)]\t training loss: 0.000844\n",
      "epoch: 7 [1600/60000 (3%)]\t training loss: 0.164492\n",
      "epoch: 7 [1920/60000 (3%)]\t training loss: 0.005808\n",
      "epoch: 7 [2240/60000 (4%)]\t training loss: 0.089950\n",
      "epoch: 7 [2560/60000 (4%)]\t training loss: 0.010739\n",
      "epoch: 7 [2880/60000 (5%)]\t training loss: 0.000580\n",
      "epoch: 7 [3200/60000 (5%)]\t training loss: 0.003096\n",
      "epoch: 7 [3520/60000 (6%)]\t training loss: 0.028123\n",
      "epoch: 7 [3840/60000 (6%)]\t training loss: 0.110453\n",
      "epoch: 7 [4160/60000 (7%)]\t training loss: 0.047188\n",
      "epoch: 7 [4480/60000 (7%)]\t training loss: 0.000661\n",
      "epoch: 7 [4800/60000 (8%)]\t training loss: 0.278327\n",
      "epoch: 7 [5120/60000 (9%)]\t training loss: 0.009153\n",
      "epoch: 7 [5440/60000 (9%)]\t training loss: 0.000617\n",
      "epoch: 7 [5760/60000 (10%)]\t training loss: 0.046600\n",
      "epoch: 7 [6080/60000 (10%)]\t training loss: 0.001976\n",
      "epoch: 7 [6400/60000 (11%)]\t training loss: 0.096705\n",
      "epoch: 7 [6720/60000 (11%)]\t training loss: 0.010090\n",
      "epoch: 7 [7040/60000 (12%)]\t training loss: 0.002655\n",
      "epoch: 7 [7360/60000 (12%)]\t training loss: 0.256538\n",
      "epoch: 7 [7680/60000 (13%)]\t training loss: 0.010445\n",
      "epoch: 7 [8000/60000 (13%)]\t training loss: 0.002534\n",
      "epoch: 7 [8320/60000 (14%)]\t training loss: 0.006069\n",
      "epoch: 7 [8640/60000 (14%)]\t training loss: 0.000592\n",
      "epoch: 7 [8960/60000 (15%)]\t training loss: 0.032307\n",
      "epoch: 7 [9280/60000 (15%)]\t training loss: 0.012957\n",
      "epoch: 7 [9600/60000 (16%)]\t training loss: 0.004805\n",
      "epoch: 7 [9920/60000 (17%)]\t training loss: 0.061089\n",
      "epoch: 7 [10240/60000 (17%)]\t training loss: 0.000838\n",
      "epoch: 7 [10560/60000 (18%)]\t training loss: 0.003656\n",
      "epoch: 7 [10880/60000 (18%)]\t training loss: 0.006407\n",
      "epoch: 7 [11200/60000 (19%)]\t training loss: 0.122424\n",
      "epoch: 7 [11520/60000 (19%)]\t training loss: 0.001196\n",
      "epoch: 7 [11840/60000 (20%)]\t training loss: 0.000614\n",
      "epoch: 7 [12160/60000 (20%)]\t training loss: 0.024721\n",
      "epoch: 7 [12480/60000 (21%)]\t training loss: 0.002864\n",
      "epoch: 7 [12800/60000 (21%)]\t training loss: 0.000540\n",
      "epoch: 7 [13120/60000 (22%)]\t training loss: 0.000217\n",
      "epoch: 7 [13440/60000 (22%)]\t training loss: 0.001407\n",
      "epoch: 7 [13760/60000 (23%)]\t training loss: 0.000959\n",
      "epoch: 7 [14080/60000 (23%)]\t training loss: 0.016466\n",
      "epoch: 7 [14400/60000 (24%)]\t training loss: 0.047693\n",
      "epoch: 7 [14720/60000 (25%)]\t training loss: 0.001221\n",
      "epoch: 7 [15040/60000 (25%)]\t training loss: 0.000228\n",
      "epoch: 7 [15360/60000 (26%)]\t training loss: 0.063817\n",
      "epoch: 7 [15680/60000 (26%)]\t training loss: 0.000089\n",
      "epoch: 7 [16000/60000 (27%)]\t training loss: 0.001303\n",
      "epoch: 7 [16320/60000 (27%)]\t training loss: 0.009469\n",
      "epoch: 7 [16640/60000 (28%)]\t training loss: 0.019051\n",
      "epoch: 7 [16960/60000 (28%)]\t training loss: 0.103356\n",
      "epoch: 7 [17280/60000 (29%)]\t training loss: 0.298783\n",
      "epoch: 7 [17600/60000 (29%)]\t training loss: 0.037938\n",
      "epoch: 7 [17920/60000 (30%)]\t training loss: 0.011778\n",
      "epoch: 7 [18240/60000 (30%)]\t training loss: 0.009319\n",
      "epoch: 7 [18560/60000 (31%)]\t training loss: 0.036146\n",
      "epoch: 7 [18880/60000 (31%)]\t training loss: 0.008503\n",
      "epoch: 7 [19200/60000 (32%)]\t training loss: 0.001291\n",
      "epoch: 7 [19520/60000 (33%)]\t training loss: 0.055380\n",
      "epoch: 7 [19840/60000 (33%)]\t training loss: 0.003045\n",
      "epoch: 7 [20160/60000 (34%)]\t training loss: 0.012838\n",
      "epoch: 7 [20480/60000 (34%)]\t training loss: 0.038900\n",
      "epoch: 7 [20800/60000 (35%)]\t training loss: 0.004284\n",
      "epoch: 7 [21120/60000 (35%)]\t training loss: 0.001692\n",
      "epoch: 7 [21440/60000 (36%)]\t training loss: 0.026878\n",
      "epoch: 7 [21760/60000 (36%)]\t training loss: 0.007952\n",
      "epoch: 7 [22080/60000 (37%)]\t training loss: 0.028047\n",
      "epoch: 7 [22400/60000 (37%)]\t training loss: 0.001185\n",
      "epoch: 7 [22720/60000 (38%)]\t training loss: 0.097835\n",
      "epoch: 7 [23040/60000 (38%)]\t training loss: 0.002545\n",
      "epoch: 7 [23360/60000 (39%)]\t training loss: 0.041519\n",
      "epoch: 7 [23680/60000 (39%)]\t training loss: 0.060170\n",
      "epoch: 7 [24000/60000 (40%)]\t training loss: 0.067488\n",
      "epoch: 7 [24320/60000 (41%)]\t training loss: 0.001790\n",
      "epoch: 7 [24640/60000 (41%)]\t training loss: 0.033166\n",
      "epoch: 7 [24960/60000 (42%)]\t training loss: 0.000176\n",
      "epoch: 7 [25280/60000 (42%)]\t training loss: 0.155722\n",
      "epoch: 7 [25600/60000 (43%)]\t training loss: 0.004403\n",
      "epoch: 7 [25920/60000 (43%)]\t training loss: 0.009060\n",
      "epoch: 7 [26240/60000 (44%)]\t training loss: 0.003144\n",
      "epoch: 7 [26560/60000 (44%)]\t training loss: 0.003386\n",
      "epoch: 7 [26880/60000 (45%)]\t training loss: 0.134606\n",
      "epoch: 7 [27200/60000 (45%)]\t training loss: 0.037823\n",
      "epoch: 7 [27520/60000 (46%)]\t training loss: 0.007792\n",
      "epoch: 7 [27840/60000 (46%)]\t training loss: 0.228892\n",
      "epoch: 7 [28160/60000 (47%)]\t training loss: 0.076860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 [28480/60000 (47%)]\t training loss: 0.000587\n",
      "epoch: 7 [28800/60000 (48%)]\t training loss: 0.036599\n",
      "epoch: 7 [29120/60000 (49%)]\t training loss: 0.002834\n",
      "epoch: 7 [29440/60000 (49%)]\t training loss: 0.148385\n",
      "epoch: 7 [29760/60000 (50%)]\t training loss: 0.004178\n",
      "epoch: 7 [30080/60000 (50%)]\t training loss: 0.006388\n",
      "epoch: 7 [30400/60000 (51%)]\t training loss: 0.001270\n",
      "epoch: 7 [30720/60000 (51%)]\t training loss: 0.001692\n",
      "epoch: 7 [31040/60000 (52%)]\t training loss: 0.041071\n",
      "epoch: 7 [31360/60000 (52%)]\t training loss: 0.329523\n",
      "epoch: 7 [31680/60000 (53%)]\t training loss: 0.000068\n",
      "epoch: 7 [32000/60000 (53%)]\t training loss: 0.010583\n",
      "epoch: 7 [32320/60000 (54%)]\t training loss: 0.199859\n",
      "epoch: 7 [32640/60000 (54%)]\t training loss: 0.000986\n",
      "epoch: 7 [32960/60000 (55%)]\t training loss: 0.038600\n",
      "epoch: 7 [33280/60000 (55%)]\t training loss: 0.005068\n",
      "epoch: 7 [33600/60000 (56%)]\t training loss: 0.000119\n",
      "epoch: 7 [33920/60000 (57%)]\t training loss: 0.078540\n",
      "epoch: 7 [34240/60000 (57%)]\t training loss: 0.001953\n",
      "epoch: 7 [34560/60000 (58%)]\t training loss: 0.000846\n",
      "epoch: 7 [34880/60000 (58%)]\t training loss: 0.001839\n",
      "epoch: 7 [35200/60000 (59%)]\t training loss: 0.004606\n",
      "epoch: 7 [35520/60000 (59%)]\t training loss: 0.002044\n",
      "epoch: 7 [35840/60000 (60%)]\t training loss: 0.109367\n",
      "epoch: 7 [36160/60000 (60%)]\t training loss: 0.005942\n",
      "epoch: 7 [36480/60000 (61%)]\t training loss: 0.011703\n",
      "epoch: 7 [36800/60000 (61%)]\t training loss: 0.008262\n",
      "epoch: 7 [37120/60000 (62%)]\t training loss: 0.001403\n",
      "epoch: 7 [37440/60000 (62%)]\t training loss: 0.000815\n",
      "epoch: 7 [37760/60000 (63%)]\t training loss: 0.002125\n",
      "epoch: 7 [38080/60000 (63%)]\t training loss: 0.002018\n",
      "epoch: 7 [38400/60000 (64%)]\t training loss: 0.008099\n",
      "epoch: 7 [38720/60000 (65%)]\t training loss: 0.028951\n",
      "epoch: 7 [39040/60000 (65%)]\t training loss: 0.003581\n",
      "epoch: 7 [39360/60000 (66%)]\t training loss: 0.001532\n",
      "epoch: 7 [39680/60000 (66%)]\t training loss: 0.000952\n",
      "epoch: 7 [40000/60000 (67%)]\t training loss: 0.060227\n",
      "epoch: 7 [40320/60000 (67%)]\t training loss: 0.003111\n",
      "epoch: 7 [40640/60000 (68%)]\t training loss: 0.097227\n",
      "epoch: 7 [40960/60000 (68%)]\t training loss: 0.000184\n",
      "epoch: 7 [41280/60000 (69%)]\t training loss: 0.003544\n",
      "epoch: 7 [41600/60000 (69%)]\t training loss: 0.079283\n",
      "epoch: 7 [41920/60000 (70%)]\t training loss: 0.000300\n",
      "epoch: 7 [42240/60000 (70%)]\t training loss: 0.162747\n",
      "epoch: 7 [42560/60000 (71%)]\t training loss: 0.000044\n",
      "epoch: 7 [42880/60000 (71%)]\t training loss: 0.000366\n",
      "epoch: 7 [43200/60000 (72%)]\t training loss: 0.000303\n",
      "epoch: 7 [43520/60000 (73%)]\t training loss: 0.000565\n",
      "epoch: 7 [43840/60000 (73%)]\t training loss: 0.000196\n",
      "epoch: 7 [44160/60000 (74%)]\t training loss: 0.007849\n",
      "epoch: 7 [44480/60000 (74%)]\t training loss: 0.001511\n",
      "epoch: 7 [44800/60000 (75%)]\t training loss: 0.000390\n",
      "epoch: 7 [45120/60000 (75%)]\t training loss: 0.024156\n",
      "epoch: 7 [45440/60000 (76%)]\t training loss: 0.043633\n",
      "epoch: 7 [45760/60000 (76%)]\t training loss: 0.000670\n",
      "epoch: 7 [46080/60000 (77%)]\t training loss: 0.017463\n",
      "epoch: 7 [46400/60000 (77%)]\t training loss: 0.016481\n",
      "epoch: 7 [46720/60000 (78%)]\t training loss: 0.038352\n",
      "epoch: 7 [47040/60000 (78%)]\t training loss: 0.210848\n",
      "epoch: 7 [47360/60000 (79%)]\t training loss: 0.007939\n",
      "epoch: 7 [47680/60000 (79%)]\t training loss: 0.036062\n",
      "epoch: 7 [48000/60000 (80%)]\t training loss: 0.034710\n",
      "epoch: 7 [48320/60000 (81%)]\t training loss: 0.001291\n",
      "epoch: 7 [48640/60000 (81%)]\t training loss: 0.005284\n",
      "epoch: 7 [48960/60000 (82%)]\t training loss: 0.001648\n",
      "epoch: 7 [49280/60000 (82%)]\t training loss: 0.071880\n",
      "epoch: 7 [49600/60000 (83%)]\t training loss: 0.011208\n",
      "epoch: 7 [49920/60000 (83%)]\t training loss: 0.004944\n",
      "epoch: 7 [50240/60000 (84%)]\t training loss: 0.014755\n",
      "epoch: 7 [50560/60000 (84%)]\t training loss: 0.113532\n",
      "epoch: 7 [50880/60000 (85%)]\t training loss: 0.024081\n",
      "epoch: 7 [51200/60000 (85%)]\t training loss: 0.003516\n",
      "epoch: 7 [51520/60000 (86%)]\t training loss: 0.052273\n",
      "epoch: 7 [51840/60000 (86%)]\t training loss: 0.000311\n",
      "epoch: 7 [52160/60000 (87%)]\t training loss: 0.004985\n",
      "epoch: 7 [52480/60000 (87%)]\t training loss: 0.043148\n",
      "epoch: 7 [52800/60000 (88%)]\t training loss: 0.114065\n",
      "epoch: 7 [53120/60000 (89%)]\t training loss: 0.000614\n",
      "epoch: 7 [53440/60000 (89%)]\t training loss: 0.007145\n",
      "epoch: 7 [53760/60000 (90%)]\t training loss: 0.003248\n",
      "epoch: 7 [54080/60000 (90%)]\t training loss: 0.004246\n",
      "epoch: 7 [54400/60000 (91%)]\t training loss: 0.176548\n",
      "epoch: 7 [54720/60000 (91%)]\t training loss: 0.002102\n",
      "epoch: 7 [55040/60000 (92%)]\t training loss: 0.001731\n",
      "epoch: 7 [55360/60000 (92%)]\t training loss: 0.001547\n",
      "epoch: 7 [55680/60000 (93%)]\t training loss: 0.002465\n",
      "epoch: 7 [56000/60000 (93%)]\t training loss: 0.000871\n",
      "epoch: 7 [56320/60000 (94%)]\t training loss: 0.000390\n",
      "epoch: 7 [56640/60000 (94%)]\t training loss: 0.308954\n",
      "epoch: 7 [56960/60000 (95%)]\t training loss: 0.000642\n",
      "epoch: 7 [57280/60000 (95%)]\t training loss: 0.074907\n",
      "epoch: 7 [57600/60000 (96%)]\t training loss: 0.000549\n",
      "epoch: 7 [57920/60000 (97%)]\t training loss: 0.006800\n",
      "epoch: 7 [58240/60000 (97%)]\t training loss: 0.000078\n",
      "epoch: 7 [58560/60000 (98%)]\t training loss: 0.034498\n",
      "epoch: 7 [58880/60000 (98%)]\t training loss: 0.000921\n",
      "epoch: 7 [59200/60000 (99%)]\t training loss: 0.000338\n",
      "epoch: 7 [59520/60000 (99%)]\t training loss: 0.010178\n",
      "epoch: 7 [59840/60000 (100%)]\t training loss: 0.000629\n",
      "\n",
      "Test dataset: Overall Loss: 0.0345, Overall Accuracy: 9905/10000 (99%)\n",
      "\n",
      "epoch: 8 [0/60000 (0%)]\t training loss: 0.072474\n",
      "epoch: 8 [320/60000 (1%)]\t training loss: 0.092481\n",
      "epoch: 8 [640/60000 (1%)]\t training loss: 0.034074\n",
      "epoch: 8 [960/60000 (2%)]\t training loss: 0.017108\n",
      "epoch: 8 [1280/60000 (2%)]\t training loss: 0.001772\n",
      "epoch: 8 [1600/60000 (3%)]\t training loss: 0.023869\n",
      "epoch: 8 [1920/60000 (3%)]\t training loss: 0.005847\n",
      "epoch: 8 [2240/60000 (4%)]\t training loss: 0.004044\n",
      "epoch: 8 [2560/60000 (4%)]\t training loss: 0.035259\n",
      "epoch: 8 [2880/60000 (5%)]\t training loss: 0.000968\n",
      "epoch: 8 [3200/60000 (5%)]\t training loss: 0.000156\n",
      "epoch: 8 [3520/60000 (6%)]\t training loss: 0.011701\n",
      "epoch: 8 [3840/60000 (6%)]\t training loss: 0.131797\n",
      "epoch: 8 [4160/60000 (7%)]\t training loss: 0.002452\n",
      "epoch: 8 [4480/60000 (7%)]\t training loss: 0.011719\n",
      "epoch: 8 [4800/60000 (8%)]\t training loss: 0.009628\n",
      "epoch: 8 [5120/60000 (9%)]\t training loss: 0.003484\n",
      "epoch: 8 [5440/60000 (9%)]\t training loss: 0.007140\n",
      "epoch: 8 [5760/60000 (10%)]\t training loss: 0.000160\n",
      "epoch: 8 [6080/60000 (10%)]\t training loss: 0.036415\n",
      "epoch: 8 [6400/60000 (11%)]\t training loss: 0.008206\n",
      "epoch: 8 [6720/60000 (11%)]\t training loss: 0.028622\n",
      "epoch: 8 [7040/60000 (12%)]\t training loss: 0.000505\n",
      "epoch: 8 [7360/60000 (12%)]\t training loss: 0.008920\n",
      "epoch: 8 [7680/60000 (13%)]\t training loss: 0.023873\n",
      "epoch: 8 [8000/60000 (13%)]\t training loss: 0.000957\n",
      "epoch: 8 [8320/60000 (14%)]\t training loss: 0.000852\n",
      "epoch: 8 [8640/60000 (14%)]\t training loss: 0.008337\n",
      "epoch: 8 [8960/60000 (15%)]\t training loss: 0.000871\n",
      "epoch: 8 [9280/60000 (15%)]\t training loss: 0.000328\n",
      "epoch: 8 [9600/60000 (16%)]\t training loss: 0.002768\n",
      "epoch: 8 [9920/60000 (17%)]\t training loss: 0.000563\n",
      "epoch: 8 [10240/60000 (17%)]\t training loss: 0.004486\n",
      "epoch: 8 [10560/60000 (18%)]\t training loss: 0.208879\n",
      "epoch: 8 [10880/60000 (18%)]\t training loss: 0.044148\n",
      "epoch: 8 [11200/60000 (19%)]\t training loss: 0.037540\n",
      "epoch: 8 [11520/60000 (19%)]\t training loss: 0.000046\n",
      "epoch: 8 [11840/60000 (20%)]\t training loss: 0.000434\n",
      "epoch: 8 [12160/60000 (20%)]\t training loss: 0.120800\n",
      "epoch: 8 [12480/60000 (21%)]\t training loss: 0.028627\n",
      "epoch: 8 [12800/60000 (21%)]\t training loss: 0.006484\n",
      "epoch: 8 [13120/60000 (22%)]\t training loss: 0.059793\n",
      "epoch: 8 [13440/60000 (22%)]\t training loss: 0.065995\n",
      "epoch: 8 [13760/60000 (23%)]\t training loss: 0.033341\n",
      "epoch: 8 [14080/60000 (23%)]\t training loss: 0.000888\n",
      "epoch: 8 [14400/60000 (24%)]\t training loss: 0.001170\n",
      "epoch: 8 [14720/60000 (25%)]\t training loss: 0.022763\n",
      "epoch: 8 [15040/60000 (25%)]\t training loss: 0.000148\n",
      "epoch: 8 [15360/60000 (26%)]\t training loss: 0.000172\n",
      "epoch: 8 [15680/60000 (26%)]\t training loss: 0.000084\n",
      "epoch: 8 [16000/60000 (27%)]\t training loss: 0.022146\n",
      "epoch: 8 [16320/60000 (27%)]\t training loss: 0.021021\n",
      "epoch: 8 [16640/60000 (28%)]\t training loss: 0.006424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 [16960/60000 (28%)]\t training loss: 0.018272\n",
      "epoch: 8 [17280/60000 (29%)]\t training loss: 0.021515\n",
      "epoch: 8 [17600/60000 (29%)]\t training loss: 0.087931\n",
      "epoch: 8 [17920/60000 (30%)]\t training loss: 0.009689\n",
      "epoch: 8 [18240/60000 (30%)]\t training loss: 0.008298\n",
      "epoch: 8 [18560/60000 (31%)]\t training loss: 0.190545\n",
      "epoch: 8 [18880/60000 (31%)]\t training loss: 0.317700\n",
      "epoch: 8 [19200/60000 (32%)]\t training loss: 0.003710\n",
      "epoch: 8 [19520/60000 (33%)]\t training loss: 0.014792\n",
      "epoch: 8 [19840/60000 (33%)]\t training loss: 0.001124\n",
      "epoch: 8 [20160/60000 (34%)]\t training loss: 0.002587\n",
      "epoch: 8 [20480/60000 (34%)]\t training loss: 0.012084\n",
      "epoch: 8 [20800/60000 (35%)]\t training loss: 0.038801\n",
      "epoch: 8 [21120/60000 (35%)]\t training loss: 0.000152\n",
      "epoch: 8 [21440/60000 (36%)]\t training loss: 0.070894\n",
      "epoch: 8 [21760/60000 (36%)]\t training loss: 0.084773\n",
      "epoch: 8 [22080/60000 (37%)]\t training loss: 0.030737\n",
      "epoch: 8 [22400/60000 (37%)]\t training loss: 0.003192\n",
      "epoch: 8 [22720/60000 (38%)]\t training loss: 0.016323\n",
      "epoch: 8 [23040/60000 (38%)]\t training loss: 0.000288\n",
      "epoch: 8 [23360/60000 (39%)]\t training loss: 0.000415\n",
      "epoch: 8 [23680/60000 (39%)]\t training loss: 0.000429\n",
      "epoch: 8 [24000/60000 (40%)]\t training loss: 0.001153\n",
      "epoch: 8 [24320/60000 (41%)]\t training loss: 0.004830\n",
      "epoch: 8 [24640/60000 (41%)]\t training loss: 0.012250\n",
      "epoch: 8 [24960/60000 (42%)]\t training loss: 0.001288\n",
      "epoch: 8 [25280/60000 (42%)]\t training loss: 0.000215\n",
      "epoch: 8 [25600/60000 (43%)]\t training loss: 0.170683\n",
      "epoch: 8 [25920/60000 (43%)]\t training loss: 0.026552\n",
      "epoch: 8 [26240/60000 (44%)]\t training loss: 0.004347\n",
      "epoch: 8 [26560/60000 (44%)]\t training loss: 0.050397\n",
      "epoch: 8 [26880/60000 (45%)]\t training loss: 0.009420\n",
      "epoch: 8 [27200/60000 (45%)]\t training loss: 0.003242\n",
      "epoch: 8 [27520/60000 (46%)]\t training loss: 0.000214\n",
      "epoch: 8 [27840/60000 (46%)]\t training loss: 0.010099\n",
      "epoch: 8 [28160/60000 (47%)]\t training loss: 0.020089\n",
      "epoch: 8 [28480/60000 (47%)]\t training loss: 0.000651\n",
      "epoch: 8 [28800/60000 (48%)]\t training loss: 0.004463\n",
      "epoch: 8 [29120/60000 (49%)]\t training loss: 0.070880\n",
      "epoch: 8 [29440/60000 (49%)]\t training loss: 0.010683\n",
      "epoch: 8 [29760/60000 (50%)]\t training loss: 0.071721\n",
      "epoch: 8 [30080/60000 (50%)]\t training loss: 0.272843\n",
      "epoch: 8 [30400/60000 (51%)]\t training loss: 0.016663\n",
      "epoch: 8 [30720/60000 (51%)]\t training loss: 0.004786\n",
      "epoch: 8 [31040/60000 (52%)]\t training loss: 0.000974\n",
      "epoch: 8 [31360/60000 (52%)]\t training loss: 0.008883\n",
      "epoch: 8 [31680/60000 (53%)]\t training loss: 0.000565\n",
      "epoch: 8 [32000/60000 (53%)]\t training loss: 0.000118\n",
      "epoch: 8 [32320/60000 (54%)]\t training loss: 0.006918\n",
      "epoch: 8 [32640/60000 (54%)]\t training loss: 0.002010\n",
      "epoch: 8 [32960/60000 (55%)]\t training loss: 0.025494\n",
      "epoch: 8 [33280/60000 (55%)]\t training loss: 0.001369\n",
      "epoch: 8 [33600/60000 (56%)]\t training loss: 0.000339\n",
      "epoch: 8 [33920/60000 (57%)]\t training loss: 0.041417\n",
      "epoch: 8 [34240/60000 (57%)]\t training loss: 0.039829\n",
      "epoch: 8 [34560/60000 (58%)]\t training loss: 0.002438\n",
      "epoch: 8 [34880/60000 (58%)]\t training loss: 0.000091\n",
      "epoch: 8 [35200/60000 (59%)]\t training loss: 0.004144\n",
      "epoch: 8 [35520/60000 (59%)]\t training loss: 0.001696\n",
      "epoch: 8 [35840/60000 (60%)]\t training loss: 0.000032\n",
      "epoch: 8 [36160/60000 (60%)]\t training loss: 0.000044\n",
      "epoch: 8 [36480/60000 (61%)]\t training loss: 0.102347\n",
      "epoch: 8 [36800/60000 (61%)]\t training loss: 0.000203\n",
      "epoch: 8 [37120/60000 (62%)]\t training loss: 0.000684\n",
      "epoch: 8 [37440/60000 (62%)]\t training loss: 0.088426\n",
      "epoch: 8 [37760/60000 (63%)]\t training loss: 0.004654\n",
      "epoch: 8 [38080/60000 (63%)]\t training loss: 0.090120\n",
      "epoch: 8 [38400/60000 (64%)]\t training loss: 0.000079\n",
      "epoch: 8 [38720/60000 (65%)]\t training loss: 0.362465\n",
      "epoch: 8 [39040/60000 (65%)]\t training loss: 0.019854\n",
      "epoch: 8 [39360/60000 (66%)]\t training loss: 0.111859\n",
      "epoch: 8 [39680/60000 (66%)]\t training loss: 0.005524\n",
      "epoch: 8 [40000/60000 (67%)]\t training loss: 0.003574\n",
      "epoch: 8 [40320/60000 (67%)]\t training loss: 0.019848\n",
      "epoch: 8 [40640/60000 (68%)]\t training loss: 0.006543\n",
      "epoch: 8 [40960/60000 (68%)]\t training loss: 0.130284\n",
      "epoch: 8 [41280/60000 (69%)]\t training loss: 0.000276\n",
      "epoch: 8 [41600/60000 (69%)]\t training loss: 0.009770\n",
      "epoch: 8 [41920/60000 (70%)]\t training loss: 0.002090\n",
      "epoch: 8 [42240/60000 (70%)]\t training loss: 0.001318\n",
      "epoch: 8 [42560/60000 (71%)]\t training loss: 0.000377\n",
      "epoch: 8 [42880/60000 (71%)]\t training loss: 0.006090\n",
      "epoch: 8 [43200/60000 (72%)]\t training loss: 0.001835\n",
      "epoch: 8 [43520/60000 (73%)]\t training loss: 0.006565\n",
      "epoch: 8 [43840/60000 (73%)]\t training loss: 0.013832\n",
      "epoch: 8 [44160/60000 (74%)]\t training loss: 0.000341\n",
      "epoch: 8 [44480/60000 (74%)]\t training loss: 0.008659\n",
      "epoch: 8 [44800/60000 (75%)]\t training loss: 0.045421\n",
      "epoch: 8 [45120/60000 (75%)]\t training loss: 0.016734\n",
      "epoch: 8 [45440/60000 (76%)]\t training loss: 0.029803\n",
      "epoch: 8 [45760/60000 (76%)]\t training loss: 0.000233\n",
      "epoch: 8 [46080/60000 (77%)]\t training loss: 0.044417\n",
      "epoch: 8 [46400/60000 (77%)]\t training loss: 0.056911\n",
      "epoch: 8 [46720/60000 (78%)]\t training loss: 0.024480\n",
      "epoch: 8 [47040/60000 (78%)]\t training loss: 0.000319\n",
      "epoch: 8 [47360/60000 (79%)]\t training loss: 0.185891\n",
      "epoch: 8 [47680/60000 (79%)]\t training loss: 0.040795\n",
      "epoch: 8 [48000/60000 (80%)]\t training loss: 0.281314\n",
      "epoch: 8 [48320/60000 (81%)]\t training loss: 0.022553\n",
      "epoch: 8 [48640/60000 (81%)]\t training loss: 0.018886\n",
      "epoch: 8 [48960/60000 (82%)]\t training loss: 0.003403\n",
      "epoch: 8 [49280/60000 (82%)]\t training loss: 0.142005\n",
      "epoch: 8 [49600/60000 (83%)]\t training loss: 0.017319\n",
      "epoch: 8 [49920/60000 (83%)]\t training loss: 0.010083\n",
      "epoch: 8 [50240/60000 (84%)]\t training loss: 0.000311\n",
      "epoch: 8 [50560/60000 (84%)]\t training loss: 0.014885\n",
      "epoch: 8 [50880/60000 (85%)]\t training loss: 0.025617\n",
      "epoch: 8 [51200/60000 (85%)]\t training loss: 0.001933\n",
      "epoch: 8 [51520/60000 (86%)]\t training loss: 0.061748\n",
      "epoch: 8 [51840/60000 (86%)]\t training loss: 0.045454\n",
      "epoch: 8 [52160/60000 (87%)]\t training loss: 0.003752\n",
      "epoch: 8 [52480/60000 (87%)]\t training loss: 0.004589\n",
      "epoch: 8 [52800/60000 (88%)]\t training loss: 0.168529\n",
      "epoch: 8 [53120/60000 (89%)]\t training loss: 0.012016\n",
      "epoch: 8 [53440/60000 (89%)]\t training loss: 0.000378\n",
      "epoch: 8 [53760/60000 (90%)]\t training loss: 0.002229\n",
      "epoch: 8 [54080/60000 (90%)]\t training loss: 0.201542\n",
      "epoch: 8 [54400/60000 (91%)]\t training loss: 0.244421\n",
      "epoch: 8 [54720/60000 (91%)]\t training loss: 0.000162\n",
      "epoch: 8 [55040/60000 (92%)]\t training loss: 0.000960\n",
      "epoch: 8 [55360/60000 (92%)]\t training loss: 0.002656\n",
      "epoch: 8 [55680/60000 (93%)]\t training loss: 0.005345\n",
      "epoch: 8 [56000/60000 (93%)]\t training loss: 0.000380\n",
      "epoch: 8 [56320/60000 (94%)]\t training loss: 0.015638\n",
      "epoch: 8 [56640/60000 (94%)]\t training loss: 0.024332\n",
      "epoch: 8 [56960/60000 (95%)]\t training loss: 0.004740\n",
      "epoch: 8 [57280/60000 (95%)]\t training loss: 0.001361\n",
      "epoch: 8 [57600/60000 (96%)]\t training loss: 0.000054\n",
      "epoch: 8 [57920/60000 (97%)]\t training loss: 0.000054\n",
      "epoch: 8 [58240/60000 (97%)]\t training loss: 0.005766\n",
      "epoch: 8 [58560/60000 (98%)]\t training loss: 0.210638\n",
      "epoch: 8 [58880/60000 (98%)]\t training loss: 0.009194\n",
      "epoch: 8 [59200/60000 (99%)]\t training loss: 0.000084\n",
      "epoch: 8 [59520/60000 (99%)]\t training loss: 0.002300\n",
      "epoch: 8 [59840/60000 (100%)]\t training loss: 0.009321\n",
      "\n",
      "Test dataset: Overall Loss: 0.0312, Overall Accuracy: 9911/10000 (99%)\n",
      "\n",
      "epoch: 9 [0/60000 (0%)]\t training loss: 0.009323\n",
      "epoch: 9 [320/60000 (1%)]\t training loss: 0.000163\n",
      "epoch: 9 [640/60000 (1%)]\t training loss: 0.001574\n",
      "epoch: 9 [960/60000 (2%)]\t training loss: 0.000170\n",
      "epoch: 9 [1280/60000 (2%)]\t training loss: 0.158246\n",
      "epoch: 9 [1600/60000 (3%)]\t training loss: 0.000411\n",
      "epoch: 9 [1920/60000 (3%)]\t training loss: 0.009655\n",
      "epoch: 9 [2240/60000 (4%)]\t training loss: 0.001361\n",
      "epoch: 9 [2560/60000 (4%)]\t training loss: 0.014223\n",
      "epoch: 9 [2880/60000 (5%)]\t training loss: 0.083382\n",
      "epoch: 9 [3200/60000 (5%)]\t training loss: 0.035933\n",
      "epoch: 9 [3520/60000 (6%)]\t training loss: 0.002207\n",
      "epoch: 9 [3840/60000 (6%)]\t training loss: 0.000635\n",
      "epoch: 9 [4160/60000 (7%)]\t training loss: 0.000032\n",
      "epoch: 9 [4480/60000 (7%)]\t training loss: 0.023192\n",
      "epoch: 9 [4800/60000 (8%)]\t training loss: 0.004670\n",
      "epoch: 9 [5120/60000 (9%)]\t training loss: 0.000149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 [5440/60000 (9%)]\t training loss: 0.058256\n",
      "epoch: 9 [5760/60000 (10%)]\t training loss: 0.001104\n",
      "epoch: 9 [6080/60000 (10%)]\t training loss: 0.007344\n",
      "epoch: 9 [6400/60000 (11%)]\t training loss: 0.068893\n",
      "epoch: 9 [6720/60000 (11%)]\t training loss: 0.026050\n",
      "epoch: 9 [7040/60000 (12%)]\t training loss: 0.006308\n",
      "epoch: 9 [7360/60000 (12%)]\t training loss: 0.001014\n",
      "epoch: 9 [7680/60000 (13%)]\t training loss: 0.278803\n",
      "epoch: 9 [8000/60000 (13%)]\t training loss: 0.003600\n",
      "epoch: 9 [8320/60000 (14%)]\t training loss: 0.001685\n",
      "epoch: 9 [8640/60000 (14%)]\t training loss: 0.000868\n",
      "epoch: 9 [8960/60000 (15%)]\t training loss: 0.003480\n",
      "epoch: 9 [9280/60000 (15%)]\t training loss: 0.000139\n",
      "epoch: 9 [9600/60000 (16%)]\t training loss: 0.003940\n",
      "epoch: 9 [9920/60000 (17%)]\t training loss: 0.042228\n",
      "epoch: 9 [10240/60000 (17%)]\t training loss: 0.002606\n",
      "epoch: 9 [10560/60000 (18%)]\t training loss: 0.123327\n",
      "epoch: 9 [10880/60000 (18%)]\t training loss: 0.004359\n",
      "epoch: 9 [11200/60000 (19%)]\t training loss: 0.001249\n",
      "epoch: 9 [11520/60000 (19%)]\t training loss: 0.021510\n",
      "epoch: 9 [11840/60000 (20%)]\t training loss: 0.000475\n",
      "epoch: 9 [12160/60000 (20%)]\t training loss: 0.006774\n",
      "epoch: 9 [12480/60000 (21%)]\t training loss: 0.000556\n",
      "epoch: 9 [12800/60000 (21%)]\t training loss: 0.000664\n",
      "epoch: 9 [13120/60000 (22%)]\t training loss: 0.002899\n",
      "epoch: 9 [13440/60000 (22%)]\t training loss: 0.005218\n",
      "epoch: 9 [13760/60000 (23%)]\t training loss: 0.004466\n",
      "epoch: 9 [14080/60000 (23%)]\t training loss: 0.065216\n",
      "epoch: 9 [14400/60000 (24%)]\t training loss: 0.000756\n",
      "epoch: 9 [14720/60000 (25%)]\t training loss: 0.004301\n",
      "epoch: 9 [15040/60000 (25%)]\t training loss: 0.002186\n",
      "epoch: 9 [15360/60000 (26%)]\t training loss: 0.001086\n",
      "epoch: 9 [15680/60000 (26%)]\t training loss: 0.002010\n",
      "epoch: 9 [16000/60000 (27%)]\t training loss: 0.001733\n",
      "epoch: 9 [16320/60000 (27%)]\t training loss: 0.000795\n",
      "epoch: 9 [16640/60000 (28%)]\t training loss: 0.014042\n",
      "epoch: 9 [16960/60000 (28%)]\t training loss: 0.000031\n",
      "epoch: 9 [17280/60000 (29%)]\t training loss: 0.006351\n",
      "epoch: 9 [17600/60000 (29%)]\t training loss: 0.000331\n",
      "epoch: 9 [17920/60000 (30%)]\t training loss: 0.040978\n",
      "epoch: 9 [18240/60000 (30%)]\t training loss: 0.000584\n",
      "epoch: 9 [18560/60000 (31%)]\t training loss: 0.016509\n",
      "epoch: 9 [18880/60000 (31%)]\t training loss: 0.003137\n",
      "epoch: 9 [19200/60000 (32%)]\t training loss: 0.034015\n",
      "epoch: 9 [19520/60000 (33%)]\t training loss: 0.011517\n",
      "epoch: 9 [19840/60000 (33%)]\t training loss: 0.000466\n",
      "epoch: 9 [20160/60000 (34%)]\t training loss: 0.002687\n",
      "epoch: 9 [20480/60000 (34%)]\t training loss: 0.000415\n",
      "epoch: 9 [20800/60000 (35%)]\t training loss: 0.000819\n",
      "epoch: 9 [21120/60000 (35%)]\t training loss: 0.000627\n",
      "epoch: 9 [21440/60000 (36%)]\t training loss: 0.002100\n",
      "epoch: 9 [21760/60000 (36%)]\t training loss: 0.001148\n",
      "epoch: 9 [22080/60000 (37%)]\t training loss: 0.289488\n",
      "epoch: 9 [22400/60000 (37%)]\t training loss: 0.005666\n",
      "epoch: 9 [22720/60000 (38%)]\t training loss: 0.078840\n",
      "epoch: 9 [23040/60000 (38%)]\t training loss: 0.010367\n",
      "epoch: 9 [23360/60000 (39%)]\t training loss: 0.422148\n",
      "epoch: 9 [23680/60000 (39%)]\t training loss: 0.005672\n",
      "epoch: 9 [24000/60000 (40%)]\t training loss: 0.001721\n",
      "epoch: 9 [24320/60000 (41%)]\t training loss: 0.009808\n",
      "epoch: 9 [24640/60000 (41%)]\t training loss: 0.278003\n",
      "epoch: 9 [24960/60000 (42%)]\t training loss: 0.001964\n",
      "epoch: 9 [25280/60000 (42%)]\t training loss: 0.011165\n",
      "epoch: 9 [25600/60000 (43%)]\t training loss: 0.003329\n",
      "epoch: 9 [25920/60000 (43%)]\t training loss: 0.002888\n",
      "epoch: 9 [26240/60000 (44%)]\t training loss: 0.032914\n",
      "epoch: 9 [26560/60000 (44%)]\t training loss: 0.003143\n",
      "epoch: 9 [26880/60000 (45%)]\t training loss: 0.000431\n",
      "epoch: 9 [27200/60000 (45%)]\t training loss: 0.011989\n",
      "epoch: 9 [27520/60000 (46%)]\t training loss: 0.242939\n",
      "epoch: 9 [27840/60000 (46%)]\t training loss: 0.003499\n",
      "epoch: 9 [28160/60000 (47%)]\t training loss: 0.013124\n",
      "epoch: 9 [28480/60000 (47%)]\t training loss: 0.013886\n",
      "epoch: 9 [28800/60000 (48%)]\t training loss: 0.003147\n",
      "epoch: 9 [29120/60000 (49%)]\t training loss: 0.001117\n",
      "epoch: 9 [29440/60000 (49%)]\t training loss: 0.000081\n",
      "epoch: 9 [29760/60000 (50%)]\t training loss: 0.366595\n",
      "epoch: 9 [30080/60000 (50%)]\t training loss: 0.046362\n",
      "epoch: 9 [30400/60000 (51%)]\t training loss: 0.000212\n",
      "epoch: 9 [30720/60000 (51%)]\t training loss: 0.024279\n",
      "epoch: 9 [31040/60000 (52%)]\t training loss: 0.062983\n",
      "epoch: 9 [31360/60000 (52%)]\t training loss: 0.008510\n",
      "epoch: 9 [31680/60000 (53%)]\t training loss: 0.012019\n",
      "epoch: 9 [32000/60000 (53%)]\t training loss: 0.052711\n",
      "epoch: 9 [32320/60000 (54%)]\t training loss: 0.157814\n",
      "epoch: 9 [32640/60000 (54%)]\t training loss: 0.025448\n",
      "epoch: 9 [32960/60000 (55%)]\t training loss: 0.018397\n",
      "epoch: 9 [33280/60000 (55%)]\t training loss: 0.025363\n",
      "epoch: 9 [33600/60000 (56%)]\t training loss: 0.004267\n",
      "epoch: 9 [33920/60000 (57%)]\t training loss: 0.003486\n",
      "epoch: 9 [34240/60000 (57%)]\t training loss: 0.146967\n",
      "epoch: 9 [34560/60000 (58%)]\t training loss: 0.050897\n",
      "epoch: 9 [34880/60000 (58%)]\t training loss: 0.000865\n",
      "epoch: 9 [35200/60000 (59%)]\t training loss: 0.079237\n",
      "epoch: 9 [35520/60000 (59%)]\t training loss: 0.016235\n",
      "epoch: 9 [35840/60000 (60%)]\t training loss: 0.024408\n",
      "epoch: 9 [36160/60000 (60%)]\t training loss: 0.000068\n",
      "epoch: 9 [36480/60000 (61%)]\t training loss: 0.028291\n",
      "epoch: 9 [36800/60000 (61%)]\t training loss: 0.027169\n",
      "epoch: 9 [37120/60000 (62%)]\t training loss: 0.078074\n",
      "epoch: 9 [37440/60000 (62%)]\t training loss: 0.002813\n",
      "epoch: 9 [37760/60000 (63%)]\t training loss: 0.002812\n",
      "epoch: 9 [38080/60000 (63%)]\t training loss: 0.001468\n",
      "epoch: 9 [38400/60000 (64%)]\t training loss: 0.001749\n",
      "epoch: 9 [38720/60000 (65%)]\t training loss: 0.018623\n",
      "epoch: 9 [39040/60000 (65%)]\t training loss: 0.000848\n",
      "epoch: 9 [39360/60000 (66%)]\t training loss: 0.017147\n",
      "epoch: 9 [39680/60000 (66%)]\t training loss: 0.026792\n",
      "epoch: 9 [40000/60000 (67%)]\t training loss: 0.000183\n",
      "epoch: 9 [40320/60000 (67%)]\t training loss: 0.003827\n",
      "epoch: 9 [40640/60000 (68%)]\t training loss: 0.013050\n",
      "epoch: 9 [40960/60000 (68%)]\t training loss: 0.000009\n",
      "epoch: 9 [41280/60000 (69%)]\t training loss: 0.000329\n",
      "epoch: 9 [41600/60000 (69%)]\t training loss: 0.063533\n",
      "epoch: 9 [41920/60000 (70%)]\t training loss: 0.000743\n",
      "epoch: 9 [42240/60000 (70%)]\t training loss: 0.018874\n",
      "epoch: 9 [42560/60000 (71%)]\t training loss: 0.093808\n",
      "epoch: 9 [42880/60000 (71%)]\t training loss: 0.003960\n",
      "epoch: 9 [43200/60000 (72%)]\t training loss: 0.002056\n",
      "epoch: 9 [43520/60000 (73%)]\t training loss: 0.004788\n",
      "epoch: 9 [43840/60000 (73%)]\t training loss: 0.035509\n",
      "epoch: 9 [44160/60000 (74%)]\t training loss: 0.020765\n",
      "epoch: 9 [44480/60000 (74%)]\t training loss: 0.000951\n",
      "epoch: 9 [44800/60000 (75%)]\t training loss: 0.084985\n",
      "epoch: 9 [45120/60000 (75%)]\t training loss: 0.026467\n",
      "epoch: 9 [45440/60000 (76%)]\t training loss: 0.008798\n",
      "epoch: 9 [45760/60000 (76%)]\t training loss: 0.020253\n",
      "epoch: 9 [46080/60000 (77%)]\t training loss: 0.146140\n",
      "epoch: 9 [46400/60000 (77%)]\t training loss: 0.000334\n",
      "epoch: 9 [46720/60000 (78%)]\t training loss: 0.000301\n",
      "epoch: 9 [47040/60000 (78%)]\t training loss: 0.002848\n",
      "epoch: 9 [47360/60000 (79%)]\t training loss: 0.002114\n",
      "epoch: 9 [47680/60000 (79%)]\t training loss: 0.048704\n",
      "epoch: 9 [48000/60000 (80%)]\t training loss: 0.001189\n",
      "epoch: 9 [48320/60000 (81%)]\t training loss: 0.000129\n",
      "epoch: 9 [48640/60000 (81%)]\t training loss: 0.036193\n",
      "epoch: 9 [48960/60000 (82%)]\t training loss: 0.000549\n",
      "epoch: 9 [49280/60000 (82%)]\t training loss: 0.000199\n",
      "epoch: 9 [49600/60000 (83%)]\t training loss: 0.001652\n",
      "epoch: 9 [49920/60000 (83%)]\t training loss: 0.001193\n",
      "epoch: 9 [50240/60000 (84%)]\t training loss: 0.024547\n",
      "epoch: 9 [50560/60000 (84%)]\t training loss: 0.004104\n",
      "epoch: 9 [50880/60000 (85%)]\t training loss: 0.002354\n",
      "epoch: 9 [51200/60000 (85%)]\t training loss: 0.099940\n",
      "epoch: 9 [51520/60000 (86%)]\t training loss: 0.000316\n",
      "epoch: 9 [51840/60000 (86%)]\t training loss: 0.007552\n",
      "epoch: 9 [52160/60000 (87%)]\t training loss: 0.044311\n",
      "epoch: 9 [52480/60000 (87%)]\t training loss: 0.003240\n",
      "epoch: 9 [52800/60000 (88%)]\t training loss: 0.060638\n",
      "epoch: 9 [53120/60000 (89%)]\t training loss: 0.001562\n",
      "epoch: 9 [53440/60000 (89%)]\t training loss: 0.003041\n",
      "epoch: 9 [53760/60000 (90%)]\t training loss: 0.007418\n",
      "epoch: 9 [54080/60000 (90%)]\t training loss: 0.006798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 [54400/60000 (91%)]\t training loss: 0.000195\n",
      "epoch: 9 [54720/60000 (91%)]\t training loss: 0.000131\n",
      "epoch: 9 [55040/60000 (92%)]\t training loss: 0.007233\n",
      "epoch: 9 [55360/60000 (92%)]\t training loss: 0.027878\n",
      "epoch: 9 [55680/60000 (93%)]\t training loss: 0.002412\n",
      "epoch: 9 [56000/60000 (93%)]\t training loss: 0.003024\n",
      "epoch: 9 [56320/60000 (94%)]\t training loss: 0.000203\n",
      "epoch: 9 [56640/60000 (94%)]\t training loss: 0.001753\n",
      "epoch: 9 [56960/60000 (95%)]\t training loss: 0.001031\n",
      "epoch: 9 [57280/60000 (95%)]\t training loss: 0.002708\n",
      "epoch: 9 [57600/60000 (96%)]\t training loss: 0.066091\n",
      "epoch: 9 [57920/60000 (97%)]\t training loss: 0.000020\n",
      "epoch: 9 [58240/60000 (97%)]\t training loss: 0.008144\n",
      "epoch: 9 [58560/60000 (98%)]\t training loss: 0.012474\n",
      "epoch: 9 [58880/60000 (98%)]\t training loss: 0.009536\n",
      "epoch: 9 [59200/60000 (99%)]\t training loss: 0.048553\n",
      "epoch: 9 [59520/60000 (99%)]\t training loss: 0.072044\n",
      "epoch: 9 [59840/60000 (100%)]\t training loss: 0.047149\n",
      "\n",
      "Test dataset: Overall Loss: 0.0334, Overall Accuracy: 9895/10000 (99%)\n",
      "\n",
      "epoch: 10 [0/60000 (0%)]\t training loss: 0.037763\n",
      "epoch: 10 [320/60000 (1%)]\t training loss: 0.005444\n",
      "epoch: 10 [640/60000 (1%)]\t training loss: 0.017869\n",
      "epoch: 10 [960/60000 (2%)]\t training loss: 0.040207\n",
      "epoch: 10 [1280/60000 (2%)]\t training loss: 0.059691\n",
      "epoch: 10 [1600/60000 (3%)]\t training loss: 0.000086\n",
      "epoch: 10 [1920/60000 (3%)]\t training loss: 0.002042\n",
      "epoch: 10 [2240/60000 (4%)]\t training loss: 0.024435\n",
      "epoch: 10 [2560/60000 (4%)]\t training loss: 0.001311\n",
      "epoch: 10 [2880/60000 (5%)]\t training loss: 0.008291\n",
      "epoch: 10 [3200/60000 (5%)]\t training loss: 0.004554\n",
      "epoch: 10 [3520/60000 (6%)]\t training loss: 0.000160\n",
      "epoch: 10 [3840/60000 (6%)]\t training loss: 0.158602\n",
      "epoch: 10 [4160/60000 (7%)]\t training loss: 0.019845\n",
      "epoch: 10 [4480/60000 (7%)]\t training loss: 0.160069\n",
      "epoch: 10 [4800/60000 (8%)]\t training loss: 0.002646\n",
      "epoch: 10 [5120/60000 (9%)]\t training loss: 0.004351\n",
      "epoch: 10 [5440/60000 (9%)]\t training loss: 0.000092\n",
      "epoch: 10 [5760/60000 (10%)]\t training loss: 0.001780\n",
      "epoch: 10 [6080/60000 (10%)]\t training loss: 0.006430\n",
      "epoch: 10 [6400/60000 (11%)]\t training loss: 0.000371\n",
      "epoch: 10 [6720/60000 (11%)]\t training loss: 0.166561\n",
      "epoch: 10 [7040/60000 (12%)]\t training loss: 0.000111\n",
      "epoch: 10 [7360/60000 (12%)]\t training loss: 0.001001\n",
      "epoch: 10 [7680/60000 (13%)]\t training loss: 0.028147\n",
      "epoch: 10 [8000/60000 (13%)]\t training loss: 0.005424\n",
      "epoch: 10 [8320/60000 (14%)]\t training loss: 0.000224\n",
      "epoch: 10 [8640/60000 (14%)]\t training loss: 0.004471\n",
      "epoch: 10 [8960/60000 (15%)]\t training loss: 0.000052\n",
      "epoch: 10 [9280/60000 (15%)]\t training loss: 0.028157\n",
      "epoch: 10 [9600/60000 (16%)]\t training loss: 0.003831\n",
      "epoch: 10 [9920/60000 (17%)]\t training loss: 0.009042\n",
      "epoch: 10 [10240/60000 (17%)]\t training loss: 0.146120\n",
      "epoch: 10 [10560/60000 (18%)]\t training loss: 0.001135\n",
      "epoch: 10 [10880/60000 (18%)]\t training loss: 0.001174\n",
      "epoch: 10 [11200/60000 (19%)]\t training loss: 0.021577\n",
      "epoch: 10 [11520/60000 (19%)]\t training loss: 0.012437\n",
      "epoch: 10 [11840/60000 (20%)]\t training loss: 0.001226\n",
      "epoch: 10 [12160/60000 (20%)]\t training loss: 0.086645\n",
      "epoch: 10 [12480/60000 (21%)]\t training loss: 0.000226\n",
      "epoch: 10 [12800/60000 (21%)]\t training loss: 0.014248\n",
      "epoch: 10 [13120/60000 (22%)]\t training loss: 0.062598\n",
      "epoch: 10 [13440/60000 (22%)]\t training loss: 0.000292\n",
      "epoch: 10 [13760/60000 (23%)]\t training loss: 0.000021\n",
      "epoch: 10 [14080/60000 (23%)]\t training loss: 0.000916\n",
      "epoch: 10 [14400/60000 (24%)]\t training loss: 0.001106\n",
      "epoch: 10 [14720/60000 (25%)]\t training loss: 0.003334\n",
      "epoch: 10 [15040/60000 (25%)]\t training loss: 0.000309\n",
      "epoch: 10 [15360/60000 (26%)]\t training loss: 0.000154\n",
      "epoch: 10 [15680/60000 (26%)]\t training loss: 0.000047\n",
      "epoch: 10 [16000/60000 (27%)]\t training loss: 0.001032\n",
      "epoch: 10 [16320/60000 (27%)]\t training loss: 0.001266\n",
      "epoch: 10 [16640/60000 (28%)]\t training loss: 0.012067\n",
      "epoch: 10 [16960/60000 (28%)]\t training loss: 0.004725\n",
      "epoch: 10 [17280/60000 (29%)]\t training loss: 0.004620\n",
      "epoch: 10 [17600/60000 (29%)]\t training loss: 0.155813\n",
      "epoch: 10 [17920/60000 (30%)]\t training loss: 0.130389\n",
      "epoch: 10 [18240/60000 (30%)]\t training loss: 0.022756\n",
      "epoch: 10 [18560/60000 (31%)]\t training loss: 0.005778\n",
      "epoch: 10 [18880/60000 (31%)]\t training loss: 0.002397\n",
      "epoch: 10 [19200/60000 (32%)]\t training loss: 0.004903\n",
      "epoch: 10 [19520/60000 (33%)]\t training loss: 0.000409\n",
      "epoch: 10 [19840/60000 (33%)]\t training loss: 0.041190\n",
      "epoch: 10 [20160/60000 (34%)]\t training loss: 0.006316\n",
      "epoch: 10 [20480/60000 (34%)]\t training loss: 0.017798\n",
      "epoch: 10 [20800/60000 (35%)]\t training loss: 0.006430\n",
      "epoch: 10 [21120/60000 (35%)]\t training loss: 0.000848\n",
      "epoch: 10 [21440/60000 (36%)]\t training loss: 0.109243\n",
      "epoch: 10 [21760/60000 (36%)]\t training loss: 0.000512\n",
      "epoch: 10 [22080/60000 (37%)]\t training loss: 0.000706\n",
      "epoch: 10 [22400/60000 (37%)]\t training loss: 0.003141\n",
      "epoch: 10 [22720/60000 (38%)]\t training loss: 0.001263\n",
      "epoch: 10 [23040/60000 (38%)]\t training loss: 0.274327\n",
      "epoch: 10 [23360/60000 (39%)]\t training loss: 0.042292\n",
      "epoch: 10 [23680/60000 (39%)]\t training loss: 0.189541\n",
      "epoch: 10 [24000/60000 (40%)]\t training loss: 0.000471\n",
      "epoch: 10 [24320/60000 (41%)]\t training loss: 0.000559\n",
      "epoch: 10 [24640/60000 (41%)]\t training loss: 0.031485\n",
      "epoch: 10 [24960/60000 (42%)]\t training loss: 0.058683\n",
      "epoch: 10 [25280/60000 (42%)]\t training loss: 0.003698\n",
      "epoch: 10 [25600/60000 (43%)]\t training loss: 0.196274\n",
      "epoch: 10 [25920/60000 (43%)]\t training loss: 0.069291\n",
      "epoch: 10 [26240/60000 (44%)]\t training loss: 0.148848\n",
      "epoch: 10 [26560/60000 (44%)]\t training loss: 0.225429\n",
      "epoch: 10 [26880/60000 (45%)]\t training loss: 0.146190\n",
      "epoch: 10 [27200/60000 (45%)]\t training loss: 0.059696\n",
      "epoch: 10 [27520/60000 (46%)]\t training loss: 0.370109\n",
      "epoch: 10 [27840/60000 (46%)]\t training loss: 0.001419\n",
      "epoch: 10 [28160/60000 (47%)]\t training loss: 0.000101\n",
      "epoch: 10 [28480/60000 (47%)]\t training loss: 0.309806\n",
      "epoch: 10 [28800/60000 (48%)]\t training loss: 0.019627\n",
      "epoch: 10 [29120/60000 (49%)]\t training loss: 0.000585\n",
      "epoch: 10 [29440/60000 (49%)]\t training loss: 0.005244\n",
      "epoch: 10 [29760/60000 (50%)]\t training loss: 0.243611\n",
      "epoch: 10 [30080/60000 (50%)]\t training loss: 0.048050\n",
      "epoch: 10 [30400/60000 (51%)]\t training loss: 0.003928\n",
      "epoch: 10 [30720/60000 (51%)]\t training loss: 0.000827\n",
      "epoch: 10 [31040/60000 (52%)]\t training loss: 0.002018\n",
      "epoch: 10 [31360/60000 (52%)]\t training loss: 0.000083\n",
      "epoch: 10 [31680/60000 (53%)]\t training loss: 0.013857\n",
      "epoch: 10 [32000/60000 (53%)]\t training loss: 0.003191\n",
      "epoch: 10 [32320/60000 (54%)]\t training loss: 0.002159\n",
      "epoch: 10 [32640/60000 (54%)]\t training loss: 0.003355\n",
      "epoch: 10 [32960/60000 (55%)]\t training loss: 0.000782\n",
      "epoch: 10 [33280/60000 (55%)]\t training loss: 0.001803\n",
      "epoch: 10 [33600/60000 (56%)]\t training loss: 0.000645\n",
      "epoch: 10 [33920/60000 (57%)]\t training loss: 0.000113\n",
      "epoch: 10 [34240/60000 (57%)]\t training loss: 0.001355\n",
      "epoch: 10 [34560/60000 (58%)]\t training loss: 0.000190\n",
      "epoch: 10 [34880/60000 (58%)]\t training loss: 0.023245\n",
      "epoch: 10 [35200/60000 (59%)]\t training loss: 0.010054\n",
      "epoch: 10 [35520/60000 (59%)]\t training loss: 0.001392\n",
      "epoch: 10 [35840/60000 (60%)]\t training loss: 0.019885\n",
      "epoch: 10 [36160/60000 (60%)]\t training loss: 0.002298\n",
      "epoch: 10 [36480/60000 (61%)]\t training loss: 0.000048\n",
      "epoch: 10 [36800/60000 (61%)]\t training loss: 0.000360\n",
      "epoch: 10 [37120/60000 (62%)]\t training loss: 0.000550\n",
      "epoch: 10 [37440/60000 (62%)]\t training loss: 0.090443\n",
      "epoch: 10 [37760/60000 (63%)]\t training loss: 0.146847\n",
      "epoch: 10 [38080/60000 (63%)]\t training loss: 0.075643\n",
      "epoch: 10 [38400/60000 (64%)]\t training loss: 0.000569\n",
      "epoch: 10 [38720/60000 (65%)]\t training loss: 0.003205\n",
      "epoch: 10 [39040/60000 (65%)]\t training loss: 0.079786\n",
      "epoch: 10 [39360/60000 (66%)]\t training loss: 0.004058\n",
      "epoch: 10 [39680/60000 (66%)]\t training loss: 0.032112\n",
      "epoch: 10 [40000/60000 (67%)]\t training loss: 0.001089\n",
      "epoch: 10 [40320/60000 (67%)]\t training loss: 0.006263\n",
      "epoch: 10 [40640/60000 (68%)]\t training loss: 0.034315\n",
      "epoch: 10 [40960/60000 (68%)]\t training loss: 0.001126\n",
      "epoch: 10 [41280/60000 (69%)]\t training loss: 0.006114\n",
      "epoch: 10 [41600/60000 (69%)]\t training loss: 0.017919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 [41920/60000 (70%)]\t training loss: 0.000205\n",
      "epoch: 10 [42240/60000 (70%)]\t training loss: 0.002017\n",
      "epoch: 10 [42560/60000 (71%)]\t training loss: 0.022898\n",
      "epoch: 10 [42880/60000 (71%)]\t training loss: 0.065059\n",
      "epoch: 10 [43200/60000 (72%)]\t training loss: 0.000336\n",
      "epoch: 10 [43520/60000 (73%)]\t training loss: 0.006003\n",
      "epoch: 10 [43840/60000 (73%)]\t training loss: 0.003943\n",
      "epoch: 10 [44160/60000 (74%)]\t training loss: 0.010917\n",
      "epoch: 10 [44480/60000 (74%)]\t training loss: 0.006168\n",
      "epoch: 10 [44800/60000 (75%)]\t training loss: 0.002572\n",
      "epoch: 10 [45120/60000 (75%)]\t training loss: 0.022804\n",
      "epoch: 10 [45440/60000 (76%)]\t training loss: 0.001737\n",
      "epoch: 10 [45760/60000 (76%)]\t training loss: 0.057166\n",
      "epoch: 10 [46080/60000 (77%)]\t training loss: 0.001228\n",
      "epoch: 10 [46400/60000 (77%)]\t training loss: 0.046815\n",
      "epoch: 10 [46720/60000 (78%)]\t training loss: 0.002258\n",
      "epoch: 10 [47040/60000 (78%)]\t training loss: 0.001428\n",
      "epoch: 10 [47360/60000 (79%)]\t training loss: 0.046284\n",
      "epoch: 10 [47680/60000 (79%)]\t training loss: 0.006744\n",
      "epoch: 10 [48000/60000 (80%)]\t training loss: 0.010517\n",
      "epoch: 10 [48320/60000 (81%)]\t training loss: 0.007967\n",
      "epoch: 10 [48640/60000 (81%)]\t training loss: 0.000093\n",
      "epoch: 10 [48960/60000 (82%)]\t training loss: 0.003970\n",
      "epoch: 10 [49280/60000 (82%)]\t training loss: 0.000198\n",
      "epoch: 10 [49600/60000 (83%)]\t training loss: 0.107015\n",
      "epoch: 10 [49920/60000 (83%)]\t training loss: 0.029139\n",
      "epoch: 10 [50240/60000 (84%)]\t training loss: 0.005407\n",
      "epoch: 10 [50560/60000 (84%)]\t training loss: 0.001476\n",
      "epoch: 10 [50880/60000 (85%)]\t training loss: 0.000181\n",
      "epoch: 10 [51200/60000 (85%)]\t training loss: 0.142509\n",
      "epoch: 10 [51520/60000 (86%)]\t training loss: 0.008645\n",
      "epoch: 10 [51840/60000 (86%)]\t training loss: 0.011422\n",
      "epoch: 10 [52160/60000 (87%)]\t training loss: 0.196452\n",
      "epoch: 10 [52480/60000 (87%)]\t training loss: 0.003380\n",
      "epoch: 10 [52800/60000 (88%)]\t training loss: 0.005067\n",
      "epoch: 10 [53120/60000 (89%)]\t training loss: 0.000226\n",
      "epoch: 10 [53440/60000 (89%)]\t training loss: 0.003630\n",
      "epoch: 10 [53760/60000 (90%)]\t training loss: 0.001102\n",
      "epoch: 10 [54080/60000 (90%)]\t training loss: 0.089747\n",
      "epoch: 10 [54400/60000 (91%)]\t training loss: 0.015156\n",
      "epoch: 10 [54720/60000 (91%)]\t training loss: 0.000617\n",
      "epoch: 10 [55040/60000 (92%)]\t training loss: 0.000993\n",
      "epoch: 10 [55360/60000 (92%)]\t training loss: 0.001966\n",
      "epoch: 10 [55680/60000 (93%)]\t training loss: 0.026366\n",
      "epoch: 10 [56000/60000 (93%)]\t training loss: 0.030657\n",
      "epoch: 10 [56320/60000 (94%)]\t training loss: 0.000317\n",
      "epoch: 10 [56640/60000 (94%)]\t training loss: 0.000464\n",
      "epoch: 10 [56960/60000 (95%)]\t training loss: 0.010518\n",
      "epoch: 10 [57280/60000 (95%)]\t training loss: 0.093068\n",
      "epoch: 10 [57600/60000 (96%)]\t training loss: 0.065545\n",
      "epoch: 10 [57920/60000 (97%)]\t training loss: 0.001039\n",
      "epoch: 10 [58240/60000 (97%)]\t training loss: 0.013808\n",
      "epoch: 10 [58560/60000 (98%)]\t training loss: 0.457773\n",
      "epoch: 10 [58880/60000 (98%)]\t training loss: 0.000092\n",
      "epoch: 10 [59200/60000 (99%)]\t training loss: 0.022337\n",
      "epoch: 10 [59520/60000 (99%)]\t training loss: 0.003370\n",
      "epoch: 10 [59840/60000 (100%)]\t training loss: 0.000599\n",
      "\n",
      "Test dataset: Overall Loss: 0.0350, Overall Accuracy: 9907/10000 (99%)\n",
      "\n",
      "epoch: 11 [0/60000 (0%)]\t training loss: 0.000115\n",
      "epoch: 11 [320/60000 (1%)]\t training loss: 0.002362\n",
      "epoch: 11 [640/60000 (1%)]\t training loss: 0.045372\n",
      "epoch: 11 [960/60000 (2%)]\t training loss: 0.013009\n",
      "epoch: 11 [1280/60000 (2%)]\t training loss: 0.061509\n",
      "epoch: 11 [1600/60000 (3%)]\t training loss: 0.031532\n",
      "epoch: 11 [1920/60000 (3%)]\t training loss: 0.006094\n",
      "epoch: 11 [2240/60000 (4%)]\t training loss: 0.003058\n",
      "epoch: 11 [2560/60000 (4%)]\t training loss: 0.023411\n",
      "epoch: 11 [2880/60000 (5%)]\t training loss: 0.000312\n",
      "epoch: 11 [3200/60000 (5%)]\t training loss: 0.000123\n",
      "epoch: 11 [3520/60000 (6%)]\t training loss: 0.000101\n",
      "epoch: 11 [3840/60000 (6%)]\t training loss: 0.159478\n",
      "epoch: 11 [4160/60000 (7%)]\t training loss: 0.001533\n",
      "epoch: 11 [4480/60000 (7%)]\t training loss: 0.054566\n",
      "epoch: 11 [4800/60000 (8%)]\t training loss: 0.000275\n",
      "epoch: 11 [5120/60000 (9%)]\t training loss: 0.000244\n",
      "epoch: 11 [5440/60000 (9%)]\t training loss: 0.004150\n",
      "epoch: 11 [5760/60000 (10%)]\t training loss: 0.002837\n",
      "epoch: 11 [6080/60000 (10%)]\t training loss: 0.134216\n",
      "epoch: 11 [6400/60000 (11%)]\t training loss: 0.000064\n",
      "epoch: 11 [6720/60000 (11%)]\t training loss: 0.145910\n",
      "epoch: 11 [7040/60000 (12%)]\t training loss: 0.002037\n",
      "epoch: 11 [7360/60000 (12%)]\t training loss: 0.002101\n",
      "epoch: 11 [7680/60000 (13%)]\t training loss: 0.002333\n",
      "epoch: 11 [8000/60000 (13%)]\t training loss: 0.001156\n",
      "epoch: 11 [8320/60000 (14%)]\t training loss: 0.028475\n",
      "epoch: 11 [8640/60000 (14%)]\t training loss: 0.000008\n",
      "epoch: 11 [8960/60000 (15%)]\t training loss: 0.041083\n",
      "epoch: 11 [9280/60000 (15%)]\t training loss: 0.035297\n",
      "epoch: 11 [9600/60000 (16%)]\t training loss: 0.000082\n",
      "epoch: 11 [9920/60000 (17%)]\t training loss: 0.013282\n",
      "epoch: 11 [10240/60000 (17%)]\t training loss: 0.067380\n",
      "epoch: 11 [10560/60000 (18%)]\t training loss: 0.002868\n",
      "epoch: 11 [10880/60000 (18%)]\t training loss: 0.000041\n",
      "epoch: 11 [11200/60000 (19%)]\t training loss: 0.052428\n",
      "epoch: 11 [11520/60000 (19%)]\t training loss: 0.005158\n",
      "epoch: 11 [11840/60000 (20%)]\t training loss: 0.028784\n",
      "epoch: 11 [12160/60000 (20%)]\t training loss: 0.019147\n",
      "epoch: 11 [12480/60000 (21%)]\t training loss: 0.008710\n",
      "epoch: 11 [12800/60000 (21%)]\t training loss: 0.027885\n",
      "epoch: 11 [13120/60000 (22%)]\t training loss: 0.000073\n",
      "epoch: 11 [13440/60000 (22%)]\t training loss: 0.000369\n",
      "epoch: 11 [13760/60000 (23%)]\t training loss: 0.049053\n",
      "epoch: 11 [14080/60000 (23%)]\t training loss: 0.000901\n",
      "epoch: 11 [14400/60000 (24%)]\t training loss: 0.000704\n",
      "epoch: 11 [14720/60000 (25%)]\t training loss: 0.000172\n",
      "epoch: 11 [15040/60000 (25%)]\t training loss: 0.030209\n",
      "epoch: 11 [15360/60000 (26%)]\t training loss: 0.073199\n",
      "epoch: 11 [15680/60000 (26%)]\t training loss: 0.011827\n",
      "epoch: 11 [16000/60000 (27%)]\t training loss: 0.002498\n",
      "epoch: 11 [16320/60000 (27%)]\t training loss: 0.000026\n",
      "epoch: 11 [16640/60000 (28%)]\t training loss: 0.003543\n",
      "epoch: 11 [16960/60000 (28%)]\t training loss: 0.000012\n",
      "epoch: 11 [17280/60000 (29%)]\t training loss: 0.000295\n",
      "epoch: 11 [17600/60000 (29%)]\t training loss: 0.002222\n",
      "epoch: 11 [17920/60000 (30%)]\t training loss: 0.005881\n",
      "epoch: 11 [18240/60000 (30%)]\t training loss: 0.003046\n",
      "epoch: 11 [18560/60000 (31%)]\t training loss: 0.000144\n",
      "epoch: 11 [18880/60000 (31%)]\t training loss: 0.000578\n",
      "epoch: 11 [19200/60000 (32%)]\t training loss: 0.000389\n",
      "epoch: 11 [19520/60000 (33%)]\t training loss: 0.000087\n",
      "epoch: 11 [19840/60000 (33%)]\t training loss: 0.000226\n",
      "epoch: 11 [20160/60000 (34%)]\t training loss: 0.000985\n",
      "epoch: 11 [20480/60000 (34%)]\t training loss: 0.101636\n",
      "epoch: 11 [20800/60000 (35%)]\t training loss: 0.002034\n",
      "epoch: 11 [21120/60000 (35%)]\t training loss: 0.007084\n",
      "epoch: 11 [21440/60000 (36%)]\t training loss: 0.001004\n",
      "epoch: 11 [21760/60000 (36%)]\t training loss: 0.001194\n",
      "epoch: 11 [22080/60000 (37%)]\t training loss: 0.000300\n",
      "epoch: 11 [22400/60000 (37%)]\t training loss: 0.000943\n",
      "epoch: 11 [22720/60000 (38%)]\t training loss: 0.017050\n",
      "epoch: 11 [23040/60000 (38%)]\t training loss: 0.000033\n",
      "epoch: 11 [23360/60000 (39%)]\t training loss: 0.002058\n",
      "epoch: 11 [23680/60000 (39%)]\t training loss: 0.000017\n",
      "epoch: 11 [24000/60000 (40%)]\t training loss: 0.000231\n",
      "epoch: 11 [24320/60000 (41%)]\t training loss: 0.001561\n",
      "epoch: 11 [24640/60000 (41%)]\t training loss: 0.001263\n",
      "epoch: 11 [24960/60000 (42%)]\t training loss: 0.005904\n",
      "epoch: 11 [25280/60000 (42%)]\t training loss: 0.001382\n",
      "epoch: 11 [25600/60000 (43%)]\t training loss: 0.036327\n",
      "epoch: 11 [25920/60000 (43%)]\t training loss: 0.005315\n",
      "epoch: 11 [26240/60000 (44%)]\t training loss: 0.001062\n",
      "epoch: 11 [26560/60000 (44%)]\t training loss: 0.000042\n",
      "epoch: 11 [26880/60000 (45%)]\t training loss: 0.000076\n",
      "epoch: 11 [27200/60000 (45%)]\t training loss: 0.057909\n",
      "epoch: 11 [27520/60000 (46%)]\t training loss: 0.084386\n",
      "epoch: 11 [27840/60000 (46%)]\t training loss: 0.105266\n",
      "epoch: 11 [28160/60000 (47%)]\t training loss: 0.000176\n",
      "epoch: 11 [28480/60000 (47%)]\t training loss: 0.025503\n",
      "epoch: 11 [28800/60000 (48%)]\t training loss: 0.002229\n",
      "epoch: 11 [29120/60000 (49%)]\t training loss: 0.125891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11 [29440/60000 (49%)]\t training loss: 0.019088\n",
      "epoch: 11 [29760/60000 (50%)]\t training loss: 0.153378\n",
      "epoch: 11 [30080/60000 (50%)]\t training loss: 0.035859\n",
      "epoch: 11 [30400/60000 (51%)]\t training loss: 0.002414\n",
      "epoch: 11 [30720/60000 (51%)]\t training loss: 0.011015\n",
      "epoch: 11 [31040/60000 (52%)]\t training loss: 0.000301\n",
      "epoch: 11 [31360/60000 (52%)]\t training loss: 0.089588\n",
      "epoch: 11 [31680/60000 (53%)]\t training loss: 0.035275\n",
      "epoch: 11 [32000/60000 (53%)]\t training loss: 0.013083\n",
      "epoch: 11 [32320/60000 (54%)]\t training loss: 0.199120\n",
      "epoch: 11 [32640/60000 (54%)]\t training loss: 0.005549\n",
      "epoch: 11 [32960/60000 (55%)]\t training loss: 0.006944\n",
      "epoch: 11 [33280/60000 (55%)]\t training loss: 0.000013\n",
      "epoch: 11 [33600/60000 (56%)]\t training loss: 0.010285\n",
      "epoch: 11 [33920/60000 (57%)]\t training loss: 0.000377\n",
      "epoch: 11 [34240/60000 (57%)]\t training loss: 0.044127\n",
      "epoch: 11 [34560/60000 (58%)]\t training loss: 0.007240\n",
      "epoch: 11 [34880/60000 (58%)]\t training loss: 0.065524\n",
      "epoch: 11 [35200/60000 (59%)]\t training loss: 0.163553\n",
      "epoch: 11 [35520/60000 (59%)]\t training loss: 0.000106\n",
      "epoch: 11 [35840/60000 (60%)]\t training loss: 0.000283\n",
      "epoch: 11 [36160/60000 (60%)]\t training loss: 0.001158\n",
      "epoch: 11 [36480/60000 (61%)]\t training loss: 0.012694\n",
      "epoch: 11 [36800/60000 (61%)]\t training loss: 0.076148\n",
      "epoch: 11 [37120/60000 (62%)]\t training loss: 0.001241\n",
      "epoch: 11 [37440/60000 (62%)]\t training loss: 0.000106\n",
      "epoch: 11 [37760/60000 (63%)]\t training loss: 0.001209\n",
      "epoch: 11 [38080/60000 (63%)]\t training loss: 0.005820\n",
      "epoch: 11 [38400/60000 (64%)]\t training loss: 0.001149\n",
      "epoch: 11 [38720/60000 (65%)]\t training loss: 0.000519\n",
      "epoch: 11 [39040/60000 (65%)]\t training loss: 0.000062\n",
      "epoch: 11 [39360/60000 (66%)]\t training loss: 0.011462\n",
      "epoch: 11 [39680/60000 (66%)]\t training loss: 0.000049\n",
      "epoch: 11 [40000/60000 (67%)]\t training loss: 0.000270\n",
      "epoch: 11 [40320/60000 (67%)]\t training loss: 0.000277\n",
      "epoch: 11 [40640/60000 (68%)]\t training loss: 0.000048\n",
      "epoch: 11 [40960/60000 (68%)]\t training loss: 0.013905\n",
      "epoch: 11 [41280/60000 (69%)]\t training loss: 0.004309\n",
      "epoch: 11 [41600/60000 (69%)]\t training loss: 0.107037\n",
      "epoch: 11 [41920/60000 (70%)]\t training loss: 0.003181\n",
      "epoch: 11 [42240/60000 (70%)]\t training loss: 0.049885\n",
      "epoch: 11 [42560/60000 (71%)]\t training loss: 0.048132\n",
      "epoch: 11 [42880/60000 (71%)]\t training loss: 0.001453\n",
      "epoch: 11 [43200/60000 (72%)]\t training loss: 0.000415\n",
      "epoch: 11 [43520/60000 (73%)]\t training loss: 0.016330\n",
      "epoch: 11 [43840/60000 (73%)]\t training loss: 0.003867\n",
      "epoch: 11 [44160/60000 (74%)]\t training loss: 0.001736\n",
      "epoch: 11 [44480/60000 (74%)]\t training loss: 0.001107\n",
      "epoch: 11 [44800/60000 (75%)]\t training loss: 0.028856\n",
      "epoch: 11 [45120/60000 (75%)]\t training loss: 0.002406\n",
      "epoch: 11 [45440/60000 (76%)]\t training loss: 0.001417\n",
      "epoch: 11 [45760/60000 (76%)]\t training loss: 0.000748\n",
      "epoch: 11 [46080/60000 (77%)]\t training loss: 0.002892\n",
      "epoch: 11 [46400/60000 (77%)]\t training loss: 0.010237\n",
      "epoch: 11 [46720/60000 (78%)]\t training loss: 0.013118\n",
      "epoch: 11 [47040/60000 (78%)]\t training loss: 0.082977\n",
      "epoch: 11 [47360/60000 (79%)]\t training loss: 0.000577\n",
      "epoch: 11 [47680/60000 (79%)]\t training loss: 0.000556\n",
      "epoch: 11 [48000/60000 (80%)]\t training loss: 0.001740\n",
      "epoch: 11 [48320/60000 (81%)]\t training loss: 0.000147\n",
      "epoch: 11 [48640/60000 (81%)]\t training loss: 0.000231\n",
      "epoch: 11 [48960/60000 (82%)]\t training loss: 0.021540\n",
      "epoch: 11 [49280/60000 (82%)]\t training loss: 0.022689\n",
      "epoch: 11 [49600/60000 (83%)]\t training loss: 0.027982\n",
      "epoch: 11 [49920/60000 (83%)]\t training loss: 0.010226\n",
      "epoch: 11 [50240/60000 (84%)]\t training loss: 0.016548\n",
      "epoch: 11 [50560/60000 (84%)]\t training loss: 0.071561\n",
      "epoch: 11 [50880/60000 (85%)]\t training loss: 0.006655\n",
      "epoch: 11 [51200/60000 (85%)]\t training loss: 0.000207\n",
      "epoch: 11 [51520/60000 (86%)]\t training loss: 0.000515\n",
      "epoch: 11 [51840/60000 (86%)]\t training loss: 0.099787\n",
      "epoch: 11 [52160/60000 (87%)]\t training loss: 0.055262\n",
      "epoch: 11 [52480/60000 (87%)]\t training loss: 0.044102\n",
      "epoch: 11 [52800/60000 (88%)]\t training loss: 0.011826\n",
      "epoch: 11 [53120/60000 (89%)]\t training loss: 0.000372\n",
      "epoch: 11 [53440/60000 (89%)]\t training loss: 0.000881\n",
      "epoch: 11 [53760/60000 (90%)]\t training loss: 0.000174\n",
      "epoch: 11 [54080/60000 (90%)]\t training loss: 0.002966\n",
      "epoch: 11 [54400/60000 (91%)]\t training loss: 0.005599\n",
      "epoch: 11 [54720/60000 (91%)]\t training loss: 0.000552\n",
      "epoch: 11 [55040/60000 (92%)]\t training loss: 0.000183\n",
      "epoch: 11 [55360/60000 (92%)]\t training loss: 0.000936\n",
      "epoch: 11 [55680/60000 (93%)]\t training loss: 0.020306\n",
      "epoch: 11 [56000/60000 (93%)]\t training loss: 0.191169\n",
      "epoch: 11 [56320/60000 (94%)]\t training loss: 0.001293\n",
      "epoch: 11 [56640/60000 (94%)]\t training loss: 0.002386\n",
      "epoch: 11 [56960/60000 (95%)]\t training loss: 0.003692\n",
      "epoch: 11 [57280/60000 (95%)]\t training loss: 0.003173\n",
      "epoch: 11 [57600/60000 (96%)]\t training loss: 0.017093\n",
      "epoch: 11 [57920/60000 (97%)]\t training loss: 0.015004\n",
      "epoch: 11 [58240/60000 (97%)]\t training loss: 0.002141\n",
      "epoch: 11 [58560/60000 (98%)]\t training loss: 0.004120\n",
      "epoch: 11 [58880/60000 (98%)]\t training loss: 0.003406\n",
      "epoch: 11 [59200/60000 (99%)]\t training loss: 0.001125\n",
      "epoch: 11 [59520/60000 (99%)]\t training loss: 0.077683\n",
      "epoch: 11 [59840/60000 (100%)]\t training loss: 0.000240\n",
      "\n",
      "Test dataset: Overall Loss: 0.0351, Overall Accuracy: 9905/10000 (99%)\n",
      "\n",
      "epoch: 12 [0/60000 (0%)]\t training loss: 0.008184\n",
      "epoch: 12 [320/60000 (1%)]\t training loss: 0.002955\n",
      "epoch: 12 [640/60000 (1%)]\t training loss: 0.002446\n",
      "epoch: 12 [960/60000 (2%)]\t training loss: 0.000124\n",
      "epoch: 12 [1280/60000 (2%)]\t training loss: 0.015373\n",
      "epoch: 12 [1600/60000 (3%)]\t training loss: 0.001837\n",
      "epoch: 12 [1920/60000 (3%)]\t training loss: 0.003551\n",
      "epoch: 12 [2240/60000 (4%)]\t training loss: 0.002411\n",
      "epoch: 12 [2560/60000 (4%)]\t training loss: 0.000186\n",
      "epoch: 12 [2880/60000 (5%)]\t training loss: 0.010386\n",
      "epoch: 12 [3200/60000 (5%)]\t training loss: 0.002987\n",
      "epoch: 12 [3520/60000 (6%)]\t training loss: 0.001261\n",
      "epoch: 12 [3840/60000 (6%)]\t training loss: 0.041864\n",
      "epoch: 12 [4160/60000 (7%)]\t training loss: 0.001242\n",
      "epoch: 12 [4480/60000 (7%)]\t training loss: 0.000422\n",
      "epoch: 12 [4800/60000 (8%)]\t training loss: 0.078030\n",
      "epoch: 12 [5120/60000 (9%)]\t training loss: 0.380220\n",
      "epoch: 12 [5440/60000 (9%)]\t training loss: 0.002767\n",
      "epoch: 12 [5760/60000 (10%)]\t training loss: 0.003709\n",
      "epoch: 12 [6080/60000 (10%)]\t training loss: 0.000008\n",
      "epoch: 12 [6400/60000 (11%)]\t training loss: 0.036810\n",
      "epoch: 12 [6720/60000 (11%)]\t training loss: 0.003245\n",
      "epoch: 12 [7040/60000 (12%)]\t training loss: 0.005259\n",
      "epoch: 12 [7360/60000 (12%)]\t training loss: 0.009585\n",
      "epoch: 12 [7680/60000 (13%)]\t training loss: 0.009504\n",
      "epoch: 12 [8000/60000 (13%)]\t training loss: 0.109589\n",
      "epoch: 12 [8320/60000 (14%)]\t training loss: 0.071943\n",
      "epoch: 12 [8640/60000 (14%)]\t training loss: 0.067670\n",
      "epoch: 12 [8960/60000 (15%)]\t training loss: 0.013100\n",
      "epoch: 12 [9280/60000 (15%)]\t training loss: 0.000346\n",
      "epoch: 12 [9600/60000 (16%)]\t training loss: 0.000011\n",
      "epoch: 12 [9920/60000 (17%)]\t training loss: 0.000386\n",
      "epoch: 12 [10240/60000 (17%)]\t training loss: 0.323874\n",
      "epoch: 12 [10560/60000 (18%)]\t training loss: 0.023845\n",
      "epoch: 12 [10880/60000 (18%)]\t training loss: 0.000563\n",
      "epoch: 12 [11200/60000 (19%)]\t training loss: 0.000229\n",
      "epoch: 12 [11520/60000 (19%)]\t training loss: 0.000007\n",
      "epoch: 12 [11840/60000 (20%)]\t training loss: 0.000028\n",
      "epoch: 12 [12160/60000 (20%)]\t training loss: 0.000108\n",
      "epoch: 12 [12480/60000 (21%)]\t training loss: 0.000291\n",
      "epoch: 12 [12800/60000 (21%)]\t training loss: 0.000032\n",
      "epoch: 12 [13120/60000 (22%)]\t training loss: 0.005511\n",
      "epoch: 12 [13440/60000 (22%)]\t training loss: 0.000001\n",
      "epoch: 12 [13760/60000 (23%)]\t training loss: 0.000086\n",
      "epoch: 12 [14080/60000 (23%)]\t training loss: 0.006449\n",
      "epoch: 12 [14400/60000 (24%)]\t training loss: 0.007833\n",
      "epoch: 12 [14720/60000 (25%)]\t training loss: 0.000031\n",
      "epoch: 12 [15040/60000 (25%)]\t training loss: 0.000243\n",
      "epoch: 12 [15360/60000 (26%)]\t training loss: 0.031905\n",
      "epoch: 12 [15680/60000 (26%)]\t training loss: 0.147182\n",
      "epoch: 12 [16000/60000 (27%)]\t training loss: 0.001062\n",
      "epoch: 12 [16320/60000 (27%)]\t training loss: 0.000376\n",
      "epoch: 12 [16640/60000 (28%)]\t training loss: 0.050733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12 [16960/60000 (28%)]\t training loss: 0.038294\n",
      "epoch: 12 [17280/60000 (29%)]\t training loss: 0.003550\n",
      "epoch: 12 [17600/60000 (29%)]\t training loss: 0.001535\n",
      "epoch: 12 [17920/60000 (30%)]\t training loss: 0.001515\n",
      "epoch: 12 [18240/60000 (30%)]\t training loss: 0.001292\n",
      "epoch: 12 [18560/60000 (31%)]\t training loss: 0.000542\n",
      "epoch: 12 [18880/60000 (31%)]\t training loss: 0.000178\n",
      "epoch: 12 [19200/60000 (32%)]\t training loss: 0.000172\n",
      "epoch: 12 [19520/60000 (33%)]\t training loss: 0.000380\n",
      "epoch: 12 [19840/60000 (33%)]\t training loss: 0.086875\n",
      "epoch: 12 [20160/60000 (34%)]\t training loss: 0.000794\n",
      "epoch: 12 [20480/60000 (34%)]\t training loss: 0.001335\n",
      "epoch: 12 [20800/60000 (35%)]\t training loss: 0.027173\n",
      "epoch: 12 [21120/60000 (35%)]\t training loss: 0.070624\n",
      "epoch: 12 [21440/60000 (36%)]\t training loss: 0.000093\n",
      "epoch: 12 [21760/60000 (36%)]\t training loss: 0.000063\n",
      "epoch: 12 [22080/60000 (37%)]\t training loss: 0.002238\n",
      "epoch: 12 [22400/60000 (37%)]\t training loss: 0.002341\n",
      "epoch: 12 [22720/60000 (38%)]\t training loss: 0.006773\n",
      "epoch: 12 [23040/60000 (38%)]\t training loss: 0.000055\n",
      "epoch: 12 [23360/60000 (39%)]\t training loss: 0.000126\n",
      "epoch: 12 [23680/60000 (39%)]\t training loss: 0.000517\n",
      "epoch: 12 [24000/60000 (40%)]\t training loss: 0.003878\n",
      "epoch: 12 [24320/60000 (41%)]\t training loss: 0.000322\n",
      "epoch: 12 [24640/60000 (41%)]\t training loss: 0.000198\n",
      "epoch: 12 [24960/60000 (42%)]\t training loss: 0.000462\n",
      "epoch: 12 [25280/60000 (42%)]\t training loss: 0.002385\n",
      "epoch: 12 [25600/60000 (43%)]\t training loss: 0.164892\n",
      "epoch: 12 [25920/60000 (43%)]\t training loss: 0.000643\n",
      "epoch: 12 [26240/60000 (44%)]\t training loss: 0.000301\n",
      "epoch: 12 [26560/60000 (44%)]\t training loss: 0.006881\n",
      "epoch: 12 [26880/60000 (45%)]\t training loss: 0.001436\n",
      "epoch: 12 [27200/60000 (45%)]\t training loss: 0.000296\n",
      "epoch: 12 [27520/60000 (46%)]\t training loss: 0.000140\n",
      "epoch: 12 [27840/60000 (46%)]\t training loss: 0.000065\n",
      "epoch: 12 [28160/60000 (47%)]\t training loss: 0.000291\n",
      "epoch: 12 [28480/60000 (47%)]\t training loss: 0.002539\n",
      "epoch: 12 [28800/60000 (48%)]\t training loss: 0.034068\n",
      "epoch: 12 [29120/60000 (49%)]\t training loss: 0.006775\n",
      "epoch: 12 [29440/60000 (49%)]\t training loss: 0.000073\n",
      "epoch: 12 [29760/60000 (50%)]\t training loss: 0.018354\n",
      "epoch: 12 [30080/60000 (50%)]\t training loss: 0.001066\n",
      "epoch: 12 [30400/60000 (51%)]\t training loss: 0.001611\n",
      "epoch: 12 [30720/60000 (51%)]\t training loss: 0.000869\n",
      "epoch: 12 [31040/60000 (52%)]\t training loss: 0.041681\n",
      "epoch: 12 [31360/60000 (52%)]\t training loss: 0.000475\n",
      "epoch: 12 [31680/60000 (53%)]\t training loss: 0.007893\n",
      "epoch: 12 [32000/60000 (53%)]\t training loss: 0.014171\n",
      "epoch: 12 [32320/60000 (54%)]\t training loss: 0.003620\n",
      "epoch: 12 [32640/60000 (54%)]\t training loss: 0.003539\n",
      "epoch: 12 [32960/60000 (55%)]\t training loss: 0.384384\n",
      "epoch: 12 [33280/60000 (55%)]\t training loss: 0.005268\n",
      "epoch: 12 [33600/60000 (56%)]\t training loss: 0.001246\n",
      "epoch: 12 [33920/60000 (57%)]\t training loss: 0.063485\n",
      "epoch: 12 [34240/60000 (57%)]\t training loss: 0.008231\n",
      "epoch: 12 [34560/60000 (58%)]\t training loss: 0.020253\n",
      "epoch: 12 [34880/60000 (58%)]\t training loss: 0.000003\n",
      "epoch: 12 [35200/60000 (59%)]\t training loss: 0.000214\n",
      "epoch: 12 [35520/60000 (59%)]\t training loss: 0.016224\n",
      "epoch: 12 [35840/60000 (60%)]\t training loss: 0.000415\n",
      "epoch: 12 [36160/60000 (60%)]\t training loss: 0.006825\n",
      "epoch: 12 [36480/60000 (61%)]\t training loss: 0.000002\n",
      "epoch: 12 [36800/60000 (61%)]\t training loss: 0.046732\n",
      "epoch: 12 [37120/60000 (62%)]\t training loss: 0.096155\n",
      "epoch: 12 [37440/60000 (62%)]\t training loss: 0.000994\n",
      "epoch: 12 [37760/60000 (63%)]\t training loss: 0.000771\n",
      "epoch: 12 [38080/60000 (63%)]\t training loss: 0.007862\n",
      "epoch: 12 [38400/60000 (64%)]\t training loss: 0.035671\n",
      "epoch: 12 [38720/60000 (65%)]\t training loss: 0.000954\n",
      "epoch: 12 [39040/60000 (65%)]\t training loss: 0.000798\n",
      "epoch: 12 [39360/60000 (66%)]\t training loss: 0.002054\n",
      "epoch: 12 [39680/60000 (66%)]\t training loss: 0.052598\n",
      "epoch: 12 [40000/60000 (67%)]\t training loss: 0.044524\n",
      "epoch: 12 [40320/60000 (67%)]\t training loss: 0.004201\n",
      "epoch: 12 [40640/60000 (68%)]\t training loss: 0.035372\n",
      "epoch: 12 [40960/60000 (68%)]\t training loss: 0.000003\n",
      "epoch: 12 [41280/60000 (69%)]\t training loss: 0.001808\n",
      "epoch: 12 [41600/60000 (69%)]\t training loss: 0.000668\n",
      "epoch: 12 [41920/60000 (70%)]\t training loss: 0.000507\n",
      "epoch: 12 [42240/60000 (70%)]\t training loss: 0.010442\n",
      "epoch: 12 [42560/60000 (71%)]\t training loss: 0.000307\n",
      "epoch: 12 [42880/60000 (71%)]\t training loss: 0.001530\n",
      "epoch: 12 [43200/60000 (72%)]\t training loss: 0.094400\n",
      "epoch: 12 [43520/60000 (73%)]\t training loss: 0.000253\n",
      "epoch: 12 [43840/60000 (73%)]\t training loss: 0.000891\n",
      "epoch: 12 [44160/60000 (74%)]\t training loss: 0.180666\n",
      "epoch: 12 [44480/60000 (74%)]\t training loss: 0.001367\n",
      "epoch: 12 [44800/60000 (75%)]\t training loss: 0.061213\n",
      "epoch: 12 [45120/60000 (75%)]\t training loss: 0.000653\n",
      "epoch: 12 [45440/60000 (76%)]\t training loss: 0.000758\n",
      "epoch: 12 [45760/60000 (76%)]\t training loss: 0.000011\n",
      "epoch: 12 [46080/60000 (77%)]\t training loss: 0.005202\n",
      "epoch: 12 [46400/60000 (77%)]\t training loss: 0.005204\n",
      "epoch: 12 [46720/60000 (78%)]\t training loss: 0.000050\n",
      "epoch: 12 [47040/60000 (78%)]\t training loss: 0.009220\n",
      "epoch: 12 [47360/60000 (79%)]\t training loss: 0.005778\n",
      "epoch: 12 [47680/60000 (79%)]\t training loss: 0.000215\n",
      "epoch: 12 [48000/60000 (80%)]\t training loss: 0.002806\n",
      "epoch: 12 [48320/60000 (81%)]\t training loss: 0.000031\n",
      "epoch: 12 [48640/60000 (81%)]\t training loss: 0.206529\n",
      "epoch: 12 [48960/60000 (82%)]\t training loss: 0.001750\n",
      "epoch: 12 [49280/60000 (82%)]\t training loss: 0.000130\n",
      "epoch: 12 [49600/60000 (83%)]\t training loss: 0.000025\n",
      "epoch: 12 [49920/60000 (83%)]\t training loss: 0.114901\n",
      "epoch: 12 [50240/60000 (84%)]\t training loss: 0.020118\n",
      "epoch: 12 [50560/60000 (84%)]\t training loss: 0.037129\n",
      "epoch: 12 [50880/60000 (85%)]\t training loss: 0.074792\n",
      "epoch: 12 [51200/60000 (85%)]\t training loss: 0.000222\n",
      "epoch: 12 [51520/60000 (86%)]\t training loss: 0.000738\n",
      "epoch: 12 [51840/60000 (86%)]\t training loss: 0.019667\n",
      "epoch: 12 [52160/60000 (87%)]\t training loss: 0.002208\n",
      "epoch: 12 [52480/60000 (87%)]\t training loss: 0.005956\n",
      "epoch: 12 [52800/60000 (88%)]\t training loss: 0.022176\n",
      "epoch: 12 [53120/60000 (89%)]\t training loss: 0.000128\n",
      "epoch: 12 [53440/60000 (89%)]\t training loss: 0.072994\n",
      "epoch: 12 [53760/60000 (90%)]\t training loss: 0.000713\n",
      "epoch: 12 [54080/60000 (90%)]\t training loss: 0.012473\n",
      "epoch: 12 [54400/60000 (91%)]\t training loss: 0.000488\n",
      "epoch: 12 [54720/60000 (91%)]\t training loss: 0.003045\n",
      "epoch: 12 [55040/60000 (92%)]\t training loss: 0.000248\n",
      "epoch: 12 [55360/60000 (92%)]\t training loss: 0.025821\n",
      "epoch: 12 [55680/60000 (93%)]\t training loss: 0.000232\n",
      "epoch: 12 [56000/60000 (93%)]\t training loss: 0.006915\n",
      "epoch: 12 [56320/60000 (94%)]\t training loss: 0.000883\n",
      "epoch: 12 [56640/60000 (94%)]\t training loss: 0.003056\n",
      "epoch: 12 [56960/60000 (95%)]\t training loss: 0.075450\n",
      "epoch: 12 [57280/60000 (95%)]\t training loss: 0.043545\n",
      "epoch: 12 [57600/60000 (96%)]\t training loss: 0.003990\n",
      "epoch: 12 [57920/60000 (97%)]\t training loss: 0.000293\n",
      "epoch: 12 [58240/60000 (97%)]\t training loss: 0.013975\n",
      "epoch: 12 [58560/60000 (98%)]\t training loss: 0.011530\n",
      "epoch: 12 [58880/60000 (98%)]\t training loss: 0.000266\n",
      "epoch: 12 [59200/60000 (99%)]\t training loss: 0.012869\n",
      "epoch: 12 [59520/60000 (99%)]\t training loss: 0.000726\n",
      "epoch: 12 [59840/60000 (100%)]\t training loss: 0.000495\n",
      "\n",
      "Test dataset: Overall Loss: 0.0345, Overall Accuracy: 9915/10000 (99%)\n",
      "\n",
      "epoch: 13 [0/60000 (0%)]\t training loss: 0.011499\n",
      "epoch: 13 [320/60000 (1%)]\t training loss: 0.003391\n",
      "epoch: 13 [640/60000 (1%)]\t training loss: 0.001487\n",
      "epoch: 13 [960/60000 (2%)]\t training loss: 0.005911\n",
      "epoch: 13 [1280/60000 (2%)]\t training loss: 0.117814\n",
      "epoch: 13 [1600/60000 (3%)]\t training loss: 0.000201\n",
      "epoch: 13 [1920/60000 (3%)]\t training loss: 0.023239\n",
      "epoch: 13 [2240/60000 (4%)]\t training loss: 0.003397\n",
      "epoch: 13 [2560/60000 (4%)]\t training loss: 0.000515\n",
      "epoch: 13 [2880/60000 (5%)]\t training loss: 0.000532\n",
      "epoch: 13 [3200/60000 (5%)]\t training loss: 0.000007\n",
      "epoch: 13 [3520/60000 (6%)]\t training loss: 0.013888\n",
      "epoch: 13 [3840/60000 (6%)]\t training loss: 0.004774\n",
      "epoch: 13 [4160/60000 (7%)]\t training loss: 0.002864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 [4480/60000 (7%)]\t training loss: 0.000627\n",
      "epoch: 13 [4800/60000 (8%)]\t training loss: 0.014556\n",
      "epoch: 13 [5120/60000 (9%)]\t training loss: 0.089599\n",
      "epoch: 13 [5440/60000 (9%)]\t training loss: 0.020654\n",
      "epoch: 13 [5760/60000 (10%)]\t training loss: 0.007298\n",
      "epoch: 13 [6080/60000 (10%)]\t training loss: 0.140368\n",
      "epoch: 13 [6400/60000 (11%)]\t training loss: 0.000063\n",
      "epoch: 13 [6720/60000 (11%)]\t training loss: 0.049608\n",
      "epoch: 13 [7040/60000 (12%)]\t training loss: 0.003712\n",
      "epoch: 13 [7360/60000 (12%)]\t training loss: 0.000072\n",
      "epoch: 13 [7680/60000 (13%)]\t training loss: 0.406564\n",
      "epoch: 13 [8000/60000 (13%)]\t training loss: 0.003493\n",
      "epoch: 13 [8320/60000 (14%)]\t training loss: 0.010952\n",
      "epoch: 13 [8640/60000 (14%)]\t training loss: 0.122486\n",
      "epoch: 13 [8960/60000 (15%)]\t training loss: 0.009045\n",
      "epoch: 13 [9280/60000 (15%)]\t training loss: 0.034399\n",
      "epoch: 13 [9600/60000 (16%)]\t training loss: 0.040408\n",
      "epoch: 13 [9920/60000 (17%)]\t training loss: 0.000085\n",
      "epoch: 13 [10240/60000 (17%)]\t training loss: 0.000110\n",
      "epoch: 13 [10560/60000 (18%)]\t training loss: 0.001177\n",
      "epoch: 13 [10880/60000 (18%)]\t training loss: 0.001306\n",
      "epoch: 13 [11200/60000 (19%)]\t training loss: 0.000068\n",
      "epoch: 13 [11520/60000 (19%)]\t training loss: 0.008992\n",
      "epoch: 13 [11840/60000 (20%)]\t training loss: 0.000898\n",
      "epoch: 13 [12160/60000 (20%)]\t training loss: 0.000219\n",
      "epoch: 13 [12480/60000 (21%)]\t training loss: 0.031348\n",
      "epoch: 13 [12800/60000 (21%)]\t training loss: 0.000140\n",
      "epoch: 13 [13120/60000 (22%)]\t training loss: 0.000176\n",
      "epoch: 13 [13440/60000 (22%)]\t training loss: 0.011785\n",
      "epoch: 13 [13760/60000 (23%)]\t training loss: 0.046182\n",
      "epoch: 13 [14080/60000 (23%)]\t training loss: 0.000262\n",
      "epoch: 13 [14400/60000 (24%)]\t training loss: 0.008748\n",
      "epoch: 13 [14720/60000 (25%)]\t training loss: 0.000136\n",
      "epoch: 13 [15040/60000 (25%)]\t training loss: 0.000532\n",
      "epoch: 13 [15360/60000 (26%)]\t training loss: 0.037860\n",
      "epoch: 13 [15680/60000 (26%)]\t training loss: 0.000657\n",
      "epoch: 13 [16000/60000 (27%)]\t training loss: 0.003942\n",
      "epoch: 13 [16320/60000 (27%)]\t training loss: 0.000696\n",
      "epoch: 13 [16640/60000 (28%)]\t training loss: 0.000151\n",
      "epoch: 13 [16960/60000 (28%)]\t training loss: 0.008195\n",
      "epoch: 13 [17280/60000 (29%)]\t training loss: 0.021313\n",
      "epoch: 13 [17600/60000 (29%)]\t training loss: 0.096754\n",
      "epoch: 13 [17920/60000 (30%)]\t training loss: 0.001498\n",
      "epoch: 13 [18240/60000 (30%)]\t training loss: 0.005767\n",
      "epoch: 13 [18560/60000 (31%)]\t training loss: 0.031303\n",
      "epoch: 13 [18880/60000 (31%)]\t training loss: 0.051559\n",
      "epoch: 13 [19200/60000 (32%)]\t training loss: 0.000826\n",
      "epoch: 13 [19520/60000 (33%)]\t training loss: 0.000409\n",
      "epoch: 13 [19840/60000 (33%)]\t training loss: 0.002886\n",
      "epoch: 13 [20160/60000 (34%)]\t training loss: 0.000807\n",
      "epoch: 13 [20480/60000 (34%)]\t training loss: 0.023970\n",
      "epoch: 13 [20800/60000 (35%)]\t training loss: 0.000393\n",
      "epoch: 13 [21120/60000 (35%)]\t training loss: 0.000004\n",
      "epoch: 13 [21440/60000 (36%)]\t training loss: 0.024058\n",
      "epoch: 13 [21760/60000 (36%)]\t training loss: 0.000088\n",
      "epoch: 13 [22080/60000 (37%)]\t training loss: 0.000427\n",
      "epoch: 13 [22400/60000 (37%)]\t training loss: 0.020703\n",
      "epoch: 13 [22720/60000 (38%)]\t training loss: 0.000772\n",
      "epoch: 13 [23040/60000 (38%)]\t training loss: 0.000534\n",
      "epoch: 13 [23360/60000 (39%)]\t training loss: 0.000301\n",
      "epoch: 13 [23680/60000 (39%)]\t training loss: 0.000588\n",
      "epoch: 13 [24000/60000 (40%)]\t training loss: 0.000057\n",
      "epoch: 13 [24320/60000 (41%)]\t training loss: 0.000145\n",
      "epoch: 13 [24640/60000 (41%)]\t training loss: 0.000028\n",
      "epoch: 13 [24960/60000 (42%)]\t training loss: 0.003274\n",
      "epoch: 13 [25280/60000 (42%)]\t training loss: 0.060158\n",
      "epoch: 13 [25600/60000 (43%)]\t training loss: 0.023859\n",
      "epoch: 13 [25920/60000 (43%)]\t training loss: 0.000540\n",
      "epoch: 13 [26240/60000 (44%)]\t training loss: 0.002430\n",
      "epoch: 13 [26560/60000 (44%)]\t training loss: 0.002800\n",
      "epoch: 13 [26880/60000 (45%)]\t training loss: 0.000451\n",
      "epoch: 13 [27200/60000 (45%)]\t training loss: 0.000044\n",
      "epoch: 13 [27520/60000 (46%)]\t training loss: 0.000207\n",
      "epoch: 13 [27840/60000 (46%)]\t training loss: 0.001201\n",
      "epoch: 13 [28160/60000 (47%)]\t training loss: 0.028618\n",
      "epoch: 13 [28480/60000 (47%)]\t training loss: 0.223505\n",
      "epoch: 13 [28800/60000 (48%)]\t training loss: 0.008633\n",
      "epoch: 13 [29120/60000 (49%)]\t training loss: 0.071973\n",
      "epoch: 13 [29440/60000 (49%)]\t training loss: 0.000622\n",
      "epoch: 13 [29760/60000 (50%)]\t training loss: 0.006043\n",
      "epoch: 13 [30080/60000 (50%)]\t training loss: 0.033153\n",
      "epoch: 13 [30400/60000 (51%)]\t training loss: 0.000216\n",
      "epoch: 13 [30720/60000 (51%)]\t training loss: 0.001171\n",
      "epoch: 13 [31040/60000 (52%)]\t training loss: 0.002607\n",
      "epoch: 13 [31360/60000 (52%)]\t training loss: 0.000055\n",
      "epoch: 13 [31680/60000 (53%)]\t training loss: 0.001453\n",
      "epoch: 13 [32000/60000 (53%)]\t training loss: 0.003111\n",
      "epoch: 13 [32320/60000 (54%)]\t training loss: 0.000618\n",
      "epoch: 13 [32640/60000 (54%)]\t training loss: 0.003009\n",
      "epoch: 13 [32960/60000 (55%)]\t training loss: 0.079233\n",
      "epoch: 13 [33280/60000 (55%)]\t training loss: 0.000765\n",
      "epoch: 13 [33600/60000 (56%)]\t training loss: 0.000357\n",
      "epoch: 13 [33920/60000 (57%)]\t training loss: 0.002207\n",
      "epoch: 13 [34240/60000 (57%)]\t training loss: 0.010629\n",
      "epoch: 13 [34560/60000 (58%)]\t training loss: 0.000197\n",
      "epoch: 13 [34880/60000 (58%)]\t training loss: 0.000003\n",
      "epoch: 13 [35200/60000 (59%)]\t training loss: 0.066383\n",
      "epoch: 13 [35520/60000 (59%)]\t training loss: 0.000257\n",
      "epoch: 13 [35840/60000 (60%)]\t training loss: 0.004453\n",
      "epoch: 13 [36160/60000 (60%)]\t training loss: 0.001076\n",
      "epoch: 13 [36480/60000 (61%)]\t training loss: 0.002563\n",
      "epoch: 13 [36800/60000 (61%)]\t training loss: 0.016040\n",
      "epoch: 13 [37120/60000 (62%)]\t training loss: 0.000041\n",
      "epoch: 13 [37440/60000 (62%)]\t training loss: 0.000052\n",
      "epoch: 13 [37760/60000 (63%)]\t training loss: 0.002688\n",
      "epoch: 13 [38080/60000 (63%)]\t training loss: 0.000328\n",
      "epoch: 13 [38400/60000 (64%)]\t training loss: 0.000554\n",
      "epoch: 13 [38720/60000 (65%)]\t training loss: 0.000065\n",
      "epoch: 13 [39040/60000 (65%)]\t training loss: 0.000296\n",
      "epoch: 13 [39360/60000 (66%)]\t training loss: 0.004957\n",
      "epoch: 13 [39680/60000 (66%)]\t training loss: 0.021463\n",
      "epoch: 13 [40000/60000 (67%)]\t training loss: 0.002681\n",
      "epoch: 13 [40320/60000 (67%)]\t training loss: 0.001085\n",
      "epoch: 13 [40640/60000 (68%)]\t training loss: 0.000042\n",
      "epoch: 13 [40960/60000 (68%)]\t training loss: 0.000163\n",
      "epoch: 13 [41280/60000 (69%)]\t training loss: 0.021175\n",
      "epoch: 13 [41600/60000 (69%)]\t training loss: 0.000035\n",
      "epoch: 13 [41920/60000 (70%)]\t training loss: 0.037936\n",
      "epoch: 13 [42240/60000 (70%)]\t training loss: 0.000836\n",
      "epoch: 13 [42560/60000 (71%)]\t training loss: 0.265710\n",
      "epoch: 13 [42880/60000 (71%)]\t training loss: 0.000019\n",
      "epoch: 13 [43200/60000 (72%)]\t training loss: 0.000048\n",
      "epoch: 13 [43520/60000 (73%)]\t training loss: 0.266395\n",
      "epoch: 13 [43840/60000 (73%)]\t training loss: 0.001340\n",
      "epoch: 13 [44160/60000 (74%)]\t training loss: 0.000303\n",
      "epoch: 13 [44480/60000 (74%)]\t training loss: 0.001777\n",
      "epoch: 13 [44800/60000 (75%)]\t training loss: 0.395090\n",
      "epoch: 13 [45120/60000 (75%)]\t training loss: 0.000119\n",
      "epoch: 13 [45440/60000 (76%)]\t training loss: 0.000469\n",
      "epoch: 13 [45760/60000 (76%)]\t training loss: 0.000199\n",
      "epoch: 13 [46080/60000 (77%)]\t training loss: 0.000141\n",
      "epoch: 13 [46400/60000 (77%)]\t training loss: 0.000492\n",
      "epoch: 13 [46720/60000 (78%)]\t training loss: 0.005507\n",
      "epoch: 13 [47040/60000 (78%)]\t training loss: 0.009698\n",
      "epoch: 13 [47360/60000 (79%)]\t training loss: 0.000322\n",
      "epoch: 13 [47680/60000 (79%)]\t training loss: 0.004090\n",
      "epoch: 13 [48000/60000 (80%)]\t training loss: 0.014845\n",
      "epoch: 13 [48320/60000 (81%)]\t training loss: 0.000005\n",
      "epoch: 13 [48640/60000 (81%)]\t training loss: 0.000833\n",
      "epoch: 13 [48960/60000 (82%)]\t training loss: 0.046944\n",
      "epoch: 13 [49280/60000 (82%)]\t training loss: 0.013183\n",
      "epoch: 13 [49600/60000 (83%)]\t training loss: 0.000363\n",
      "epoch: 13 [49920/60000 (83%)]\t training loss: 0.000045\n",
      "epoch: 13 [50240/60000 (84%)]\t training loss: 0.013509\n",
      "epoch: 13 [50560/60000 (84%)]\t training loss: 0.020933\n",
      "epoch: 13 [50880/60000 (85%)]\t training loss: 0.000587\n",
      "epoch: 13 [51200/60000 (85%)]\t training loss: 0.001154\n",
      "epoch: 13 [51520/60000 (86%)]\t training loss: 0.000889\n",
      "epoch: 13 [51840/60000 (86%)]\t training loss: 0.209390\n",
      "epoch: 13 [52160/60000 (87%)]\t training loss: 0.074203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13 [52480/60000 (87%)]\t training loss: 0.000764\n",
      "epoch: 13 [52800/60000 (88%)]\t training loss: 0.000017\n",
      "epoch: 13 [53120/60000 (89%)]\t training loss: 0.001306\n",
      "epoch: 13 [53440/60000 (89%)]\t training loss: 0.021752\n",
      "epoch: 13 [53760/60000 (90%)]\t training loss: 0.001519\n",
      "epoch: 13 [54080/60000 (90%)]\t training loss: 0.000116\n",
      "epoch: 13 [54400/60000 (91%)]\t training loss: 0.000177\n",
      "epoch: 13 [54720/60000 (91%)]\t training loss: 0.000200\n",
      "epoch: 13 [55040/60000 (92%)]\t training loss: 0.006733\n",
      "epoch: 13 [55360/60000 (92%)]\t training loss: 0.006740\n",
      "epoch: 13 [55680/60000 (93%)]\t training loss: 0.050465\n",
      "epoch: 13 [56000/60000 (93%)]\t training loss: 0.006101\n",
      "epoch: 13 [56320/60000 (94%)]\t training loss: 0.004351\n",
      "epoch: 13 [56640/60000 (94%)]\t training loss: 0.000011\n",
      "epoch: 13 [56960/60000 (95%)]\t training loss: 0.000341\n",
      "epoch: 13 [57280/60000 (95%)]\t training loss: 0.002264\n",
      "epoch: 13 [57600/60000 (96%)]\t training loss: 0.003785\n",
      "epoch: 13 [57920/60000 (97%)]\t training loss: 0.000405\n",
      "epoch: 13 [58240/60000 (97%)]\t training loss: 0.000223\n",
      "epoch: 13 [58560/60000 (98%)]\t training loss: 0.014987\n",
      "epoch: 13 [58880/60000 (98%)]\t training loss: 0.001099\n",
      "epoch: 13 [59200/60000 (99%)]\t training loss: 0.000016\n",
      "epoch: 13 [59520/60000 (99%)]\t training loss: 0.001565\n",
      "epoch: 13 [59840/60000 (100%)]\t training loss: 0.028732\n",
      "\n",
      "Test dataset: Overall Loss: 0.0408, Overall Accuracy: 9907/10000 (99%)\n",
      "\n",
      "epoch: 14 [0/60000 (0%)]\t training loss: 0.001201\n",
      "epoch: 14 [320/60000 (1%)]\t training loss: 0.000046\n",
      "epoch: 14 [640/60000 (1%)]\t training loss: 0.213264\n",
      "epoch: 14 [960/60000 (2%)]\t training loss: 0.000738\n",
      "epoch: 14 [1280/60000 (2%)]\t training loss: 0.003923\n",
      "epoch: 14 [1600/60000 (3%)]\t training loss: 0.000042\n",
      "epoch: 14 [1920/60000 (3%)]\t training loss: 0.000763\n",
      "epoch: 14 [2240/60000 (4%)]\t training loss: 0.000119\n",
      "epoch: 14 [2560/60000 (4%)]\t training loss: 0.022596\n",
      "epoch: 14 [2880/60000 (5%)]\t training loss: 0.008693\n",
      "epoch: 14 [3200/60000 (5%)]\t training loss: 0.125816\n",
      "epoch: 14 [3520/60000 (6%)]\t training loss: 0.010247\n",
      "epoch: 14 [3840/60000 (6%)]\t training loss: 0.007472\n",
      "epoch: 14 [4160/60000 (7%)]\t training loss: 0.002173\n",
      "epoch: 14 [4480/60000 (7%)]\t training loss: 0.000517\n",
      "epoch: 14 [4800/60000 (8%)]\t training loss: 0.026258\n",
      "epoch: 14 [5120/60000 (9%)]\t training loss: 0.001350\n",
      "epoch: 14 [5440/60000 (9%)]\t training loss: 0.000053\n",
      "epoch: 14 [5760/60000 (10%)]\t training loss: 0.000024\n",
      "epoch: 14 [6080/60000 (10%)]\t training loss: 0.255748\n",
      "epoch: 14 [6400/60000 (11%)]\t training loss: 0.081110\n",
      "epoch: 14 [6720/60000 (11%)]\t training loss: 0.007730\n",
      "epoch: 14 [7040/60000 (12%)]\t training loss: 0.001695\n",
      "epoch: 14 [7360/60000 (12%)]\t training loss: 0.003916\n",
      "epoch: 14 [7680/60000 (13%)]\t training loss: 0.029611\n",
      "epoch: 14 [8000/60000 (13%)]\t training loss: 0.025204\n",
      "epoch: 14 [8320/60000 (14%)]\t training loss: 0.007313\n",
      "epoch: 14 [8640/60000 (14%)]\t training loss: 0.016841\n",
      "epoch: 14 [8960/60000 (15%)]\t training loss: 0.000096\n",
      "epoch: 14 [9280/60000 (15%)]\t training loss: 0.000096\n",
      "epoch: 14 [9600/60000 (16%)]\t training loss: 0.001780\n",
      "epoch: 14 [9920/60000 (17%)]\t training loss: 0.002043\n",
      "epoch: 14 [10240/60000 (17%)]\t training loss: 0.000011\n",
      "epoch: 14 [10560/60000 (18%)]\t training loss: 0.000018\n",
      "epoch: 14 [10880/60000 (18%)]\t training loss: 0.001497\n",
      "epoch: 14 [11200/60000 (19%)]\t training loss: 0.004298\n",
      "epoch: 14 [11520/60000 (19%)]\t training loss: 0.001790\n",
      "epoch: 14 [11840/60000 (20%)]\t training loss: 0.003275\n",
      "epoch: 14 [12160/60000 (20%)]\t training loss: 0.025549\n",
      "epoch: 14 [12480/60000 (21%)]\t training loss: 0.000078\n",
      "epoch: 14 [12800/60000 (21%)]\t training loss: 0.001403\n",
      "epoch: 14 [13120/60000 (22%)]\t training loss: 0.000375\n",
      "epoch: 14 [13440/60000 (22%)]\t training loss: 0.006373\n",
      "epoch: 14 [13760/60000 (23%)]\t training loss: 0.000186\n",
      "epoch: 14 [14080/60000 (23%)]\t training loss: 0.001122\n",
      "epoch: 14 [14400/60000 (24%)]\t training loss: 0.000966\n",
      "epoch: 14 [14720/60000 (25%)]\t training loss: 0.005352\n",
      "epoch: 14 [15040/60000 (25%)]\t training loss: 0.002561\n",
      "epoch: 14 [15360/60000 (26%)]\t training loss: 0.003167\n",
      "epoch: 14 [15680/60000 (26%)]\t training loss: 0.000415\n",
      "epoch: 14 [16000/60000 (27%)]\t training loss: 0.000158\n",
      "epoch: 14 [16320/60000 (27%)]\t training loss: 0.006771\n",
      "epoch: 14 [16640/60000 (28%)]\t training loss: 0.000027\n",
      "epoch: 14 [16960/60000 (28%)]\t training loss: 0.021205\n",
      "epoch: 14 [17280/60000 (29%)]\t training loss: 0.000590\n",
      "epoch: 14 [17600/60000 (29%)]\t training loss: 0.001577\n",
      "epoch: 14 [17920/60000 (30%)]\t training loss: 0.000501\n",
      "epoch: 14 [18240/60000 (30%)]\t training loss: 0.171491\n",
      "epoch: 14 [18560/60000 (31%)]\t training loss: 0.000101\n",
      "epoch: 14 [18880/60000 (31%)]\t training loss: 0.000429\n",
      "epoch: 14 [19200/60000 (32%)]\t training loss: 0.000004\n",
      "epoch: 14 [19520/60000 (33%)]\t training loss: 0.493354\n",
      "epoch: 14 [19840/60000 (33%)]\t training loss: 0.000373\n",
      "epoch: 14 [20160/60000 (34%)]\t training loss: 0.017573\n",
      "epoch: 14 [20480/60000 (34%)]\t training loss: 0.000405\n",
      "epoch: 14 [20800/60000 (35%)]\t training loss: 0.176540\n",
      "epoch: 14 [21120/60000 (35%)]\t training loss: 0.000361\n",
      "epoch: 14 [21440/60000 (36%)]\t training loss: 0.000819\n",
      "epoch: 14 [21760/60000 (36%)]\t training loss: 0.000328\n",
      "epoch: 14 [22080/60000 (37%)]\t training loss: 0.095875\n",
      "epoch: 14 [22400/60000 (37%)]\t training loss: 0.002544\n",
      "epoch: 14 [22720/60000 (38%)]\t training loss: 0.028302\n",
      "epoch: 14 [23040/60000 (38%)]\t training loss: 0.193029\n",
      "epoch: 14 [23360/60000 (39%)]\t training loss: 0.002090\n",
      "epoch: 14 [23680/60000 (39%)]\t training loss: 0.000047\n",
      "epoch: 14 [24000/60000 (40%)]\t training loss: 0.008996\n",
      "epoch: 14 [24320/60000 (41%)]\t training loss: 0.007170\n",
      "epoch: 14 [24640/60000 (41%)]\t training loss: 0.023961\n",
      "epoch: 14 [24960/60000 (42%)]\t training loss: 0.014918\n",
      "epoch: 14 [25280/60000 (42%)]\t training loss: 0.009416\n",
      "epoch: 14 [25600/60000 (43%)]\t training loss: 0.225036\n",
      "epoch: 14 [25920/60000 (43%)]\t training loss: 0.000709\n",
      "epoch: 14 [26240/60000 (44%)]\t training loss: 0.000166\n",
      "epoch: 14 [26560/60000 (44%)]\t training loss: 0.000935\n",
      "epoch: 14 [26880/60000 (45%)]\t training loss: 0.019469\n",
      "epoch: 14 [27200/60000 (45%)]\t training loss: 0.002703\n",
      "epoch: 14 [27520/60000 (46%)]\t training loss: 0.000105\n",
      "epoch: 14 [27840/60000 (46%)]\t training loss: 0.015640\n",
      "epoch: 14 [28160/60000 (47%)]\t training loss: 0.000797\n",
      "epoch: 14 [28480/60000 (47%)]\t training loss: 0.011613\n",
      "epoch: 14 [28800/60000 (48%)]\t training loss: 0.000299\n",
      "epoch: 14 [29120/60000 (49%)]\t training loss: 0.001642\n",
      "epoch: 14 [29440/60000 (49%)]\t training loss: 0.000581\n",
      "epoch: 14 [29760/60000 (50%)]\t training loss: 0.000274\n",
      "epoch: 14 [30080/60000 (50%)]\t training loss: 0.051272\n",
      "epoch: 14 [30400/60000 (51%)]\t training loss: 0.000538\n",
      "epoch: 14 [30720/60000 (51%)]\t training loss: 0.008019\n",
      "epoch: 14 [31040/60000 (52%)]\t training loss: 0.000311\n",
      "epoch: 14 [31360/60000 (52%)]\t training loss: 0.001909\n",
      "epoch: 14 [31680/60000 (53%)]\t training loss: 0.001688\n",
      "epoch: 14 [32000/60000 (53%)]\t training loss: 0.000823\n",
      "epoch: 14 [32320/60000 (54%)]\t training loss: 0.004203\n",
      "epoch: 14 [32640/60000 (54%)]\t training loss: 0.000606\n",
      "epoch: 14 [32960/60000 (55%)]\t training loss: 0.000110\n",
      "epoch: 14 [33280/60000 (55%)]\t training loss: 0.001214\n",
      "epoch: 14 [33600/60000 (56%)]\t training loss: 0.015925\n",
      "epoch: 14 [33920/60000 (57%)]\t training loss: 0.012794\n",
      "epoch: 14 [34240/60000 (57%)]\t training loss: 0.039639\n",
      "epoch: 14 [34560/60000 (58%)]\t training loss: 0.000124\n",
      "epoch: 14 [34880/60000 (58%)]\t training loss: 0.000025\n",
      "epoch: 14 [35200/60000 (59%)]\t training loss: 0.000241\n",
      "epoch: 14 [35520/60000 (59%)]\t training loss: 0.147245\n",
      "epoch: 14 [35840/60000 (60%)]\t training loss: 0.000324\n",
      "epoch: 14 [36160/60000 (60%)]\t training loss: 0.000576\n",
      "epoch: 14 [36480/60000 (61%)]\t training loss: 0.007653\n",
      "epoch: 14 [36800/60000 (61%)]\t training loss: 0.010091\n",
      "epoch: 14 [37120/60000 (62%)]\t training loss: 0.000557\n",
      "epoch: 14 [37440/60000 (62%)]\t training loss: 0.005245\n",
      "epoch: 14 [37760/60000 (63%)]\t training loss: 0.002452\n",
      "epoch: 14 [38080/60000 (63%)]\t training loss: 0.000897\n",
      "epoch: 14 [38400/60000 (64%)]\t training loss: 0.002062\n",
      "epoch: 14 [38720/60000 (65%)]\t training loss: 0.000721\n",
      "epoch: 14 [39040/60000 (65%)]\t training loss: 0.000302\n",
      "epoch: 14 [39360/60000 (66%)]\t training loss: 0.129879\n",
      "epoch: 14 [39680/60000 (66%)]\t training loss: 0.002596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14 [40000/60000 (67%)]\t training loss: 0.002122\n",
      "epoch: 14 [40320/60000 (67%)]\t training loss: 0.005684\n",
      "epoch: 14 [40640/60000 (68%)]\t training loss: 0.000799\n",
      "epoch: 14 [40960/60000 (68%)]\t training loss: 0.003873\n",
      "epoch: 14 [41280/60000 (69%)]\t training loss: 0.001122\n",
      "epoch: 14 [41600/60000 (69%)]\t training loss: 0.023078\n",
      "epoch: 14 [41920/60000 (70%)]\t training loss: 0.007158\n",
      "epoch: 14 [42240/60000 (70%)]\t training loss: 0.000219\n",
      "epoch: 14 [42560/60000 (71%)]\t training loss: 0.001506\n",
      "epoch: 14 [42880/60000 (71%)]\t training loss: 0.000932\n",
      "epoch: 14 [43200/60000 (72%)]\t training loss: 0.000450\n",
      "epoch: 14 [43520/60000 (73%)]\t training loss: 0.002113\n",
      "epoch: 14 [43840/60000 (73%)]\t training loss: 0.002615\n",
      "epoch: 14 [44160/60000 (74%)]\t training loss: 0.002657\n",
      "epoch: 14 [44480/60000 (74%)]\t training loss: 0.090202\n",
      "epoch: 14 [44800/60000 (75%)]\t training loss: 0.000002\n",
      "epoch: 14 [45120/60000 (75%)]\t training loss: 0.078418\n",
      "epoch: 14 [45440/60000 (76%)]\t training loss: 0.011661\n",
      "epoch: 14 [45760/60000 (76%)]\t training loss: 0.000649\n",
      "epoch: 14 [46080/60000 (77%)]\t training loss: 0.001109\n",
      "epoch: 14 [46400/60000 (77%)]\t training loss: 0.001447\n",
      "epoch: 14 [46720/60000 (78%)]\t training loss: 0.053328\n",
      "epoch: 14 [47040/60000 (78%)]\t training loss: 0.004895\n",
      "epoch: 14 [47360/60000 (79%)]\t training loss: 0.008748\n",
      "epoch: 14 [47680/60000 (79%)]\t training loss: 0.001097\n",
      "epoch: 14 [48000/60000 (80%)]\t training loss: 0.013119\n",
      "epoch: 14 [48320/60000 (81%)]\t training loss: 0.024753\n",
      "epoch: 14 [48640/60000 (81%)]\t training loss: 0.000052\n",
      "epoch: 14 [48960/60000 (82%)]\t training loss: 0.116628\n",
      "epoch: 14 [49280/60000 (82%)]\t training loss: 0.000903\n",
      "epoch: 14 [49600/60000 (83%)]\t training loss: 0.000798\n",
      "epoch: 14 [49920/60000 (83%)]\t training loss: 0.302770\n",
      "epoch: 14 [50240/60000 (84%)]\t training loss: 0.003709\n",
      "epoch: 14 [50560/60000 (84%)]\t training loss: 0.005300\n",
      "epoch: 14 [50880/60000 (85%)]\t training loss: 0.011034\n",
      "epoch: 14 [51200/60000 (85%)]\t training loss: 0.041185\n",
      "epoch: 14 [51520/60000 (86%)]\t training loss: 0.000109\n",
      "epoch: 14 [51840/60000 (86%)]\t training loss: 0.000410\n",
      "epoch: 14 [52160/60000 (87%)]\t training loss: 0.288981\n",
      "epoch: 14 [52480/60000 (87%)]\t training loss: 0.000229\n",
      "epoch: 14 [52800/60000 (88%)]\t training loss: 0.001750\n",
      "epoch: 14 [53120/60000 (89%)]\t training loss: 0.005403\n",
      "epoch: 14 [53440/60000 (89%)]\t training loss: 0.000539\n",
      "epoch: 14 [53760/60000 (90%)]\t training loss: 0.000462\n",
      "epoch: 14 [54080/60000 (90%)]\t training loss: 0.001853\n",
      "epoch: 14 [54400/60000 (91%)]\t training loss: 0.020451\n",
      "epoch: 14 [54720/60000 (91%)]\t training loss: 0.000396\n",
      "epoch: 14 [55040/60000 (92%)]\t training loss: 0.108131\n",
      "epoch: 14 [55360/60000 (92%)]\t training loss: 0.001723\n",
      "epoch: 14 [55680/60000 (93%)]\t training loss: 0.090568\n",
      "epoch: 14 [56000/60000 (93%)]\t training loss: 0.134592\n",
      "epoch: 14 [56320/60000 (94%)]\t training loss: 0.011540\n",
      "epoch: 14 [56640/60000 (94%)]\t training loss: 0.000567\n",
      "epoch: 14 [56960/60000 (95%)]\t training loss: 0.000080\n",
      "epoch: 14 [57280/60000 (95%)]\t training loss: 0.012860\n",
      "epoch: 14 [57600/60000 (96%)]\t training loss: 0.002475\n",
      "epoch: 14 [57920/60000 (97%)]\t training loss: 0.002989\n",
      "epoch: 14 [58240/60000 (97%)]\t training loss: 0.067608\n",
      "epoch: 14 [58560/60000 (98%)]\t training loss: 0.007348\n",
      "epoch: 14 [58880/60000 (98%)]\t training loss: 0.005479\n",
      "epoch: 14 [59200/60000 (99%)]\t training loss: 0.001334\n",
      "epoch: 14 [59520/60000 (99%)]\t training loss: 0.254320\n",
      "epoch: 14 [59840/60000 (100%)]\t training loss: 0.011196\n",
      "\n",
      "Test dataset: Overall Loss: 0.0372, Overall Accuracy: 9908/10000 (99%)\n",
      "\n",
      "epoch: 15 [0/60000 (0%)]\t training loss: 0.000341\n",
      "epoch: 15 [320/60000 (1%)]\t training loss: 0.014902\n",
      "epoch: 15 [640/60000 (1%)]\t training loss: 0.001780\n",
      "epoch: 15 [960/60000 (2%)]\t training loss: 0.001993\n",
      "epoch: 15 [1280/60000 (2%)]\t training loss: 0.000007\n",
      "epoch: 15 [1600/60000 (3%)]\t training loss: 0.000019\n",
      "epoch: 15 [1920/60000 (3%)]\t training loss: 0.000021\n",
      "epoch: 15 [2240/60000 (4%)]\t training loss: 0.000016\n",
      "epoch: 15 [2560/60000 (4%)]\t training loss: 0.012537\n",
      "epoch: 15 [2880/60000 (5%)]\t training loss: 0.000094\n",
      "epoch: 15 [3200/60000 (5%)]\t training loss: 0.024134\n",
      "epoch: 15 [3520/60000 (6%)]\t training loss: 0.031628\n",
      "epoch: 15 [3840/60000 (6%)]\t training loss: 0.000139\n",
      "epoch: 15 [4160/60000 (7%)]\t training loss: 0.000060\n",
      "epoch: 15 [4480/60000 (7%)]\t training loss: 0.000129\n",
      "epoch: 15 [4800/60000 (8%)]\t training loss: 0.056149\n",
      "epoch: 15 [5120/60000 (9%)]\t training loss: 0.000001\n",
      "epoch: 15 [5440/60000 (9%)]\t training loss: 0.110470\n",
      "epoch: 15 [5760/60000 (10%)]\t training loss: 0.020226\n",
      "epoch: 15 [6080/60000 (10%)]\t training loss: 0.001285\n",
      "epoch: 15 [6400/60000 (11%)]\t training loss: 0.000565\n",
      "epoch: 15 [6720/60000 (11%)]\t training loss: 0.155951\n",
      "epoch: 15 [7040/60000 (12%)]\t training loss: 0.015391\n",
      "epoch: 15 [7360/60000 (12%)]\t training loss: 0.064356\n",
      "epoch: 15 [7680/60000 (13%)]\t training loss: 0.007024\n",
      "epoch: 15 [8000/60000 (13%)]\t training loss: 0.000026\n",
      "epoch: 15 [8320/60000 (14%)]\t training loss: 0.000012\n",
      "epoch: 15 [8640/60000 (14%)]\t training loss: 0.000444\n",
      "epoch: 15 [8960/60000 (15%)]\t training loss: 0.000073\n",
      "epoch: 15 [9280/60000 (15%)]\t training loss: 0.003097\n",
      "epoch: 15 [9600/60000 (16%)]\t training loss: 0.349200\n",
      "epoch: 15 [9920/60000 (17%)]\t training loss: 0.004352\n",
      "epoch: 15 [10240/60000 (17%)]\t training loss: 0.003407\n",
      "epoch: 15 [10560/60000 (18%)]\t training loss: 0.003532\n",
      "epoch: 15 [10880/60000 (18%)]\t training loss: 0.000106\n",
      "epoch: 15 [11200/60000 (19%)]\t training loss: 0.189041\n",
      "epoch: 15 [11520/60000 (19%)]\t training loss: 0.000192\n",
      "epoch: 15 [11840/60000 (20%)]\t training loss: 0.001163\n",
      "epoch: 15 [12160/60000 (20%)]\t training loss: 0.000046\n",
      "epoch: 15 [12480/60000 (21%)]\t training loss: 0.000030\n",
      "epoch: 15 [12800/60000 (21%)]\t training loss: 0.001005\n",
      "epoch: 15 [13120/60000 (22%)]\t training loss: 0.289654\n",
      "epoch: 15 [13440/60000 (22%)]\t training loss: 0.000037\n",
      "epoch: 15 [13760/60000 (23%)]\t training loss: 0.015016\n",
      "epoch: 15 [14080/60000 (23%)]\t training loss: 0.007596\n",
      "epoch: 15 [14400/60000 (24%)]\t training loss: 0.029382\n",
      "epoch: 15 [14720/60000 (25%)]\t training loss: 0.009856\n",
      "epoch: 15 [15040/60000 (25%)]\t training loss: 0.000155\n",
      "epoch: 15 [15360/60000 (26%)]\t training loss: 0.000054\n",
      "epoch: 15 [15680/60000 (26%)]\t training loss: 0.002408\n",
      "epoch: 15 [16000/60000 (27%)]\t training loss: 0.000018\n",
      "epoch: 15 [16320/60000 (27%)]\t training loss: 0.000610\n",
      "epoch: 15 [16640/60000 (28%)]\t training loss: 0.003762\n",
      "epoch: 15 [16960/60000 (28%)]\t training loss: 0.000328\n",
      "epoch: 15 [17280/60000 (29%)]\t training loss: 0.000098\n",
      "epoch: 15 [17600/60000 (29%)]\t training loss: 0.000016\n",
      "epoch: 15 [17920/60000 (30%)]\t training loss: 0.004382\n",
      "epoch: 15 [18240/60000 (30%)]\t training loss: 0.003312\n",
      "epoch: 15 [18560/60000 (31%)]\t training loss: 0.009561\n",
      "epoch: 15 [18880/60000 (31%)]\t training loss: 0.005669\n",
      "epoch: 15 [19200/60000 (32%)]\t training loss: 0.031496\n",
      "epoch: 15 [19520/60000 (33%)]\t training loss: 0.001956\n",
      "epoch: 15 [19840/60000 (33%)]\t training loss: 0.001417\n",
      "epoch: 15 [20160/60000 (34%)]\t training loss: 0.000082\n",
      "epoch: 15 [20480/60000 (34%)]\t training loss: 0.003602\n",
      "epoch: 15 [20800/60000 (35%)]\t training loss: 0.000420\n",
      "epoch: 15 [21120/60000 (35%)]\t training loss: 0.000007\n",
      "epoch: 15 [21440/60000 (36%)]\t training loss: 0.105729\n",
      "epoch: 15 [21760/60000 (36%)]\t training loss: 0.002289\n",
      "epoch: 15 [22080/60000 (37%)]\t training loss: 0.000152\n",
      "epoch: 15 [22400/60000 (37%)]\t training loss: 0.000086\n",
      "epoch: 15 [22720/60000 (38%)]\t training loss: 0.002879\n",
      "epoch: 15 [23040/60000 (38%)]\t training loss: 0.020683\n",
      "epoch: 15 [23360/60000 (39%)]\t training loss: 0.002341\n",
      "epoch: 15 [23680/60000 (39%)]\t training loss: 0.000275\n",
      "epoch: 15 [24000/60000 (40%)]\t training loss: 0.130941\n",
      "epoch: 15 [24320/60000 (41%)]\t training loss: 0.020133\n",
      "epoch: 15 [24640/60000 (41%)]\t training loss: 0.003153\n",
      "epoch: 15 [24960/60000 (42%)]\t training loss: 0.001923\n",
      "epoch: 15 [25280/60000 (42%)]\t training loss: 0.019354\n",
      "epoch: 15 [25600/60000 (43%)]\t training loss: 0.006614\n",
      "epoch: 15 [25920/60000 (43%)]\t training loss: 0.010207\n",
      "epoch: 15 [26240/60000 (44%)]\t training loss: 0.000279\n",
      "epoch: 15 [26560/60000 (44%)]\t training loss: 0.012518\n",
      "epoch: 15 [26880/60000 (45%)]\t training loss: 0.000461\n",
      "epoch: 15 [27200/60000 (45%)]\t training loss: 0.004513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15 [27520/60000 (46%)]\t training loss: 0.000653\n",
      "epoch: 15 [27840/60000 (46%)]\t training loss: 0.000016\n",
      "epoch: 15 [28160/60000 (47%)]\t training loss: 0.018881\n",
      "epoch: 15 [28480/60000 (47%)]\t training loss: 0.038367\n",
      "epoch: 15 [28800/60000 (48%)]\t training loss: 0.000067\n",
      "epoch: 15 [29120/60000 (49%)]\t training loss: 0.001267\n",
      "epoch: 15 [29440/60000 (49%)]\t training loss: 0.123102\n",
      "epoch: 15 [29760/60000 (50%)]\t training loss: 0.005644\n",
      "epoch: 15 [30080/60000 (50%)]\t training loss: 0.002502\n",
      "epoch: 15 [30400/60000 (51%)]\t training loss: 0.000081\n",
      "epoch: 15 [30720/60000 (51%)]\t training loss: 0.000623\n",
      "epoch: 15 [31040/60000 (52%)]\t training loss: 0.035079\n",
      "epoch: 15 [31360/60000 (52%)]\t training loss: 0.016611\n",
      "epoch: 15 [31680/60000 (53%)]\t training loss: 0.139136\n",
      "epoch: 15 [32000/60000 (53%)]\t training loss: 0.000268\n",
      "epoch: 15 [32320/60000 (54%)]\t training loss: 0.002709\n",
      "epoch: 15 [32640/60000 (54%)]\t training loss: 0.005341\n",
      "epoch: 15 [32960/60000 (55%)]\t training loss: 0.000902\n",
      "epoch: 15 [33280/60000 (55%)]\t training loss: 0.052817\n",
      "epoch: 15 [33600/60000 (56%)]\t training loss: 0.011927\n",
      "epoch: 15 [33920/60000 (57%)]\t training loss: 0.005917\n",
      "epoch: 15 [34240/60000 (57%)]\t training loss: 0.002511\n",
      "epoch: 15 [34560/60000 (58%)]\t training loss: 0.000701\n",
      "epoch: 15 [34880/60000 (58%)]\t training loss: 0.000321\n",
      "epoch: 15 [35200/60000 (59%)]\t training loss: 0.311432\n",
      "epoch: 15 [35520/60000 (59%)]\t training loss: 0.002079\n",
      "epoch: 15 [35840/60000 (60%)]\t training loss: 0.030340\n",
      "epoch: 15 [36160/60000 (60%)]\t training loss: 0.041337\n",
      "epoch: 15 [36480/60000 (61%)]\t training loss: 0.000275\n",
      "epoch: 15 [36800/60000 (61%)]\t training loss: 0.000772\n",
      "epoch: 15 [37120/60000 (62%)]\t training loss: 0.037656\n",
      "epoch: 15 [37440/60000 (62%)]\t training loss: 0.000152\n",
      "epoch: 15 [37760/60000 (63%)]\t training loss: 0.000638\n",
      "epoch: 15 [38080/60000 (63%)]\t training loss: 0.001687\n",
      "epoch: 15 [38400/60000 (64%)]\t training loss: 0.044621\n",
      "epoch: 15 [38720/60000 (65%)]\t training loss: 0.000010\n",
      "epoch: 15 [39040/60000 (65%)]\t training loss: 0.018450\n",
      "epoch: 15 [39360/60000 (66%)]\t training loss: 0.001545\n",
      "epoch: 15 [39680/60000 (66%)]\t training loss: 0.000060\n",
      "epoch: 15 [40000/60000 (67%)]\t training loss: 0.000355\n",
      "epoch: 15 [40320/60000 (67%)]\t training loss: 0.030624\n",
      "epoch: 15 [40640/60000 (68%)]\t training loss: 0.002256\n",
      "epoch: 15 [40960/60000 (68%)]\t training loss: 0.022806\n",
      "epoch: 15 [41280/60000 (69%)]\t training loss: 0.000964\n",
      "epoch: 15 [41600/60000 (69%)]\t training loss: 0.001051\n",
      "epoch: 15 [41920/60000 (70%)]\t training loss: 0.010385\n",
      "epoch: 15 [42240/60000 (70%)]\t training loss: 0.197632\n",
      "epoch: 15 [42560/60000 (71%)]\t training loss: 0.000002\n",
      "epoch: 15 [42880/60000 (71%)]\t training loss: 0.000081\n",
      "epoch: 15 [43200/60000 (72%)]\t training loss: 0.005009\n",
      "epoch: 15 [43520/60000 (73%)]\t training loss: 0.001983\n",
      "epoch: 15 [43840/60000 (73%)]\t training loss: 0.257578\n",
      "epoch: 15 [44160/60000 (74%)]\t training loss: 0.005605\n",
      "epoch: 15 [44480/60000 (74%)]\t training loss: 0.001624\n",
      "epoch: 15 [44800/60000 (75%)]\t training loss: 0.000107\n",
      "epoch: 15 [45120/60000 (75%)]\t training loss: 0.045562\n",
      "epoch: 15 [45440/60000 (76%)]\t training loss: 0.000295\n",
      "epoch: 15 [45760/60000 (76%)]\t training loss: 0.012042\n",
      "epoch: 15 [46080/60000 (77%)]\t training loss: 0.000299\n",
      "epoch: 15 [46400/60000 (77%)]\t training loss: 0.000407\n",
      "epoch: 15 [46720/60000 (78%)]\t training loss: 0.115003\n",
      "epoch: 15 [47040/60000 (78%)]\t training loss: 0.000337\n",
      "epoch: 15 [47360/60000 (79%)]\t training loss: 0.000658\n",
      "epoch: 15 [47680/60000 (79%)]\t training loss: 0.000117\n",
      "epoch: 15 [48000/60000 (80%)]\t training loss: 0.000342\n",
      "epoch: 15 [48320/60000 (81%)]\t training loss: 0.043723\n",
      "epoch: 15 [48640/60000 (81%)]\t training loss: 0.004441\n",
      "epoch: 15 [48960/60000 (82%)]\t training loss: 0.000012\n",
      "epoch: 15 [49280/60000 (82%)]\t training loss: 0.001542\n",
      "epoch: 15 [49600/60000 (83%)]\t training loss: 0.002729\n",
      "epoch: 15 [49920/60000 (83%)]\t training loss: 0.000425\n",
      "epoch: 15 [50240/60000 (84%)]\t training loss: 0.014161\n",
      "epoch: 15 [50560/60000 (84%)]\t training loss: 0.001272\n",
      "epoch: 15 [50880/60000 (85%)]\t training loss: 0.001139\n",
      "epoch: 15 [51200/60000 (85%)]\t training loss: 0.000122\n",
      "epoch: 15 [51520/60000 (86%)]\t training loss: 0.013991\n",
      "epoch: 15 [51840/60000 (86%)]\t training loss: 0.001720\n",
      "epoch: 15 [52160/60000 (87%)]\t training loss: 0.000424\n",
      "epoch: 15 [52480/60000 (87%)]\t training loss: 0.000327\n",
      "epoch: 15 [52800/60000 (88%)]\t training loss: 0.000408\n",
      "epoch: 15 [53120/60000 (89%)]\t training loss: 0.011760\n",
      "epoch: 15 [53440/60000 (89%)]\t training loss: 0.260515\n",
      "epoch: 15 [53760/60000 (90%)]\t training loss: 0.008517\n",
      "epoch: 15 [54080/60000 (90%)]\t training loss: 0.000213\n",
      "epoch: 15 [54400/60000 (91%)]\t training loss: 0.001818\n",
      "epoch: 15 [54720/60000 (91%)]\t training loss: 0.000242\n",
      "epoch: 15 [55040/60000 (92%)]\t training loss: 0.002556\n",
      "epoch: 15 [55360/60000 (92%)]\t training loss: 0.000359\n",
      "epoch: 15 [55680/60000 (93%)]\t training loss: 0.000009\n",
      "epoch: 15 [56000/60000 (93%)]\t training loss: 0.000347\n",
      "epoch: 15 [56320/60000 (94%)]\t training loss: 0.030874\n",
      "epoch: 15 [56640/60000 (94%)]\t training loss: 0.005398\n",
      "epoch: 15 [56960/60000 (95%)]\t training loss: 0.002282\n",
      "epoch: 15 [57280/60000 (95%)]\t training loss: 0.001721\n",
      "epoch: 15 [57600/60000 (96%)]\t training loss: 0.068028\n",
      "epoch: 15 [57920/60000 (97%)]\t training loss: 0.000088\n",
      "epoch: 15 [58240/60000 (97%)]\t training loss: 0.257423\n",
      "epoch: 15 [58560/60000 (98%)]\t training loss: 0.000925\n",
      "epoch: 15 [58880/60000 (98%)]\t training loss: 0.000062\n",
      "epoch: 15 [59200/60000 (99%)]\t training loss: 0.001921\n",
      "epoch: 15 [59520/60000 (99%)]\t training loss: 0.000068\n",
      "epoch: 15 [59840/60000 (100%)]\t training loss: 0.000168\n",
      "\n",
      "Test dataset: Overall Loss: 0.0350, Overall Accuracy: 9917/10000 (99%)\n",
      "\n",
      "epoch: 16 [0/60000 (0%)]\t training loss: 0.000153\n",
      "epoch: 16 [320/60000 (1%)]\t training loss: 0.000514\n",
      "epoch: 16 [640/60000 (1%)]\t training loss: 0.000159\n",
      "epoch: 16 [960/60000 (2%)]\t training loss: 0.171822\n",
      "epoch: 16 [1280/60000 (2%)]\t training loss: 0.000067\n",
      "epoch: 16 [1600/60000 (3%)]\t training loss: 0.002865\n",
      "epoch: 16 [1920/60000 (3%)]\t training loss: 0.022837\n",
      "epoch: 16 [2240/60000 (4%)]\t training loss: 0.001408\n",
      "epoch: 16 [2560/60000 (4%)]\t training loss: 0.001293\n",
      "epoch: 16 [2880/60000 (5%)]\t training loss: 0.085022\n",
      "epoch: 16 [3200/60000 (5%)]\t training loss: 0.004718\n",
      "epoch: 16 [3520/60000 (6%)]\t training loss: 0.004533\n",
      "epoch: 16 [3840/60000 (6%)]\t training loss: 0.021829\n",
      "epoch: 16 [4160/60000 (7%)]\t training loss: 0.001277\n",
      "epoch: 16 [4480/60000 (7%)]\t training loss: 0.010882\n",
      "epoch: 16 [4800/60000 (8%)]\t training loss: 0.079905\n",
      "epoch: 16 [5120/60000 (9%)]\t training loss: 0.008195\n",
      "epoch: 16 [5440/60000 (9%)]\t training loss: 0.036312\n",
      "epoch: 16 [5760/60000 (10%)]\t training loss: 0.261625\n",
      "epoch: 16 [6080/60000 (10%)]\t training loss: 0.005044\n",
      "epoch: 16 [6400/60000 (11%)]\t training loss: 0.019253\n",
      "epoch: 16 [6720/60000 (11%)]\t training loss: 0.012481\n",
      "epoch: 16 [7040/60000 (12%)]\t training loss: 0.000107\n",
      "epoch: 16 [7360/60000 (12%)]\t training loss: 0.000077\n",
      "epoch: 16 [7680/60000 (13%)]\t training loss: 0.000030\n",
      "epoch: 16 [8000/60000 (13%)]\t training loss: 0.000265\n",
      "epoch: 16 [8320/60000 (14%)]\t training loss: 0.005335\n",
      "epoch: 16 [8640/60000 (14%)]\t training loss: 0.000638\n",
      "epoch: 16 [8960/60000 (15%)]\t training loss: 0.165349\n",
      "epoch: 16 [9280/60000 (15%)]\t training loss: 0.007080\n",
      "epoch: 16 [9600/60000 (16%)]\t training loss: 0.029848\n",
      "epoch: 16 [9920/60000 (17%)]\t training loss: 0.013469\n",
      "epoch: 16 [10240/60000 (17%)]\t training loss: 0.000156\n",
      "epoch: 16 [10560/60000 (18%)]\t training loss: 0.000141\n",
      "epoch: 16 [10880/60000 (18%)]\t training loss: 0.000056\n",
      "epoch: 16 [11200/60000 (19%)]\t training loss: 0.002067\n",
      "epoch: 16 [11520/60000 (19%)]\t training loss: 0.000516\n",
      "epoch: 16 [11840/60000 (20%)]\t training loss: 0.004343\n",
      "epoch: 16 [12160/60000 (20%)]\t training loss: 0.000063\n",
      "epoch: 16 [12480/60000 (21%)]\t training loss: 0.000034\n",
      "epoch: 16 [12800/60000 (21%)]\t training loss: 0.006265\n",
      "epoch: 16 [13120/60000 (22%)]\t training loss: 0.000116\n",
      "epoch: 16 [13440/60000 (22%)]\t training loss: 0.026710\n",
      "epoch: 16 [13760/60000 (23%)]\t training loss: 0.000915\n",
      "epoch: 16 [14080/60000 (23%)]\t training loss: 0.089121\n",
      "epoch: 16 [14400/60000 (24%)]\t training loss: 0.000016\n",
      "epoch: 16 [14720/60000 (25%)]\t training loss: 0.004063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16 [15040/60000 (25%)]\t training loss: 0.000004\n",
      "epoch: 16 [15360/60000 (26%)]\t training loss: 0.000012\n",
      "epoch: 16 [15680/60000 (26%)]\t training loss: 0.000533\n",
      "epoch: 16 [16000/60000 (27%)]\t training loss: 0.000123\n",
      "epoch: 16 [16320/60000 (27%)]\t training loss: 0.005864\n",
      "epoch: 16 [16640/60000 (28%)]\t training loss: 0.017874\n",
      "epoch: 16 [16960/60000 (28%)]\t training loss: 0.001386\n",
      "epoch: 16 [17280/60000 (29%)]\t training loss: 0.000154\n",
      "epoch: 16 [17600/60000 (29%)]\t training loss: 0.000004\n",
      "epoch: 16 [17920/60000 (30%)]\t training loss: 0.045544\n",
      "epoch: 16 [18240/60000 (30%)]\t training loss: 0.000676\n",
      "epoch: 16 [18560/60000 (31%)]\t training loss: 0.000084\n",
      "epoch: 16 [18880/60000 (31%)]\t training loss: 0.004694\n",
      "epoch: 16 [19200/60000 (32%)]\t training loss: 0.005859\n",
      "epoch: 16 [19520/60000 (33%)]\t training loss: 0.000213\n",
      "epoch: 16 [19840/60000 (33%)]\t training loss: 0.059978\n",
      "epoch: 16 [20160/60000 (34%)]\t training loss: 0.042538\n",
      "epoch: 16 [20480/60000 (34%)]\t training loss: 0.000017\n",
      "epoch: 16 [20800/60000 (35%)]\t training loss: 0.000522\n",
      "epoch: 16 [21120/60000 (35%)]\t training loss: 0.000019\n",
      "epoch: 16 [21440/60000 (36%)]\t training loss: 0.010566\n",
      "epoch: 16 [21760/60000 (36%)]\t training loss: 0.000059\n",
      "epoch: 16 [22080/60000 (37%)]\t training loss: 0.000714\n",
      "epoch: 16 [22400/60000 (37%)]\t training loss: 0.001945\n",
      "epoch: 16 [22720/60000 (38%)]\t training loss: 0.000325\n",
      "epoch: 16 [23040/60000 (38%)]\t training loss: 0.011987\n",
      "epoch: 16 [23360/60000 (39%)]\t training loss: 0.001110\n",
      "epoch: 16 [23680/60000 (39%)]\t training loss: 0.016459\n",
      "epoch: 16 [24000/60000 (40%)]\t training loss: 0.000727\n",
      "epoch: 16 [24320/60000 (41%)]\t training loss: 0.000063\n",
      "epoch: 16 [24640/60000 (41%)]\t training loss: 0.003660\n",
      "epoch: 16 [24960/60000 (42%)]\t training loss: 0.000151\n",
      "epoch: 16 [25280/60000 (42%)]\t training loss: 0.000952\n",
      "epoch: 16 [25600/60000 (43%)]\t training loss: 0.000479\n",
      "epoch: 16 [25920/60000 (43%)]\t training loss: 0.001688\n",
      "epoch: 16 [26240/60000 (44%)]\t training loss: 0.000023\n",
      "epoch: 16 [26560/60000 (44%)]\t training loss: 0.001901\n",
      "epoch: 16 [26880/60000 (45%)]\t training loss: 0.000470\n",
      "epoch: 16 [27200/60000 (45%)]\t training loss: 0.001027\n",
      "epoch: 16 [27520/60000 (46%)]\t training loss: 0.011069\n",
      "epoch: 16 [27840/60000 (46%)]\t training loss: 0.008418\n",
      "epoch: 16 [28160/60000 (47%)]\t training loss: 0.003621\n",
      "epoch: 16 [28480/60000 (47%)]\t training loss: 0.001380\n",
      "epoch: 16 [28800/60000 (48%)]\t training loss: 0.002083\n",
      "epoch: 16 [29120/60000 (49%)]\t training loss: 0.000348\n",
      "epoch: 16 [29440/60000 (49%)]\t training loss: 0.000260\n",
      "epoch: 16 [29760/60000 (50%)]\t training loss: 0.012647\n",
      "epoch: 16 [30080/60000 (50%)]\t training loss: 0.000307\n",
      "epoch: 16 [30400/60000 (51%)]\t training loss: 0.003649\n",
      "epoch: 16 [30720/60000 (51%)]\t training loss: 0.039365\n",
      "epoch: 16 [31040/60000 (52%)]\t training loss: 0.003847\n",
      "epoch: 16 [31360/60000 (52%)]\t training loss: 0.000107\n",
      "epoch: 16 [31680/60000 (53%)]\t training loss: 0.004798\n",
      "epoch: 16 [32000/60000 (53%)]\t training loss: 0.029344\n",
      "epoch: 16 [32320/60000 (54%)]\t training loss: 0.202485\n",
      "epoch: 16 [32640/60000 (54%)]\t training loss: 0.000029\n",
      "epoch: 16 [32960/60000 (55%)]\t training loss: 0.003707\n",
      "epoch: 16 [33280/60000 (55%)]\t training loss: 0.000232\n",
      "epoch: 16 [33600/60000 (56%)]\t training loss: 0.004485\n",
      "epoch: 16 [33920/60000 (57%)]\t training loss: 0.000752\n",
      "epoch: 16 [34240/60000 (57%)]\t training loss: 0.002916\n",
      "epoch: 16 [34560/60000 (58%)]\t training loss: 0.000097\n",
      "epoch: 16 [34880/60000 (58%)]\t training loss: 0.060718\n",
      "epoch: 16 [35200/60000 (59%)]\t training loss: 0.136792\n",
      "epoch: 16 [35520/60000 (59%)]\t training loss: 0.000079\n",
      "epoch: 16 [35840/60000 (60%)]\t training loss: 0.003858\n",
      "epoch: 16 [36160/60000 (60%)]\t training loss: 0.001099\n",
      "epoch: 16 [36480/60000 (61%)]\t training loss: 0.000226\n",
      "epoch: 16 [36800/60000 (61%)]\t training loss: 0.000164\n",
      "epoch: 16 [37120/60000 (62%)]\t training loss: 0.000810\n",
      "epoch: 16 [37440/60000 (62%)]\t training loss: 0.000561\n",
      "epoch: 16 [37760/60000 (63%)]\t training loss: 0.000057\n",
      "epoch: 16 [38080/60000 (63%)]\t training loss: 0.000078\n",
      "epoch: 16 [38400/60000 (64%)]\t training loss: 0.015178\n",
      "epoch: 16 [38720/60000 (65%)]\t training loss: 0.000362\n",
      "epoch: 16 [39040/60000 (65%)]\t training loss: 0.000268\n",
      "epoch: 16 [39360/60000 (66%)]\t training loss: 0.022066\n",
      "epoch: 16 [39680/60000 (66%)]\t training loss: 0.012449\n",
      "epoch: 16 [40000/60000 (67%)]\t training loss: 0.002736\n",
      "epoch: 16 [40320/60000 (67%)]\t training loss: 0.000041\n",
      "epoch: 16 [40640/60000 (68%)]\t training loss: 0.001977\n",
      "epoch: 16 [40960/60000 (68%)]\t training loss: 0.090065\n",
      "epoch: 16 [41280/60000 (69%)]\t training loss: 0.001344\n",
      "epoch: 16 [41600/60000 (69%)]\t training loss: 0.015748\n",
      "epoch: 16 [41920/60000 (70%)]\t training loss: 0.002920\n",
      "epoch: 16 [42240/60000 (70%)]\t training loss: 0.116063\n",
      "epoch: 16 [42560/60000 (71%)]\t training loss: 0.009480\n",
      "epoch: 16 [42880/60000 (71%)]\t training loss: 0.164691\n",
      "epoch: 16 [43200/60000 (72%)]\t training loss: 0.010568\n",
      "epoch: 16 [43520/60000 (73%)]\t training loss: 0.000123\n",
      "epoch: 16 [43840/60000 (73%)]\t training loss: 0.000035\n",
      "epoch: 16 [44160/60000 (74%)]\t training loss: 0.385391\n",
      "epoch: 16 [44480/60000 (74%)]\t training loss: 0.004213\n",
      "epoch: 16 [44800/60000 (75%)]\t training loss: 0.000272\n",
      "epoch: 16 [45120/60000 (75%)]\t training loss: 0.004215\n",
      "epoch: 16 [45440/60000 (76%)]\t training loss: 0.000803\n",
      "epoch: 16 [45760/60000 (76%)]\t training loss: 0.016306\n",
      "epoch: 16 [46080/60000 (77%)]\t training loss: 0.016964\n",
      "epoch: 16 [46400/60000 (77%)]\t training loss: 0.008078\n",
      "epoch: 16 [46720/60000 (78%)]\t training loss: 0.000027\n",
      "epoch: 16 [47040/60000 (78%)]\t training loss: 0.009682\n",
      "epoch: 16 [47360/60000 (79%)]\t training loss: 0.000083\n",
      "epoch: 16 [47680/60000 (79%)]\t training loss: 0.000052\n",
      "epoch: 16 [48000/60000 (80%)]\t training loss: 0.000195\n",
      "epoch: 16 [48320/60000 (81%)]\t training loss: 0.000126\n",
      "epoch: 16 [48640/60000 (81%)]\t training loss: 0.001686\n",
      "epoch: 16 [48960/60000 (82%)]\t training loss: 0.019124\n",
      "epoch: 16 [49280/60000 (82%)]\t training loss: 0.000655\n",
      "epoch: 16 [49600/60000 (83%)]\t training loss: 0.000770\n",
      "epoch: 16 [49920/60000 (83%)]\t training loss: 0.001392\n",
      "epoch: 16 [50240/60000 (84%)]\t training loss: 0.002409\n",
      "epoch: 16 [50560/60000 (84%)]\t training loss: 0.000001\n",
      "epoch: 16 [50880/60000 (85%)]\t training loss: 0.000011\n",
      "epoch: 16 [51200/60000 (85%)]\t training loss: 0.000016\n",
      "epoch: 16 [51520/60000 (86%)]\t training loss: 0.011690\n",
      "epoch: 16 [51840/60000 (86%)]\t training loss: 0.003702\n",
      "epoch: 16 [52160/60000 (87%)]\t training loss: 0.000059\n",
      "epoch: 16 [52480/60000 (87%)]\t training loss: 0.002754\n",
      "epoch: 16 [52800/60000 (88%)]\t training loss: 0.000002\n",
      "epoch: 16 [53120/60000 (89%)]\t training loss: 0.000403\n",
      "epoch: 16 [53440/60000 (89%)]\t training loss: 0.075581\n",
      "epoch: 16 [53760/60000 (90%)]\t training loss: 0.000134\n",
      "epoch: 16 [54080/60000 (90%)]\t training loss: 0.004951\n",
      "epoch: 16 [54400/60000 (91%)]\t training loss: 0.000533\n",
      "epoch: 16 [54720/60000 (91%)]\t training loss: 0.000005\n",
      "epoch: 16 [55040/60000 (92%)]\t training loss: 0.000088\n",
      "epoch: 16 [55360/60000 (92%)]\t training loss: 0.000166\n",
      "epoch: 16 [55680/60000 (93%)]\t training loss: 0.018880\n",
      "epoch: 16 [56000/60000 (93%)]\t training loss: 0.000307\n",
      "epoch: 16 [56320/60000 (94%)]\t training loss: 0.000685\n",
      "epoch: 16 [56640/60000 (94%)]\t training loss: 0.001074\n",
      "epoch: 16 [56960/60000 (95%)]\t training loss: 0.000097\n",
      "epoch: 16 [57280/60000 (95%)]\t training loss: 0.002011\n",
      "epoch: 16 [57600/60000 (96%)]\t training loss: 0.000114\n",
      "epoch: 16 [57920/60000 (97%)]\t training loss: 0.000246\n",
      "epoch: 16 [58240/60000 (97%)]\t training loss: 0.000225\n",
      "epoch: 16 [58560/60000 (98%)]\t training loss: 0.056445\n",
      "epoch: 16 [58880/60000 (98%)]\t training loss: 0.000760\n",
      "epoch: 16 [59200/60000 (99%)]\t training loss: 0.094490\n",
      "epoch: 16 [59520/60000 (99%)]\t training loss: 0.002283\n",
      "epoch: 16 [59840/60000 (100%)]\t training loss: 0.018933\n",
      "\n",
      "Test dataset: Overall Loss: 0.0357, Overall Accuracy: 9914/10000 (99%)\n",
      "\n",
      "epoch: 17 [0/60000 (0%)]\t training loss: 0.018906\n",
      "epoch: 17 [320/60000 (1%)]\t training loss: 0.015662\n",
      "epoch: 17 [640/60000 (1%)]\t training loss: 0.000025\n",
      "epoch: 17 [960/60000 (2%)]\t training loss: 0.004692\n",
      "epoch: 17 [1280/60000 (2%)]\t training loss: 0.000129\n",
      "epoch: 17 [1600/60000 (3%)]\t training loss: 0.000239\n",
      "epoch: 17 [1920/60000 (3%)]\t training loss: 0.003475\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 [2240/60000 (4%)]\t training loss: 0.000190\n",
      "epoch: 17 [2560/60000 (4%)]\t training loss: 0.007022\n",
      "epoch: 17 [2880/60000 (5%)]\t training loss: 0.000026\n",
      "epoch: 17 [3200/60000 (5%)]\t training loss: 0.000299\n",
      "epoch: 17 [3520/60000 (6%)]\t training loss: 0.000126\n",
      "epoch: 17 [3840/60000 (6%)]\t training loss: 0.000817\n",
      "epoch: 17 [4160/60000 (7%)]\t training loss: 0.000894\n",
      "epoch: 17 [4480/60000 (7%)]\t training loss: 0.002803\n",
      "epoch: 17 [4800/60000 (8%)]\t training loss: 0.034422\n",
      "epoch: 17 [5120/60000 (9%)]\t training loss: 0.003799\n",
      "epoch: 17 [5440/60000 (9%)]\t training loss: 0.000050\n",
      "epoch: 17 [5760/60000 (10%)]\t training loss: 0.003492\n",
      "epoch: 17 [6080/60000 (10%)]\t training loss: 0.000001\n",
      "epoch: 17 [6400/60000 (11%)]\t training loss: 0.002048\n",
      "epoch: 17 [6720/60000 (11%)]\t training loss: 0.010294\n",
      "epoch: 17 [7040/60000 (12%)]\t training loss: 0.000027\n",
      "epoch: 17 [7360/60000 (12%)]\t training loss: 0.002072\n",
      "epoch: 17 [7680/60000 (13%)]\t training loss: 0.001444\n",
      "epoch: 17 [8000/60000 (13%)]\t training loss: 0.000389\n",
      "epoch: 17 [8320/60000 (14%)]\t training loss: 0.000008\n",
      "epoch: 17 [8640/60000 (14%)]\t training loss: 0.012150\n",
      "epoch: 17 [8960/60000 (15%)]\t training loss: 0.008523\n",
      "epoch: 17 [9280/60000 (15%)]\t training loss: 0.013735\n",
      "epoch: 17 [9600/60000 (16%)]\t training loss: 0.207695\n",
      "epoch: 17 [9920/60000 (17%)]\t training loss: 0.001847\n",
      "epoch: 17 [10240/60000 (17%)]\t training loss: 0.182596\n",
      "epoch: 17 [10560/60000 (18%)]\t training loss: 0.000002\n",
      "epoch: 17 [10880/60000 (18%)]\t training loss: 0.000025\n",
      "epoch: 17 [11200/60000 (19%)]\t training loss: 0.047205\n",
      "epoch: 17 [11520/60000 (19%)]\t training loss: 0.004588\n",
      "epoch: 17 [11840/60000 (20%)]\t training loss: 0.008552\n",
      "epoch: 17 [12160/60000 (20%)]\t training loss: 0.000225\n",
      "epoch: 17 [12480/60000 (21%)]\t training loss: 0.000137\n",
      "epoch: 17 [12800/60000 (21%)]\t training loss: 0.003734\n",
      "epoch: 17 [13120/60000 (22%)]\t training loss: 0.005214\n",
      "epoch: 17 [13440/60000 (22%)]\t training loss: 0.006943\n",
      "epoch: 17 [13760/60000 (23%)]\t training loss: 0.001643\n",
      "epoch: 17 [14080/60000 (23%)]\t training loss: 0.001536\n",
      "epoch: 17 [14400/60000 (24%)]\t training loss: 0.000025\n",
      "epoch: 17 [14720/60000 (25%)]\t training loss: 0.000115\n",
      "epoch: 17 [15040/60000 (25%)]\t training loss: 0.001773\n",
      "epoch: 17 [15360/60000 (26%)]\t training loss: 0.000387\n",
      "epoch: 17 [15680/60000 (26%)]\t training loss: 0.003394\n",
      "epoch: 17 [16000/60000 (27%)]\t training loss: 0.000185\n",
      "epoch: 17 [16320/60000 (27%)]\t training loss: 0.010091\n",
      "epoch: 17 [16640/60000 (28%)]\t training loss: 0.003742\n",
      "epoch: 17 [16960/60000 (28%)]\t training loss: 0.029960\n",
      "epoch: 17 [17280/60000 (29%)]\t training loss: 0.003340\n",
      "epoch: 17 [17600/60000 (29%)]\t training loss: 0.002879\n",
      "epoch: 17 [17920/60000 (30%)]\t training loss: 0.000018\n",
      "epoch: 17 [18240/60000 (30%)]\t training loss: 0.002914\n",
      "epoch: 17 [18560/60000 (31%)]\t training loss: 0.000214\n",
      "epoch: 17 [18880/60000 (31%)]\t training loss: 0.000017\n",
      "epoch: 17 [19200/60000 (32%)]\t training loss: 0.000009\n",
      "epoch: 17 [19520/60000 (33%)]\t training loss: 0.000036\n",
      "epoch: 17 [19840/60000 (33%)]\t training loss: 0.010983\n",
      "epoch: 17 [20160/60000 (34%)]\t training loss: 0.004457\n",
      "epoch: 17 [20480/60000 (34%)]\t training loss: 0.000372\n",
      "epoch: 17 [20800/60000 (35%)]\t training loss: 0.005925\n",
      "epoch: 17 [21120/60000 (35%)]\t training loss: 0.021989\n",
      "epoch: 17 [21440/60000 (36%)]\t training loss: 0.133044\n",
      "epoch: 17 [21760/60000 (36%)]\t training loss: 0.019179\n",
      "epoch: 17 [22080/60000 (37%)]\t training loss: 0.001143\n",
      "epoch: 17 [22400/60000 (37%)]\t training loss: 0.017853\n",
      "epoch: 17 [22720/60000 (38%)]\t training loss: 0.001423\n",
      "epoch: 17 [23040/60000 (38%)]\t training loss: 0.000531\n",
      "epoch: 17 [23360/60000 (39%)]\t training loss: 0.029249\n",
      "epoch: 17 [23680/60000 (39%)]\t training loss: 0.023729\n",
      "epoch: 17 [24000/60000 (40%)]\t training loss: 0.000063\n",
      "epoch: 17 [24320/60000 (41%)]\t training loss: 0.000142\n",
      "epoch: 17 [24640/60000 (41%)]\t training loss: 0.075511\n",
      "epoch: 17 [24960/60000 (42%)]\t training loss: 0.000149\n",
      "epoch: 17 [25280/60000 (42%)]\t training loss: 0.001680\n",
      "epoch: 17 [25600/60000 (43%)]\t training loss: 0.010892\n",
      "epoch: 17 [25920/60000 (43%)]\t training loss: 0.000018\n",
      "epoch: 17 [26240/60000 (44%)]\t training loss: 0.000188\n",
      "epoch: 17 [26560/60000 (44%)]\t training loss: 0.000354\n",
      "epoch: 17 [26880/60000 (45%)]\t training loss: 0.041871\n",
      "epoch: 17 [27200/60000 (45%)]\t training loss: 0.000910\n",
      "epoch: 17 [27520/60000 (46%)]\t training loss: 0.000010\n",
      "epoch: 17 [27840/60000 (46%)]\t training loss: 0.000024\n",
      "epoch: 17 [28160/60000 (47%)]\t training loss: 0.001127\n",
      "epoch: 17 [28480/60000 (47%)]\t training loss: 0.002104\n",
      "epoch: 17 [28800/60000 (48%)]\t training loss: 0.000010\n",
      "epoch: 17 [29120/60000 (49%)]\t training loss: 0.284648\n",
      "epoch: 17 [29440/60000 (49%)]\t training loss: 0.000001\n",
      "epoch: 17 [29760/60000 (50%)]\t training loss: 0.001945\n",
      "epoch: 17 [30080/60000 (50%)]\t training loss: 0.000004\n",
      "epoch: 17 [30400/60000 (51%)]\t training loss: 0.000850\n",
      "epoch: 17 [30720/60000 (51%)]\t training loss: 0.000583\n",
      "epoch: 17 [31040/60000 (52%)]\t training loss: 0.000455\n",
      "epoch: 17 [31360/60000 (52%)]\t training loss: 0.000462\n",
      "epoch: 17 [31680/60000 (53%)]\t training loss: 0.001645\n",
      "epoch: 17 [32000/60000 (53%)]\t training loss: 0.001635\n",
      "epoch: 17 [32320/60000 (54%)]\t training loss: 0.005047\n",
      "epoch: 17 [32640/60000 (54%)]\t training loss: 0.047540\n",
      "epoch: 17 [32960/60000 (55%)]\t training loss: 0.008105\n",
      "epoch: 17 [33280/60000 (55%)]\t training loss: 0.000524\n",
      "epoch: 17 [33600/60000 (56%)]\t training loss: 0.000195\n",
      "epoch: 17 [33920/60000 (57%)]\t training loss: 0.003546\n",
      "epoch: 17 [34240/60000 (57%)]\t training loss: 0.061061\n",
      "epoch: 17 [34560/60000 (58%)]\t training loss: 0.010328\n",
      "epoch: 17 [34880/60000 (58%)]\t training loss: 0.012965\n",
      "epoch: 17 [35200/60000 (59%)]\t training loss: 0.017803\n",
      "epoch: 17 [35520/60000 (59%)]\t training loss: 0.000149\n",
      "epoch: 17 [35840/60000 (60%)]\t training loss: 0.000795\n",
      "epoch: 17 [36160/60000 (60%)]\t training loss: 0.000923\n",
      "epoch: 17 [36480/60000 (61%)]\t training loss: 0.021542\n",
      "epoch: 17 [36800/60000 (61%)]\t training loss: 0.000337\n",
      "epoch: 17 [37120/60000 (62%)]\t training loss: 0.001392\n",
      "epoch: 17 [37440/60000 (62%)]\t training loss: 0.000496\n",
      "epoch: 17 [37760/60000 (63%)]\t training loss: 0.081223\n",
      "epoch: 17 [38080/60000 (63%)]\t training loss: 0.000150\n",
      "epoch: 17 [38400/60000 (64%)]\t training loss: 0.000155\n",
      "epoch: 17 [38720/60000 (65%)]\t training loss: 0.001265\n",
      "epoch: 17 [39040/60000 (65%)]\t training loss: 0.006352\n",
      "epoch: 17 [39360/60000 (66%)]\t training loss: 0.144103\n",
      "epoch: 17 [39680/60000 (66%)]\t training loss: 0.010997\n",
      "epoch: 17 [40000/60000 (67%)]\t training loss: 0.000018\n",
      "epoch: 17 [40320/60000 (67%)]\t training loss: 0.000050\n",
      "epoch: 17 [40640/60000 (68%)]\t training loss: 0.000002\n",
      "epoch: 17 [40960/60000 (68%)]\t training loss: 0.003717\n",
      "epoch: 17 [41280/60000 (69%)]\t training loss: 0.000431\n",
      "epoch: 17 [41600/60000 (69%)]\t training loss: 0.000824\n",
      "epoch: 17 [41920/60000 (70%)]\t training loss: 0.012728\n",
      "epoch: 17 [42240/60000 (70%)]\t training loss: 0.001548\n",
      "epoch: 17 [42560/60000 (71%)]\t training loss: 0.004150\n",
      "epoch: 17 [42880/60000 (71%)]\t training loss: 0.000003\n",
      "epoch: 17 [43200/60000 (72%)]\t training loss: 0.001650\n",
      "epoch: 17 [43520/60000 (73%)]\t training loss: 0.064113\n",
      "epoch: 17 [43840/60000 (73%)]\t training loss: 0.001738\n",
      "epoch: 17 [44160/60000 (74%)]\t training loss: 0.014218\n",
      "epoch: 17 [44480/60000 (74%)]\t training loss: 0.000346\n",
      "epoch: 17 [44800/60000 (75%)]\t training loss: 0.080053\n",
      "epoch: 17 [45120/60000 (75%)]\t training loss: 0.007933\n",
      "epoch: 17 [45440/60000 (76%)]\t training loss: 0.000404\n",
      "epoch: 17 [45760/60000 (76%)]\t training loss: 0.088315\n",
      "epoch: 17 [46080/60000 (77%)]\t training loss: 0.000368\n",
      "epoch: 17 [46400/60000 (77%)]\t training loss: 0.000015\n",
      "epoch: 17 [46720/60000 (78%)]\t training loss: 0.011738\n",
      "epoch: 17 [47040/60000 (78%)]\t training loss: 0.000378\n",
      "epoch: 17 [47360/60000 (79%)]\t training loss: 0.002563\n",
      "epoch: 17 [47680/60000 (79%)]\t training loss: 0.006849\n",
      "epoch: 17 [48000/60000 (80%)]\t training loss: 0.006696\n",
      "epoch: 17 [48320/60000 (81%)]\t training loss: 0.000266\n",
      "epoch: 17 [48640/60000 (81%)]\t training loss: 0.000646\n",
      "epoch: 17 [48960/60000 (82%)]\t training loss: 0.015116\n",
      "epoch: 17 [49280/60000 (82%)]\t training loss: 0.048632\n",
      "epoch: 17 [49600/60000 (83%)]\t training loss: 0.000010\n",
      "epoch: 17 [49920/60000 (83%)]\t training loss: 0.000160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17 [50240/60000 (84%)]\t training loss: 0.008073\n",
      "epoch: 17 [50560/60000 (84%)]\t training loss: 0.000376\n",
      "epoch: 17 [50880/60000 (85%)]\t training loss: 0.000269\n",
      "epoch: 17 [51200/60000 (85%)]\t training loss: 0.000007\n",
      "epoch: 17 [51520/60000 (86%)]\t training loss: 0.000055\n",
      "epoch: 17 [51840/60000 (86%)]\t training loss: 0.000624\n",
      "epoch: 17 [52160/60000 (87%)]\t training loss: 0.003861\n",
      "epoch: 17 [52480/60000 (87%)]\t training loss: 0.000017\n",
      "epoch: 17 [52800/60000 (88%)]\t training loss: 0.008456\n",
      "epoch: 17 [53120/60000 (89%)]\t training loss: 0.000028\n",
      "epoch: 17 [53440/60000 (89%)]\t training loss: 0.001042\n",
      "epoch: 17 [53760/60000 (90%)]\t training loss: 0.000008\n",
      "epoch: 17 [54080/60000 (90%)]\t training loss: 0.012946\n",
      "epoch: 17 [54400/60000 (91%)]\t training loss: 0.001728\n",
      "epoch: 17 [54720/60000 (91%)]\t training loss: 0.000130\n",
      "epoch: 17 [55040/60000 (92%)]\t training loss: 0.000099\n",
      "epoch: 17 [55360/60000 (92%)]\t training loss: 0.004713\n",
      "epoch: 17 [55680/60000 (93%)]\t training loss: 0.042597\n",
      "epoch: 17 [56000/60000 (93%)]\t training loss: 0.018811\n",
      "epoch: 17 [56320/60000 (94%)]\t training loss: 0.000012\n",
      "epoch: 17 [56640/60000 (94%)]\t training loss: 0.001140\n",
      "epoch: 17 [56960/60000 (95%)]\t training loss: 0.001093\n",
      "epoch: 17 [57280/60000 (95%)]\t training loss: 0.000005\n",
      "epoch: 17 [57600/60000 (96%)]\t training loss: 0.000133\n",
      "epoch: 17 [57920/60000 (97%)]\t training loss: 0.000382\n",
      "epoch: 17 [58240/60000 (97%)]\t training loss: 0.001647\n",
      "epoch: 17 [58560/60000 (98%)]\t training loss: 0.013133\n",
      "epoch: 17 [58880/60000 (98%)]\t training loss: 0.000328\n",
      "epoch: 17 [59200/60000 (99%)]\t training loss: 0.000088\n",
      "epoch: 17 [59520/60000 (99%)]\t training loss: 0.000072\n",
      "epoch: 17 [59840/60000 (100%)]\t training loss: 0.000206\n",
      "\n",
      "Test dataset: Overall Loss: 0.0416, Overall Accuracy: 9908/10000 (99%)\n",
      "\n",
      "epoch: 18 [0/60000 (0%)]\t training loss: 0.000292\n",
      "epoch: 18 [320/60000 (1%)]\t training loss: 0.000913\n",
      "epoch: 18 [640/60000 (1%)]\t training loss: 0.000049\n",
      "epoch: 18 [960/60000 (2%)]\t training loss: 0.000157\n",
      "epoch: 18 [1280/60000 (2%)]\t training loss: 0.000061\n",
      "epoch: 18 [1600/60000 (3%)]\t training loss: 0.003958\n",
      "epoch: 18 [1920/60000 (3%)]\t training loss: 0.000013\n",
      "epoch: 18 [2240/60000 (4%)]\t training loss: 0.000206\n",
      "epoch: 18 [2560/60000 (4%)]\t training loss: 0.014585\n",
      "epoch: 18 [2880/60000 (5%)]\t training loss: 0.001929\n",
      "epoch: 18 [3200/60000 (5%)]\t training loss: 0.000065\n",
      "epoch: 18 [3520/60000 (6%)]\t training loss: 0.003352\n",
      "epoch: 18 [3840/60000 (6%)]\t training loss: 0.000072\n",
      "epoch: 18 [4160/60000 (7%)]\t training loss: 0.006175\n",
      "epoch: 18 [4480/60000 (7%)]\t training loss: 0.035034\n",
      "epoch: 18 [4800/60000 (8%)]\t training loss: 0.001627\n",
      "epoch: 18 [5120/60000 (9%)]\t training loss: 0.003398\n",
      "epoch: 18 [5440/60000 (9%)]\t training loss: 0.000158\n",
      "epoch: 18 [5760/60000 (10%)]\t training loss: 0.087303\n",
      "epoch: 18 [6080/60000 (10%)]\t training loss: 0.006865\n",
      "epoch: 18 [6400/60000 (11%)]\t training loss: 0.001269\n",
      "epoch: 18 [6720/60000 (11%)]\t training loss: 0.018043\n",
      "epoch: 18 [7040/60000 (12%)]\t training loss: 0.000207\n",
      "epoch: 18 [7360/60000 (12%)]\t training loss: 0.000566\n",
      "epoch: 18 [7680/60000 (13%)]\t training loss: 0.001279\n",
      "epoch: 18 [8000/60000 (13%)]\t training loss: 0.000028\n",
      "epoch: 18 [8320/60000 (14%)]\t training loss: 0.000786\n",
      "epoch: 18 [8640/60000 (14%)]\t training loss: 0.000053\n",
      "epoch: 18 [8960/60000 (15%)]\t training loss: 0.005472\n",
      "epoch: 18 [9280/60000 (15%)]\t training loss: 0.000432\n",
      "epoch: 18 [9600/60000 (16%)]\t training loss: 0.000261\n",
      "epoch: 18 [9920/60000 (17%)]\t training loss: 0.000001\n",
      "epoch: 18 [10240/60000 (17%)]\t training loss: 0.000134\n",
      "epoch: 18 [10560/60000 (18%)]\t training loss: 0.210259\n",
      "epoch: 18 [10880/60000 (18%)]\t training loss: 0.030452\n",
      "epoch: 18 [11200/60000 (19%)]\t training loss: 0.000449\n",
      "epoch: 18 [11520/60000 (19%)]\t training loss: 0.009026\n",
      "epoch: 18 [11840/60000 (20%)]\t training loss: 0.213116\n",
      "epoch: 18 [12160/60000 (20%)]\t training loss: 0.008939\n",
      "epoch: 18 [12480/60000 (21%)]\t training loss: 0.000779\n",
      "epoch: 18 [12800/60000 (21%)]\t training loss: 0.000095\n",
      "epoch: 18 [13120/60000 (22%)]\t training loss: 0.051320\n",
      "epoch: 18 [13440/60000 (22%)]\t training loss: 0.000096\n",
      "epoch: 18 [13760/60000 (23%)]\t training loss: 0.014612\n",
      "epoch: 18 [14080/60000 (23%)]\t training loss: 0.002291\n",
      "epoch: 18 [14400/60000 (24%)]\t training loss: 0.045377\n",
      "epoch: 18 [14720/60000 (25%)]\t training loss: 0.000037\n",
      "epoch: 18 [15040/60000 (25%)]\t training loss: 0.002807\n",
      "epoch: 18 [15360/60000 (26%)]\t training loss: 0.003151\n",
      "epoch: 18 [15680/60000 (26%)]\t training loss: 0.000026\n",
      "epoch: 18 [16000/60000 (27%)]\t training loss: 0.000111\n",
      "epoch: 18 [16320/60000 (27%)]\t training loss: 0.000691\n",
      "epoch: 18 [16640/60000 (28%)]\t training loss: 0.000251\n",
      "epoch: 18 [16960/60000 (28%)]\t training loss: 0.000001\n",
      "epoch: 18 [17280/60000 (29%)]\t training loss: 0.000356\n",
      "epoch: 18 [17600/60000 (29%)]\t training loss: 0.008568\n",
      "epoch: 18 [17920/60000 (30%)]\t training loss: 0.000063\n",
      "epoch: 18 [18240/60000 (30%)]\t training loss: 0.008007\n",
      "epoch: 18 [18560/60000 (31%)]\t training loss: 0.000064\n",
      "epoch: 18 [18880/60000 (31%)]\t training loss: 0.000119\n",
      "epoch: 18 [19200/60000 (32%)]\t training loss: 0.000672\n",
      "epoch: 18 [19520/60000 (33%)]\t training loss: 0.166624\n",
      "epoch: 18 [19840/60000 (33%)]\t training loss: 0.000032\n",
      "epoch: 18 [20160/60000 (34%)]\t training loss: 0.033197\n",
      "epoch: 18 [20480/60000 (34%)]\t training loss: 0.180278\n",
      "epoch: 18 [20800/60000 (35%)]\t training loss: 0.000695\n",
      "epoch: 18 [21120/60000 (35%)]\t training loss: 0.000019\n",
      "epoch: 18 [21440/60000 (36%)]\t training loss: 0.000001\n",
      "epoch: 18 [21760/60000 (36%)]\t training loss: 0.004068\n",
      "epoch: 18 [22080/60000 (37%)]\t training loss: 0.001682\n",
      "epoch: 18 [22400/60000 (37%)]\t training loss: 0.001090\n",
      "epoch: 18 [22720/60000 (38%)]\t training loss: 0.047014\n",
      "epoch: 18 [23040/60000 (38%)]\t training loss: 0.001330\n",
      "epoch: 18 [23360/60000 (39%)]\t training loss: 0.263787\n",
      "epoch: 18 [23680/60000 (39%)]\t training loss: 0.009467\n",
      "epoch: 18 [24000/60000 (40%)]\t training loss: 0.011450\n",
      "epoch: 18 [24320/60000 (41%)]\t training loss: 0.002970\n",
      "epoch: 18 [24640/60000 (41%)]\t training loss: 0.260399\n",
      "epoch: 18 [24960/60000 (42%)]\t training loss: 0.019782\n",
      "epoch: 18 [25280/60000 (42%)]\t training loss: 0.001341\n",
      "epoch: 18 [25600/60000 (43%)]\t training loss: 0.004847\n",
      "epoch: 18 [25920/60000 (43%)]\t training loss: 0.000370\n",
      "epoch: 18 [26240/60000 (44%)]\t training loss: 0.000531\n",
      "epoch: 18 [26560/60000 (44%)]\t training loss: 0.000710\n",
      "epoch: 18 [26880/60000 (45%)]\t training loss: 0.000031\n",
      "epoch: 18 [27200/60000 (45%)]\t training loss: 0.000365\n",
      "epoch: 18 [27520/60000 (46%)]\t training loss: 0.000541\n",
      "epoch: 18 [27840/60000 (46%)]\t training loss: 0.000082\n",
      "epoch: 18 [28160/60000 (47%)]\t training loss: 0.016494\n",
      "epoch: 18 [28480/60000 (47%)]\t training loss: 0.000016\n",
      "epoch: 18 [28800/60000 (48%)]\t training loss: 0.088402\n",
      "epoch: 18 [29120/60000 (49%)]\t training loss: 0.000175\n",
      "epoch: 18 [29440/60000 (49%)]\t training loss: 0.078684\n",
      "epoch: 18 [29760/60000 (50%)]\t training loss: 0.000010\n",
      "epoch: 18 [30080/60000 (50%)]\t training loss: 0.000315\n",
      "epoch: 18 [30400/60000 (51%)]\t training loss: 0.007890\n",
      "epoch: 18 [30720/60000 (51%)]\t training loss: 0.006670\n",
      "epoch: 18 [31040/60000 (52%)]\t training loss: 0.024834\n",
      "epoch: 18 [31360/60000 (52%)]\t training loss: 0.046919\n",
      "epoch: 18 [31680/60000 (53%)]\t training loss: 0.003669\n",
      "epoch: 18 [32000/60000 (53%)]\t training loss: 0.000252\n",
      "epoch: 18 [32320/60000 (54%)]\t training loss: 0.001364\n",
      "epoch: 18 [32640/60000 (54%)]\t training loss: 0.074566\n",
      "epoch: 18 [32960/60000 (55%)]\t training loss: 0.000481\n",
      "epoch: 18 [33280/60000 (55%)]\t training loss: 0.000308\n",
      "epoch: 18 [33600/60000 (56%)]\t training loss: 0.000263\n",
      "epoch: 18 [33920/60000 (57%)]\t training loss: 0.000728\n",
      "epoch: 18 [34240/60000 (57%)]\t training loss: 0.025949\n",
      "epoch: 18 [34560/60000 (58%)]\t training loss: 0.000177\n",
      "epoch: 18 [34880/60000 (58%)]\t training loss: 0.016009\n",
      "epoch: 18 [35200/60000 (59%)]\t training loss: 0.028002\n",
      "epoch: 18 [35520/60000 (59%)]\t training loss: 0.011005\n",
      "epoch: 18 [35840/60000 (60%)]\t training loss: 0.003012\n",
      "epoch: 18 [36160/60000 (60%)]\t training loss: 0.004702\n",
      "epoch: 18 [36480/60000 (61%)]\t training loss: 0.064108\n",
      "epoch: 18 [36800/60000 (61%)]\t training loss: 0.000067\n",
      "epoch: 18 [37120/60000 (62%)]\t training loss: 0.000006\n",
      "epoch: 18 [37440/60000 (62%)]\t training loss: 0.000095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18 [37760/60000 (63%)]\t training loss: 0.000536\n",
      "epoch: 18 [38080/60000 (63%)]\t training loss: 0.000496\n",
      "epoch: 18 [38400/60000 (64%)]\t training loss: 0.031088\n",
      "epoch: 18 [38720/60000 (65%)]\t training loss: 0.062934\n",
      "epoch: 18 [39040/60000 (65%)]\t training loss: 0.016301\n",
      "epoch: 18 [39360/60000 (66%)]\t training loss: 0.126752\n",
      "epoch: 18 [39680/60000 (66%)]\t training loss: 0.029616\n",
      "epoch: 18 [40000/60000 (67%)]\t training loss: 0.000084\n",
      "epoch: 18 [40320/60000 (67%)]\t training loss: 0.000095\n",
      "epoch: 18 [40640/60000 (68%)]\t training loss: 0.000017\n",
      "epoch: 18 [40960/60000 (68%)]\t training loss: 0.000183\n",
      "epoch: 18 [41280/60000 (69%)]\t training loss: 0.000019\n",
      "epoch: 18 [41600/60000 (69%)]\t training loss: 0.000096\n",
      "epoch: 18 [41920/60000 (70%)]\t training loss: 0.000149\n",
      "epoch: 18 [42240/60000 (70%)]\t training loss: 0.166595\n",
      "epoch: 18 [42560/60000 (71%)]\t training loss: 0.000253\n",
      "epoch: 18 [42880/60000 (71%)]\t training loss: 0.000022\n",
      "epoch: 18 [43200/60000 (72%)]\t training loss: 0.000456\n",
      "epoch: 18 [43520/60000 (73%)]\t training loss: 0.007276\n",
      "epoch: 18 [43840/60000 (73%)]\t training loss: 0.086449\n",
      "epoch: 18 [44160/60000 (74%)]\t training loss: 0.000112\n",
      "epoch: 18 [44480/60000 (74%)]\t training loss: 0.008747\n",
      "epoch: 18 [44800/60000 (75%)]\t training loss: 0.011262\n",
      "epoch: 18 [45120/60000 (75%)]\t training loss: 0.089972\n",
      "epoch: 18 [45440/60000 (76%)]\t training loss: 0.019405\n",
      "epoch: 18 [45760/60000 (76%)]\t training loss: 0.007715\n",
      "epoch: 18 [46080/60000 (77%)]\t training loss: 0.000470\n",
      "epoch: 18 [46400/60000 (77%)]\t training loss: 0.000491\n",
      "epoch: 18 [46720/60000 (78%)]\t training loss: 0.000671\n",
      "epoch: 18 [47040/60000 (78%)]\t training loss: 0.000282\n",
      "epoch: 18 [47360/60000 (79%)]\t training loss: 0.000433\n",
      "epoch: 18 [47680/60000 (79%)]\t training loss: 0.002583\n",
      "epoch: 18 [48000/60000 (80%)]\t training loss: 0.000007\n",
      "epoch: 18 [48320/60000 (81%)]\t training loss: 0.003600\n",
      "epoch: 18 [48640/60000 (81%)]\t training loss: 0.000049\n",
      "epoch: 18 [48960/60000 (82%)]\t training loss: 0.004668\n",
      "epoch: 18 [49280/60000 (82%)]\t training loss: 0.000138\n",
      "epoch: 18 [49600/60000 (83%)]\t training loss: 0.000121\n",
      "epoch: 18 [49920/60000 (83%)]\t training loss: 0.000023\n",
      "epoch: 18 [50240/60000 (84%)]\t training loss: 0.000098\n",
      "epoch: 18 [50560/60000 (84%)]\t training loss: 0.001823\n",
      "epoch: 18 [50880/60000 (85%)]\t training loss: 0.000024\n",
      "epoch: 18 [51200/60000 (85%)]\t training loss: 0.003704\n",
      "epoch: 18 [51520/60000 (86%)]\t training loss: 0.058158\n",
      "epoch: 18 [51840/60000 (86%)]\t training loss: 0.000065\n",
      "epoch: 18 [52160/60000 (87%)]\t training loss: 0.000214\n",
      "epoch: 18 [52480/60000 (87%)]\t training loss: 0.000442\n",
      "epoch: 18 [52800/60000 (88%)]\t training loss: 0.035585\n",
      "epoch: 18 [53120/60000 (89%)]\t training loss: 0.000042\n",
      "epoch: 18 [53440/60000 (89%)]\t training loss: 0.069378\n",
      "epoch: 18 [53760/60000 (90%)]\t training loss: 0.000053\n",
      "epoch: 18 [54080/60000 (90%)]\t training loss: 0.078914\n",
      "epoch: 18 [54400/60000 (91%)]\t training loss: 0.057690\n",
      "epoch: 18 [54720/60000 (91%)]\t training loss: 0.001857\n",
      "epoch: 18 [55040/60000 (92%)]\t training loss: 0.001456\n",
      "epoch: 18 [55360/60000 (92%)]\t training loss: 0.015570\n",
      "epoch: 18 [55680/60000 (93%)]\t training loss: 0.001884\n",
      "epoch: 18 [56000/60000 (93%)]\t training loss: 0.134192\n",
      "epoch: 18 [56320/60000 (94%)]\t training loss: 0.002964\n",
      "epoch: 18 [56640/60000 (94%)]\t training loss: 0.039987\n",
      "epoch: 18 [56960/60000 (95%)]\t training loss: 0.003010\n",
      "epoch: 18 [57280/60000 (95%)]\t training loss: 0.000040\n",
      "epoch: 18 [57600/60000 (96%)]\t training loss: 0.000032\n",
      "epoch: 18 [57920/60000 (97%)]\t training loss: 0.000153\n",
      "epoch: 18 [58240/60000 (97%)]\t training loss: 0.003230\n",
      "epoch: 18 [58560/60000 (98%)]\t training loss: 0.000128\n",
      "epoch: 18 [58880/60000 (98%)]\t training loss: 0.003040\n",
      "epoch: 18 [59200/60000 (99%)]\t training loss: 0.000589\n",
      "epoch: 18 [59520/60000 (99%)]\t training loss: 0.001138\n",
      "epoch: 18 [59840/60000 (100%)]\t training loss: 0.000630\n",
      "\n",
      "Test dataset: Overall Loss: 0.0361, Overall Accuracy: 9914/10000 (99%)\n",
      "\n",
      "epoch: 19 [0/60000 (0%)]\t training loss: 0.080434\n",
      "epoch: 19 [320/60000 (1%)]\t training loss: 0.014266\n",
      "epoch: 19 [640/60000 (1%)]\t training loss: 0.000356\n",
      "epoch: 19 [960/60000 (2%)]\t training loss: 0.000897\n",
      "epoch: 19 [1280/60000 (2%)]\t training loss: 0.000958\n",
      "epoch: 19 [1600/60000 (3%)]\t training loss: 0.000506\n",
      "epoch: 19 [1920/60000 (3%)]\t training loss: 0.001094\n",
      "epoch: 19 [2240/60000 (4%)]\t training loss: 0.060832\n",
      "epoch: 19 [2560/60000 (4%)]\t training loss: 0.003204\n",
      "epoch: 19 [2880/60000 (5%)]\t training loss: 0.000428\n",
      "epoch: 19 [3200/60000 (5%)]\t training loss: 0.004458\n",
      "epoch: 19 [3520/60000 (6%)]\t training loss: 0.000412\n",
      "epoch: 19 [3840/60000 (6%)]\t training loss: 0.000018\n",
      "epoch: 19 [4160/60000 (7%)]\t training loss: 0.000760\n",
      "epoch: 19 [4480/60000 (7%)]\t training loss: 0.001449\n",
      "epoch: 19 [4800/60000 (8%)]\t training loss: 0.003364\n",
      "epoch: 19 [5120/60000 (9%)]\t training loss: 0.000682\n",
      "epoch: 19 [5440/60000 (9%)]\t training loss: 0.005388\n",
      "epoch: 19 [5760/60000 (10%)]\t training loss: 0.000092\n",
      "epoch: 19 [6080/60000 (10%)]\t training loss: 0.000008\n",
      "epoch: 19 [6400/60000 (11%)]\t training loss: 0.001143\n",
      "epoch: 19 [6720/60000 (11%)]\t training loss: 0.000321\n",
      "epoch: 19 [7040/60000 (12%)]\t training loss: 0.001776\n",
      "epoch: 19 [7360/60000 (12%)]\t training loss: 0.000250\n",
      "epoch: 19 [7680/60000 (13%)]\t training loss: 0.000222\n",
      "epoch: 19 [8000/60000 (13%)]\t training loss: 0.000003\n",
      "epoch: 19 [8320/60000 (14%)]\t training loss: 0.004418\n",
      "epoch: 19 [8640/60000 (14%)]\t training loss: 0.000683\n",
      "epoch: 19 [8960/60000 (15%)]\t training loss: 0.000036\n",
      "epoch: 19 [9280/60000 (15%)]\t training loss: 0.001790\n",
      "epoch: 19 [9600/60000 (16%)]\t training loss: 0.001051\n",
      "epoch: 19 [9920/60000 (17%)]\t training loss: 0.000024\n",
      "epoch: 19 [10240/60000 (17%)]\t training loss: 0.000199\n",
      "epoch: 19 [10560/60000 (18%)]\t training loss: 0.008204\n",
      "epoch: 19 [10880/60000 (18%)]\t training loss: 0.294077\n",
      "epoch: 19 [11200/60000 (19%)]\t training loss: 0.005161\n",
      "epoch: 19 [11520/60000 (19%)]\t training loss: 0.000186\n",
      "epoch: 19 [11840/60000 (20%)]\t training loss: 0.000297\n",
      "epoch: 19 [12160/60000 (20%)]\t training loss: 0.111618\n",
      "epoch: 19 [12480/60000 (21%)]\t training loss: 0.000417\n",
      "epoch: 19 [12800/60000 (21%)]\t training loss: 0.002891\n",
      "epoch: 19 [13120/60000 (22%)]\t training loss: 0.022609\n",
      "epoch: 19 [13440/60000 (22%)]\t training loss: 0.000154\n",
      "epoch: 19 [13760/60000 (23%)]\t training loss: 0.000171\n",
      "epoch: 19 [14080/60000 (23%)]\t training loss: 0.000725\n",
      "epoch: 19 [14400/60000 (24%)]\t training loss: 0.000001\n",
      "epoch: 19 [14720/60000 (25%)]\t training loss: 0.000335\n",
      "epoch: 19 [15040/60000 (25%)]\t training loss: 0.000029\n",
      "epoch: 19 [15360/60000 (26%)]\t training loss: 0.000021\n",
      "epoch: 19 [15680/60000 (26%)]\t training loss: 0.005358\n",
      "epoch: 19 [16000/60000 (27%)]\t training loss: 0.003540\n",
      "epoch: 19 [16320/60000 (27%)]\t training loss: 0.004241\n",
      "epoch: 19 [16640/60000 (28%)]\t training loss: 0.000023\n",
      "epoch: 19 [16960/60000 (28%)]\t training loss: 0.001320\n",
      "epoch: 19 [17280/60000 (29%)]\t training loss: 0.000471\n",
      "epoch: 19 [17600/60000 (29%)]\t training loss: 0.002828\n",
      "epoch: 19 [17920/60000 (30%)]\t training loss: 0.001971\n",
      "epoch: 19 [18240/60000 (30%)]\t training loss: 0.009646\n",
      "epoch: 19 [18560/60000 (31%)]\t training loss: 0.025103\n",
      "epoch: 19 [18880/60000 (31%)]\t training loss: 0.004017\n",
      "epoch: 19 [19200/60000 (32%)]\t training loss: 0.000231\n",
      "epoch: 19 [19520/60000 (33%)]\t training loss: 0.000890\n",
      "epoch: 19 [19840/60000 (33%)]\t training loss: 0.000081\n",
      "epoch: 19 [20160/60000 (34%)]\t training loss: 0.001543\n",
      "epoch: 19 [20480/60000 (34%)]\t training loss: 0.000174\n",
      "epoch: 19 [20800/60000 (35%)]\t training loss: 0.000076\n",
      "epoch: 19 [21120/60000 (35%)]\t training loss: 0.000637\n",
      "epoch: 19 [21440/60000 (36%)]\t training loss: 0.002602\n",
      "epoch: 19 [21760/60000 (36%)]\t training loss: 0.010130\n",
      "epoch: 19 [22080/60000 (37%)]\t training loss: 0.000129\n",
      "epoch: 19 [22400/60000 (37%)]\t training loss: 0.275636\n",
      "epoch: 19 [22720/60000 (38%)]\t training loss: 0.008912\n",
      "epoch: 19 [23040/60000 (38%)]\t training loss: 0.010295\n",
      "epoch: 19 [23360/60000 (39%)]\t training loss: 0.240817\n",
      "epoch: 19 [23680/60000 (39%)]\t training loss: 0.004151\n",
      "epoch: 19 [24000/60000 (40%)]\t training loss: 0.000302\n",
      "epoch: 19 [24320/60000 (41%)]\t training loss: 0.043861\n",
      "epoch: 19 [24640/60000 (41%)]\t training loss: 0.059349\n",
      "epoch: 19 [24960/60000 (42%)]\t training loss: 0.000431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19 [25280/60000 (42%)]\t training loss: 0.000173\n",
      "epoch: 19 [25600/60000 (43%)]\t training loss: 0.008073\n",
      "epoch: 19 [25920/60000 (43%)]\t training loss: 0.000035\n",
      "epoch: 19 [26240/60000 (44%)]\t training loss: 0.000069\n",
      "epoch: 19 [26560/60000 (44%)]\t training loss: 0.000118\n",
      "epoch: 19 [26880/60000 (45%)]\t training loss: 0.000194\n",
      "epoch: 19 [27200/60000 (45%)]\t training loss: 0.000027\n",
      "epoch: 19 [27520/60000 (46%)]\t training loss: 0.000024\n",
      "epoch: 19 [27840/60000 (46%)]\t training loss: 0.000236\n",
      "epoch: 19 [28160/60000 (47%)]\t training loss: 0.000014\n",
      "epoch: 19 [28480/60000 (47%)]\t training loss: 0.012394\n",
      "epoch: 19 [28800/60000 (48%)]\t training loss: 0.000285\n",
      "epoch: 19 [29120/60000 (49%)]\t training loss: 0.000047\n",
      "epoch: 19 [29440/60000 (49%)]\t training loss: 0.045202\n",
      "epoch: 19 [29760/60000 (50%)]\t training loss: 0.266525\n",
      "epoch: 19 [30080/60000 (50%)]\t training loss: 0.002557\n",
      "epoch: 19 [30400/60000 (51%)]\t training loss: 0.044860\n",
      "epoch: 19 [30720/60000 (51%)]\t training loss: 0.000291\n",
      "epoch: 19 [31040/60000 (52%)]\t training loss: 0.051361\n",
      "epoch: 19 [31360/60000 (52%)]\t training loss: 0.000838\n",
      "epoch: 19 [31680/60000 (53%)]\t training loss: 0.000152\n",
      "epoch: 19 [32000/60000 (53%)]\t training loss: 0.000114\n",
      "epoch: 19 [32320/60000 (54%)]\t training loss: 0.063252\n",
      "epoch: 19 [32640/60000 (54%)]\t training loss: 0.000151\n",
      "epoch: 19 [32960/60000 (55%)]\t training loss: 0.000003\n",
      "epoch: 19 [33280/60000 (55%)]\t training loss: 0.000056\n",
      "epoch: 19 [33600/60000 (56%)]\t training loss: 0.000117\n",
      "epoch: 19 [33920/60000 (57%)]\t training loss: 0.004314\n",
      "epoch: 19 [34240/60000 (57%)]\t training loss: 0.009977\n",
      "epoch: 19 [34560/60000 (58%)]\t training loss: 0.035024\n",
      "epoch: 19 [34880/60000 (58%)]\t training loss: 0.053284\n",
      "epoch: 19 [35200/60000 (59%)]\t training loss: 0.086022\n",
      "epoch: 19 [35520/60000 (59%)]\t training loss: 0.007561\n",
      "epoch: 19 [35840/60000 (60%)]\t training loss: 0.000012\n",
      "epoch: 19 [36160/60000 (60%)]\t training loss: 0.000001\n",
      "epoch: 19 [36480/60000 (61%)]\t training loss: 0.000008\n",
      "epoch: 19 [36800/60000 (61%)]\t training loss: 0.000015\n",
      "epoch: 19 [37120/60000 (62%)]\t training loss: 0.000000\n",
      "epoch: 19 [37440/60000 (62%)]\t training loss: 0.000719\n",
      "epoch: 19 [37760/60000 (63%)]\t training loss: 0.000085\n",
      "epoch: 19 [38080/60000 (63%)]\t training loss: 0.000356\n",
      "epoch: 19 [38400/60000 (64%)]\t training loss: 0.125183\n",
      "epoch: 19 [38720/60000 (65%)]\t training loss: 0.000713\n",
      "epoch: 19 [39040/60000 (65%)]\t training loss: 0.068255\n",
      "epoch: 19 [39360/60000 (66%)]\t training loss: 0.000101\n",
      "epoch: 19 [39680/60000 (66%)]\t training loss: 0.000011\n",
      "epoch: 19 [40000/60000 (67%)]\t training loss: 0.000685\n",
      "epoch: 19 [40320/60000 (67%)]\t training loss: 0.000106\n",
      "epoch: 19 [40640/60000 (68%)]\t training loss: 0.002020\n",
      "epoch: 19 [40960/60000 (68%)]\t training loss: 0.003871\n",
      "epoch: 19 [41280/60000 (69%)]\t training loss: 0.000040\n",
      "epoch: 19 [41600/60000 (69%)]\t training loss: 0.000953\n",
      "epoch: 19 [41920/60000 (70%)]\t training loss: 0.012763\n",
      "epoch: 19 [42240/60000 (70%)]\t training loss: 0.087049\n",
      "epoch: 19 [42560/60000 (71%)]\t training loss: 0.000712\n",
      "epoch: 19 [42880/60000 (71%)]\t training loss: 0.004267\n",
      "epoch: 19 [43200/60000 (72%)]\t training loss: 0.000036\n",
      "epoch: 19 [43520/60000 (73%)]\t training loss: 0.009814\n",
      "epoch: 19 [43840/60000 (73%)]\t training loss: 0.001640\n",
      "epoch: 19 [44160/60000 (74%)]\t training loss: 0.000821\n",
      "epoch: 19 [44480/60000 (74%)]\t training loss: 0.000010\n",
      "epoch: 19 [44800/60000 (75%)]\t training loss: 0.000379\n",
      "epoch: 19 [45120/60000 (75%)]\t training loss: 0.023054\n",
      "epoch: 19 [45440/60000 (76%)]\t training loss: 0.000062\n",
      "epoch: 19 [45760/60000 (76%)]\t training loss: 0.000410\n",
      "epoch: 19 [46080/60000 (77%)]\t training loss: 0.016261\n",
      "epoch: 19 [46400/60000 (77%)]\t training loss: 0.001376\n",
      "epoch: 19 [46720/60000 (78%)]\t training loss: 0.001217\n",
      "epoch: 19 [47040/60000 (78%)]\t training loss: 0.003869\n",
      "epoch: 19 [47360/60000 (79%)]\t training loss: 0.001711\n",
      "epoch: 19 [47680/60000 (79%)]\t training loss: 0.000003\n",
      "epoch: 19 [48000/60000 (80%)]\t training loss: 0.000019\n",
      "epoch: 19 [48320/60000 (81%)]\t training loss: 0.000012\n",
      "epoch: 19 [48640/60000 (81%)]\t training loss: 0.000270\n",
      "epoch: 19 [48960/60000 (82%)]\t training loss: 0.001115\n",
      "epoch: 19 [49280/60000 (82%)]\t training loss: 0.000003\n",
      "epoch: 19 [49600/60000 (83%)]\t training loss: 0.018693\n",
      "epoch: 19 [49920/60000 (83%)]\t training loss: 0.000340\n",
      "epoch: 19 [50240/60000 (84%)]\t training loss: 0.000103\n",
      "epoch: 19 [50560/60000 (84%)]\t training loss: 0.000320\n",
      "epoch: 19 [50880/60000 (85%)]\t training loss: 0.000031\n",
      "epoch: 19 [51200/60000 (85%)]\t training loss: 0.000005\n",
      "epoch: 19 [51520/60000 (86%)]\t training loss: 0.000150\n",
      "epoch: 19 [51840/60000 (86%)]\t training loss: 0.000110\n",
      "epoch: 19 [52160/60000 (87%)]\t training loss: 0.000782\n",
      "epoch: 19 [52480/60000 (87%)]\t training loss: 0.001843\n",
      "epoch: 19 [52800/60000 (88%)]\t training loss: 0.007714\n",
      "epoch: 19 [53120/60000 (89%)]\t training loss: 0.000205\n",
      "epoch: 19 [53440/60000 (89%)]\t training loss: 0.000406\n",
      "epoch: 19 [53760/60000 (90%)]\t training loss: 0.000278\n",
      "epoch: 19 [54080/60000 (90%)]\t training loss: 0.000022\n",
      "epoch: 19 [54400/60000 (91%)]\t training loss: 0.012038\n",
      "epoch: 19 [54720/60000 (91%)]\t training loss: 0.044563\n",
      "epoch: 19 [55040/60000 (92%)]\t training loss: 0.048597\n",
      "epoch: 19 [55360/60000 (92%)]\t training loss: 0.000004\n",
      "epoch: 19 [55680/60000 (93%)]\t training loss: 0.000090\n",
      "epoch: 19 [56000/60000 (93%)]\t training loss: 0.065253\n",
      "epoch: 19 [56320/60000 (94%)]\t training loss: 0.000050\n",
      "epoch: 19 [56640/60000 (94%)]\t training loss: 0.000133\n",
      "epoch: 19 [56960/60000 (95%)]\t training loss: 0.079392\n",
      "epoch: 19 [57280/60000 (95%)]\t training loss: 0.000895\n",
      "epoch: 19 [57600/60000 (96%)]\t training loss: 0.001179\n",
      "epoch: 19 [57920/60000 (97%)]\t training loss: 0.083490\n",
      "epoch: 19 [58240/60000 (97%)]\t training loss: 0.163427\n",
      "epoch: 19 [58560/60000 (98%)]\t training loss: 0.000912\n",
      "epoch: 19 [58880/60000 (98%)]\t training loss: 0.000002\n",
      "epoch: 19 [59200/60000 (99%)]\t training loss: 0.006162\n",
      "epoch: 19 [59520/60000 (99%)]\t training loss: 0.000939\n",
      "epoch: 19 [59840/60000 (100%)]\t training loss: 0.000032\n",
      "\n",
      "Test dataset: Overall Loss: 0.0388, Overall Accuracy: 9920/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 20):\n",
    "    train(model, device, train_dataloader, optimizer, epoch)\n",
    "    test(model, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run inference on trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_samples = enumerate(test_dataloader)\n",
    "b_i, (sample_data, sample_targets) = next(test_samples)\n",
    "\n",
    "plt.imshow(sample_data[0][0], cmap='gray', interpolation='none')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prediction is : 3\n",
      "Ground truth is : 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model prediction is : {model(sample_data).data.max(1)[1][0]}\")\n",
    "print(f\"Ground truth is : {sample_targets[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_MODEL = \"./convnet.pth\"\n",
    "torch.save(model.state_dict(), PATH_TO_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(test_dataloader)\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(images)\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (cn1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (cn2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dp1): Dropout2d(p=0.1, inplace=False)\n",
       "  (dp2): Dropout2d(p=0.25, inplace=False)\n",
       "  (fc1): Linear(in_features=4608, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 3\n",
    "\n",
    "ip = images[ind].unsqueeze(0)\n",
    "ip.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attribute_image_features(algorithm, ip, **kwargs):\n",
    "    model.zero_grad()\n",
    "    tensor_attributions = algorithm.attribute(ip,\n",
    "                                              target=labels[ind],\n",
    "                                              **kwargs\n",
    "                                             )\n",
    "    \n",
    "    return tensor_attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "saliency = Saliency(model)\n",
    "grads = saliency.attribute(ip, target=labels[ind].item())\n",
    "grads = np.reshape(grads.squeeze().cpu().detach().numpy(), (28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximation delta:  tensor([0.0554])\n"
     ]
    }
   ],
   "source": [
    "ig = IntegratedGradients(model)\n",
    "attr_ig, delta = attribute_image_features(ig, ip, baselines=ip * 0, return_convergence_delta=True)\n",
    "attr_ig = np.reshape(attr_ig.squeeze().cpu().detach().numpy(), (28, 28, 1))\n",
    "print('Approximation delta: ', abs(delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(model)\n",
    "nt = NoiseTunnel(ig)\n",
    "attr_ig_nt = attribute_image_features(nt, ip, baselines=ip * 0, nt_type='smoothgrad_sq',\n",
    "                                      n_samples=100, stdevs=0.2)\n",
    "attr_ig_nt = np.transpose(attr_ig_nt.squeeze(0).cpu().detach().numpy(), (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DeepLift(model)\n",
    "attr_dl = attribute_image_features(dl, ip, baselines=ip * 0)\n",
    "attr_dl = np.transpose(attr_dl.squeeze(0).cpu().detach().numpy(), (1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Image\n",
      "Predicted: 3  Probability: 1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Original Image')\n",
    "print('Predicted:', classes[predicted[ind]], \n",
    "      ' Probability:', torch.max(F.softmax(outputs, 1)).item())\n",
    "\n",
    "original_image = np.tile(np.transpose((images[ind].cpu().detach().numpy() / 2) + 0.5, (1, 2, 0)), (1,1,3))\n",
    "\n",
    "_ = viz.visualize_image_attr(None, original_image, cmap='gray',\n",
    "                      method=\"original_image\", title=\"Original Image\")\n",
    "\n",
    "_ = viz.visualize_image_attr(grads, original_image, method=\"blended_heat_map\", sign=\"absolute_value\",\n",
    "                          show_colorbar=True, title=\"Overlayed Gradient Magnitudes\")\n",
    "\n",
    "_ = viz.visualize_image_attr(attr_ig, original_image, method=\"blended_heat_map\",sign=\"all\",\n",
    "                          show_colorbar=True, title=\"Overlayed Integrated Gradients\")\n",
    "\n",
    "_ = viz.visualize_image_attr(attr_ig_nt, original_image, method=\"blended_heat_map\", sign=\"absolute_value\", \n",
    "                             outlier_perc=10, show_colorbar=True, \n",
    "                             title=\"Overlayed Integrated Gradients \\n with SmoothGrad Squared\")\n",
    "\n",
    "_ = viz.visualize_image_attr(attr_dl, original_image, method=\"blended_heat_map\",sign=\"all\",show_colorbar=True, \n",
    "                          title=\"Overlayed DeepLift\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
