{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_token, num_inputs, num_heads, num_hidden, num_layers, dropout=0.3):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.model_name = 'transformer'\n",
    "        self.mask_source = None\n",
    "        self.position_enc = PosEnc(num_inputs, dropout)\n",
    "        layers_enc = TransformerEncoderLayer(num_inputs, num_heads, num_hidden, dropout)\n",
    "        self.enc_transformer = TransformerEncoder(layers_enc, num_layers)\n",
    "        self.enc = nn.Embedding(num_token, num_inputs)\n",
    "        self.num_inputs = num_inputs\n",
    "        self.dec = nn.Linear(num_inputs, num_token)\n",
    "        self.init_params()\n",
    "\n",
    "    def _gen_sqr_nxt_mask(self, size):\n",
    "        msk = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
    "        msk = msk.float().masked_fill(msk == 0, float('-inf'))\n",
    "        msk = msk.masked_fill(msk == 1, float(0.0))\n",
    "        return msk\n",
    "\n",
    "    def init_params(self):\n",
    "        initial_rng = 0.12\n",
    "        self.enc.weight.data.uniform_(-initial_rng, initial_rng)\n",
    "        self.dec.bias.data.zero_()\n",
    "        self.dec.weight.data.uniform_(-initial_rng, initial_rng)\n",
    "\n",
    "    def forward(self, source):\n",
    "        if self.mask_source is None or self.mask_source.size(0) != len(source):\n",
    "            dvc = source.device\n",
    "            msk = self._gen_sqr_nxt_mask(len(source)).to(dvc)\n",
    "            self.mask_source = msk\n",
    "\n",
    "        source = self.enc(source) * math.sqrt(self.num_inputs)\n",
    "        source = self.position_enc(source)\n",
    "        op = self.enc_transformer(source, self.mask_source)\n",
    "        op = self.dec(op)\n",
    "        return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEnc(nn.Module):\n",
    "    def __init__(self, d_m, dropout=0.2, size_limit=5000):\n",
    "        super(PosEnc, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        p_enc = torch.zeros(size_limit, d_m)\n",
    "        pos = torch.arange(0, size_limit, dtype=torch.float).unsqueeze(1)\n",
    "        divider = torch.exp(torch.arange(0, d_m, 2).float() * (-math.log(10000.0) / d_m))\n",
    "        p_enc[:, 0::2] = torch.sin(pos * divider)\n",
    "        p_enc[:, 1::2] = torch.cos(pos * divider)\n",
    "        p_enc = p_enc.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('p_enc', p_enc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.p_enc[:x.size(0), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"), lower=True, eos_token='<eos>', init_token='<sos>')\n",
    "training_text, validation_text, testing_text = torchtext.datasets.PennTreebank.splits(TEXT)\n",
    "TEXT.build_vocab(training_text)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def gen_batches(text_dataset, batch_size):\n",
    "    text_dataset = TEXT.numericalize([text_dataset.examples[0].text])\n",
    "    # divide text dataset into parts of size equal to batch_size\n",
    "    num_batches = text_dataset.size(0) // batch_size\n",
    "    # remove data points that lie outside batches (remainders)\n",
    "    text_dataset = text_dataset.narrow(0, 0, num_batches * batch_size)\n",
    "    # distribute dataset across batches evenly\n",
    "    text_dataset = text_dataset.view(batch_size, -1).t().contiguous()\n",
    "    return text_dataset.to(device)\n",
    "\n",
    "training_batch_size = 32\n",
    "evaluation_batch_size = 16\n",
    "\n",
    "training_data = gen_batches(training_text, training_batch_size)\n",
    "validation_data = gen_batches(validation_text, evaluation_batch_size)\n",
    "testing_data = gen_batches(testing_text, evaluation_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 64\n",
    "def return_batch(src, k):\n",
    "    sequence_length = min(max_seq_len, len(src) - 1 - k)\n",
    "    sequence_data = src[k:k+sequence_length]\n",
    "    sequence_label = src[k+1:k+1+sequence_length].view(-1)\n",
    "    return sequence_data, sequence_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tokens = len(TEXT.vocab.stoi) # vocabulary size\n",
    "embedding_size = 256 # dimension of embedding layer\n",
    "num_hidden_params = 256 # transformer encoder's hidden (feed forward) layer dimension\n",
    "num_layers = 2 # num of transformer encoder layers within transformer encoder\n",
    "num_heads = 2 # num of heads in (multi head) attention models\n",
    "dropout = 0.25 # value (fraction) of dropout\n",
    "transformer_model = Transformer(num_tokens, embedding_size, num_heads, num_hidden_params, num_layers, \n",
    "                                     dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "lrate = 4.0 # learning rate\n",
    "optim_module = torch.optim.SGD(transformer_model.parameters(), lr=lrate)\n",
    "sched_module = torch.optim.lr_scheduler.StepLR(optim_module, 1.0, gamma=0.88)\n",
    "\n",
    "def train_model():\n",
    "    transformer_model.train()\n",
    "    loss_total = 0.\n",
    "    time_start = time.time()\n",
    "    num_tokens = len(TEXT.vocab.stoi)\n",
    "    for b, i in enumerate(range(0, training_data.size(0) - 1, max_seq_len)):\n",
    "        train_data_batch, train_label_batch = return_batch(training_data, i)\n",
    "        optim_module.zero_grad()\n",
    "        op = transformer_model(train_data_batch)\n",
    "        loss_curr = loss_func(op.view(-1, num_tokens), train_label_batch)\n",
    "        loss_curr.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(transformer_model.parameters(), 0.6)\n",
    "        optim_module.step()\n",
    "\n",
    "        loss_total += loss_curr.item()\n",
    "        interval = 100\n",
    "        if b % interval == 0 and b > 0:\n",
    "            loss_interval = loss_total / interval\n",
    "            time_delta = time.time() - time_start\n",
    "            print(f\"epoch {ep}, {b}/{len(training_data)//max_seq_len} batches, training loss {loss_interval:.2f}, training perplexity {math.exp(loss_interval):.2f}\")\n",
    "            loss_total = 0\n",
    "            time_start = time.time()\n",
    "\n",
    "def eval_model(eval_model_obj, eval_data_source):\n",
    "    eval_model_obj.eval() \n",
    "    loss_total = 0.\n",
    "    num_tokens = len(TEXT.vocab.stoi)\n",
    "    with torch.no_grad():\n",
    "        for j in range(0, eval_data_source.size(0) - 1, max_seq_len):\n",
    "            eval_data, eval_label = return_batch(eval_data_source, j)\n",
    "            op = eval_model_obj(eval_data)\n",
    "            op_flat = op.view(-1, num_tokens)\n",
    "            loss_total += len(eval_data) * loss_func(op_flat, eval_label).item()\n",
    "    return loss_total / (len(eval_data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1.00, 256/471 batches, training loss 6.74, training perplexity 846.90\n",
      "\n",
      "epoch 1.00, validation loss 5.48, validation perplexity 240.56\n",
      "\n",
      "epoch 2.00, 256/471 batches, training loss 5.44, training perplexity 229.99\n",
      "\n",
      "epoch 2.00, validation loss 5.23, validation perplexity 186.90\n",
      "\n",
      "epoch 3.00, 256/471 batches, training loss 5.15, training perplexity 173.11\n",
      "\n",
      "epoch 3.00, validation loss 5.05, validation perplexity 156.21\n",
      "\n",
      "epoch 4.00, 256/471 batches, training loss 4.98, training perplexity 146.10\n",
      "\n",
      "epoch 4.00, validation loss 4.96, validation perplexity 142.45\n",
      "\n",
      "epoch 5.00, 256/471 batches, training loss 4.85, training perplexity 128.16\n",
      "\n",
      "epoch 5.00, validation loss 4.92, validation perplexity 137.11\n",
      "\n",
      "epoch 6.00, 256/471 batches, training loss 4.75, training perplexity 115.84\n",
      "\n",
      "epoch 6.00, validation loss 4.85, validation perplexity 127.18\n",
      "\n",
      "epoch 7.00, 256/471 batches, training loss 4.67, training perplexity 106.34\n",
      "\n",
      "epoch 7.00, validation loss 4.82, validation perplexity 123.66\n",
      "\n",
      "epoch 8.00, 256/471 batches, training loss 4.59, training perplexity 98.95\n",
      "\n",
      "epoch 8.00, validation loss 4.79, validation perplexity 120.65\n",
      "\n",
      "epoch 9.00, 256/471 batches, training loss 4.53, training perplexity 93.19\n",
      "\n",
      "epoch 9.00, validation loss 4.77, validation perplexity 117.76\n",
      "\n",
      "epoch 10.00, 256/471 batches, training loss 4.48, training perplexity 88.19\n",
      "\n",
      "epoch 10.00, validation loss 4.75, validation perplexity 115.70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "min_validation_loss = float(\"inf\")\n",
    "eps = 10 \n",
    "best_model_so_far = None\n",
    "\n",
    "for ep in range(1, eps + 1):\n",
    "    ep_time_start = time.time()\n",
    "    train_model()\n",
    "    validation_loss = eval_model(transformer_model, validation_data)\n",
    "    print()\n",
    "    print(f\"epoch {ep:}, validation loss {validation_loss:.2f}, validation perplexity {math.exp(validation_loss):.2f}\")\n",
    "    print()\n",
    "\n",
    "    if validation_loss < min_validation_loss:\n",
    "        min_validation_loss = validation_loss\n",
    "        best_model_so_far = transformer_model\n",
    "\n",
    "    sched_module.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing loss 4.68, testing perplexity 107.46\n"
     ]
    }
   ],
   "source": [
    "testing_loss = eval_model(best_model_so_far, testing_data)\n",
    "print(f\"testing loss {testing_loss:.2f}, testing perplexity {math.exp(testing_loss):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
